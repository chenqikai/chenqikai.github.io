<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spark Streaming 基本操作]]></title>
    <url>%2F2019%2F11%2F21%2FSpark_Streaming%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Spark Streaming 基本操作一、案例引入这里先引入一个基本的案例来演示流的创建：获取指定端口上的数据并进行词频统计。项目依赖和代码实现如下： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.12&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021import org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object NetworkWordCount &#123; def main(args: Array[String]) &#123; /*指定时间间隔为 5s*/ val sparkConf = new SparkConf().setAppName("NetworkWordCount").setMaster("local[2]") val ssc = new StreamingContext(sparkConf, Seconds(5)) /*创建文本输入流,并进行词频统计*/ val lines = ssc.socketTextStream("hadoop001", 9999) lines.flatMap(_.split(" ")).map(x =&gt; (x, 1)).reduceByKey(_ + _).print() /*启动服务*/ ssc.start() /*等待服务结束*/ ssc.awaitTermination() &#125;&#125; 使用本地模式启动 Spark 程序，然后使用 nc -lk 9999 打开端口并输入测试数据： 123[root@hadoop001 ~]# nc -lk 9999hello world hello spark hive hive hadoopstorm storm flink azkaban 此时控制台输出如下，可以看到已经接收到数据并按行进行了词频统计。 下面针对示例代码进行讲解： 3.1 StreamingContextSpark Streaming 编程的入口类是 StreamingContext，在创建时候需要指明 sparkConf 和 batchDuration(批次时间)，Spark 流处理本质是将流数据拆分为一个个批次，然后进行微批处理，batchDuration 就是批次拆分的时间间隔。这个时间可以根据业务需求和服务器性能进行指定，如果业务要求低延迟并且服务器性能也允许，则这个时间可以指定得很短。 这里需要注意的是：示例代码使用的是本地模式，配置为 local[2]，这里不能配置为 local[1]。这是因为对于流数据的处理，Spark 必须有一个独立的 Executor 来接收数据，然后再由其他的 Executors 来处理，所以为了保证数据能够被处理，至少要有 2 个 Executors。这里我们的程序只有一个数据流，在并行读取多个数据流的时候，也需要保证有足够的 Executors 来接收和处理数据。 3.2 数据源在示例代码中使用的是 socketTextStream 来创建基于 Socket 的数据流，实际上 Spark 还支持多种数据源，分为以下两类： 基本数据源：包括文件系统、Socket 连接等； 高级数据源：包括 Kafka，Flume，Kinesis 等。 在基本数据源中，Spark 支持监听 HDFS 上指定目录，当有新文件加入时，会获取其文件内容作为输入流。创建方式如下： 1234// 对于文本文件，指明监听目录即可streamingContext.textFileStream(dataDirectory)// 对于其他文件，需要指明目录，以及键的类型、值的类型、和输入格式streamingContext.fileStream[KeyClass, ValueClass, InputFormatClass](dataDirectory) 被监听的目录可以是具体目录，如 hdfs://host:8040/logs/；也可以使用通配符，如 hdfs://host:8040/logs/2017/*。 关于高级数据源的整合单独整理至：Spark Streaming 整合 Flume 和 Spark Streaming 整合 Kafka 3.3 服务的启动与停止在示例代码中，使用 streamingContext.start() 代表启动服务，此时还要使用 streamingContext.awaitTermination() 使服务处于等待和可用的状态，直到发生异常或者手动使用 streamingContext.stop() 进行终止。 二、Transformation2.1 DStream与RDDsDStream 是 Spark Streaming 提供的基本抽象。它表示连续的数据流。在内部，DStream 由一系列连续的 RDD 表示。所以从本质上而言，应用于 DStream 的任何操作都会转换为底层 RDD 上的操作。例如，在示例代码中 flatMap 算子的操作实际上是作用在每个 RDDs 上 (如下图)。因为这个原因，所以 DStream 能够支持 RDD 大部分的transformation算子。 2.2 updateStateByKey除了能够支持 RDD 的算子外，DStream 还有部分独有的transformation算子，这当中比较常用的是 updateStateByKey。文章开头的词频统计程序，只能统计每一次输入文本中单词出现的数量，想要统计所有历史输入中单词出现的数量，可以使用 updateStateByKey 算子。代码如下： 12345678910111213141516171819202122232425262728293031323334353637object NetworkWordCountV2 &#123; def main(args: Array[String]) &#123; /* * 本地测试时最好指定 hadoop 用户名,否则会默认使用本地电脑的用户名, * 此时在 HDFS 上创建目录时可能会抛出权限不足的异常 */ System.setProperty("HADOOP_USER_NAME", "root") val sparkConf = new SparkConf().setAppName("NetworkWordCountV2").setMaster("local[2]") val ssc = new StreamingContext(sparkConf, Seconds(5)) /*必须要设置检查点*/ ssc.checkpoint("hdfs://hadoop001:8020/spark-streaming") val lines = ssc.socketTextStream("hadoop001", 9999) lines.flatMap(_.split(" ")).map(x =&gt; (x, 1)) .updateStateByKey[Int](updateFunction _) //updateStateByKey 算子 .print() ssc.start() ssc.awaitTermination() &#125; /** * 累计求和 * * @param currentValues 当前的数据 * @param preValues 之前的数据 * @return 相加后的数据 */ def updateFunction(currentValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123; val current = currentValues.sum val pre = preValues.getOrElse(0) Some(current + pre) &#125;&#125; 使用 updateStateByKey 算子，你必须使用 ssc.checkpoint() 设置检查点，这样当使用 updateStateByKey 算子时，它会去检查点中取出上一次保存的信息，并使用自定义的 updateFunction 函数将上一次的数据和本次数据进行相加，然后返回。 2.3 启动测试在监听端口输入如下测试数据： 12345[root@hadoop001 ~]# nc -lk 9999hello world hello spark hive hive hadoopstorm storm flink azkabanhello world hello spark hive hive hadoopstorm storm flink azkaban 此时控制台输出如下，所有输入都被进行了词频累计： 同时在输出日志中还可以看到检查点操作的相关信息： 1234567# 保存检查点信息19/05/27 16:21:05 INFO CheckpointWriter: Saving checkpoint for time 1558945265000 ms to file 'hdfs://hadoop001:8020/spark-streaming/checkpoint-1558945265000'# 删除已经无用的检查点信息19/05/27 16:21:30 INFO CheckpointWriter: Deleting hdfs://hadoop001:8020/spark-streaming/checkpoint-1558945265000 三、输出操作3.1 输出APISpark Streaming 支持以下输出操作： Output Operation Meaning print() 在运行流应用程序的 driver 节点上打印 DStream 中每个批次的前十个元素。用于开发调试。 saveAsTextFiles(prefix, [suffix]) 将 DStream 的内容保存为文本文件。每个批处理间隔的文件名基于前缀和后缀生成：“prefix-TIME_IN_MS [.suffix]”。 saveAsObjectFiles(prefix, [suffix]) 将 DStream 的内容序列化为 Java 对象，并保存到 SequenceFiles。每个批处理间隔的文件名基于前缀和后缀生成：“prefix-TIME_IN_MS [.suffix]”。 saveAsHadoopFiles(prefix, [suffix]) 将 DStream 的内容保存为 Hadoop 文件。每个批处理间隔的文件名基于前缀和后缀生成：“prefix-TIME_IN_MS [.suffix]”。 foreachRDD(func) 最通用的输出方式，它将函数 func 应用于从流生成的每个 RDD。此函数应将每个 RDD 中的数据推送到外部系统，例如将 RDD 保存到文件，或通过网络将其写入数据库。 前面的四个 API 都是直接调用即可，下面主要讲解通用的输出方式 foreachRDD(func)，通过该 API 你可以将数据保存到任何你需要的数据源。 3.1 foreachRDD这里我们使用 Redis 作为客户端，对文章开头示例程序进行改变，把每一次词频统计的结果写入到 Redis，并利用 Redis 的 HINCRBY 命令来进行词频统计。这里需要导入 Jedis 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 具体实现代码如下: 12345678910111213141516171819202122232425262728293031323334import org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import redis.clients.jedis.Jedisobject NetworkWordCountToRedis &#123; def main(args: Array[String]) &#123; val sparkConf = new SparkConf().setAppName("NetworkWordCountToRedis").setMaster("local[2]") val ssc = new StreamingContext(sparkConf, Seconds(5)) /*创建文本输入流,并进行词频统计*/ val lines = ssc.socketTextStream("hadoop001", 9999) val pairs: DStream[(String, Int)] = lines.flatMap(_.split(" ")).map(x =&gt; (x, 1)).reduceByKey(_ + _) /*保存数据到 Redis*/ pairs.foreachRDD &#123; rdd =&gt; rdd.foreachPartition &#123; partitionOfRecords =&gt; var jedis: Jedis = null try &#123; jedis = JedisPoolUtil.getConnection partitionOfRecords.foreach(record =&gt; jedis.hincrBy("wordCount", record._1, record._2)) &#125; catch &#123; case ex: Exception =&gt; ex.printStackTrace() &#125; finally &#123; if (jedis != null) jedis.close() &#125; &#125; &#125; ssc.start() ssc.awaitTermination() &#125;&#125; 其中 JedisPoolUtil 的代码如下： 1234567891011121314151617181920212223242526import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;import redis.clients.jedis.JedisPoolConfig;public class JedisPoolUtil &#123; /* 声明为 volatile 防止指令重排序 */ private static volatile JedisPool jedisPool = null; private static final String HOST = "localhost"; private static final int PORT = 6379; /* 双重检查锁实现懒汉式单例 */ public static Jedis getConnection() &#123; if (jedisPool == null) &#123; synchronized (JedisPoolUtil.class) &#123; if (jedisPool == null) &#123; JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(30); config.setMaxIdle(10); jedisPool = new JedisPool(config, HOST, PORT); &#125; &#125; &#125; return jedisPool.getResource(); &#125;&#125; 3.3 代码说明这里将上面保存到 Redis 的代码单独抽取出来，并去除异常判断的部分。精简后的代码如下： 1234567pairs.foreachRDD &#123; rdd =&gt; rdd.foreachPartition &#123; partitionOfRecords =&gt; val jedis = JedisPoolUtil.getConnection partitionOfRecords.foreach(record =&gt; jedis.hincrBy("wordCount", record._1, record._2)) jedis.close() &#125;&#125; 这里可以看到一共使用了三次循环，分别是循环 RDD，循环分区，循环每条记录，上面我们的代码是在循环分区的时候获取连接，也就是为每一个分区获取一个连接。但是这里大家可能会有疑问：为什么不在循环 RDD 的时候，为每一个 RDD 获取一个连接，这样所需要的连接数会更少。实际上这是不可行的，如果按照这种情况进行改写，如下： 1234567pairs.foreachRDD &#123; rdd =&gt; val jedis = JedisPoolUtil.getConnection rdd.foreachPartition &#123; partitionOfRecords =&gt; partitionOfRecords.foreach(record =&gt; jedis.hincrBy("wordCount", record._1, record._2)) &#125; jedis.close()&#125; 此时在执行时候就会抛出 Caused by: java.io.NotSerializableException: redis.clients.jedis.Jedis，这是因为在实际计算时，Spark 会将对 RDD 操作分解为多个 Task，Task 运行在具体的 Worker Node 上。在执行之前，Spark 会对任务进行闭包，之后闭包被序列化并发送给每个 Executor，而 Jedis 显然是不能被序列化的，所以会抛出异常。 第二个需要注意的是 ConnectionPool 最好是一个静态，惰性初始化连接池 。这是因为 Spark 的转换操作本身就是惰性的，且没有数据流时不会触发写出操作，所以出于性能考虑，连接池应该是惰性的，因此上面 JedisPool 在初始化时采用了懒汉式单例进行惰性初始化。 3.4 启动测试在监听端口输入如下测试数据： 12345[root@hadoop001 ~]# nc -lk 9999hello world hello spark hive hive hadoopstorm storm flink azkabanhello world hello spark hive hive hadoopstorm storm flink azkaban 使用 Redis Manager 查看写入结果 (如下图),可以看到与使用 updateStateByKey 算子得到的计算结果相同。 参考资料Spark 官方文档：http://spark.apache.org/docs/latest/streaming-programming-guide.html]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark部署模式与作业提交]]></title>
    <url>%2F2019%2F11%2F15%2FSpark%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F%E4%B8%8E%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4%2F</url>
    <content type="text"><![CDATA[Spark部署模式与作业提交一、作业提交1.1 spark-submitSpark 所有模式均使用 spark-submit 命令提交作业，其格式如下： 12345678./bin/spark-submit \ --class &lt;main-class&gt; \ # 应用程序主入口类 --master &lt;master-url&gt; \ # 集群的 Master Url --deploy-mode &lt;deploy-mode&gt; \ # 部署模式 --conf &lt;key&gt;=&lt;value&gt; \ # 可选配置 ... # other options &lt;application-jar&gt; \ # Jar 包路径 [application-arguments] #传递给主入口类的参数 需要注意的是：在集群环境下，application-jar 必须能被集群中所有节点都能访问，可以是 HDFS 上的路径；也可以是本地文件系统路径，如果是本地文件系统路径，则要求集群中每一个机器节点上的相同路径都存在该 Jar 包。 1.2 deploy-modedeploy-mode 有 cluster 和 client 两个可选参数，默认为 client。这里以 Spark On Yarn 模式对两者进行说明 ： 在 cluster 模式下，Spark Drvier 在应用程序的 Master 进程内运行，该进程由群集上的 YARN 管理，提交作业的客户端可以在启动应用程序后关闭； 在 client 模式下，Spark Drvier 在提交作业的客户端进程中运行，Master 进程仅用于从 YARN 请求资源。 1.3 master-urlmaster-url 的所有可选参数如下表所示： Master URL Meaning local 使用一个线程本地运行 Spark local[K] 使用 K 个 worker 线程本地运行 Spark local[K,F] 使用 K 个 worker 线程本地运行 , 第二个参数为 Task 的失败重试次数 local[*] 使用与 CPU 核心数一样的线程数在本地运行 Spark local[*,F] 使用与 CPU 核心数一样的线程数在本地运行 Spark第二个参数为 Task 的失败重试次数 spark://HOST:PORT 连接至指定的 standalone 集群的 master 节点。端口号默认是 7077。 spark://HOST1:PORT1,HOST2:PORT2 如果 standalone 集群采用 Zookeeper 实现高可用，则必须包含由 zookeeper 设置的所有 master 主机地址。 mesos://HOST:PORT 连接至给定的 Mesos 集群。端口默认是 5050。对于使用了 ZooKeeper 的 Mesos cluster 来说，使用 mesos://zk://... 来指定地址，使用 --deploy-mode cluster 模式来提交。 yarn 连接至一个 YARN 集群，集群由配置的 HADOOP_CONF_DIR 或者 YARN_CONF_DIR 来决定。使用 --deploy-mode 参数来配置 client 或 cluster 模式。 下面主要介绍三种常用部署模式及对应的作业提交方式。 二、Local模式Local 模式下提交作业最为简单，不需要进行任何配置，提交命令如下： 123456# 本地模式提交应用spark-submit \--class org.apache.spark.examples.SparkPi \--master local[2] \/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \100 # 传给 SparkPi 的参数 spark-examples_2.11-2.4.0.jar 是 Spark 提供的测试用例包，SparkPi 用于计算 Pi 值，执行结果如下： 三、Standalone模式Standalone 是 Spark 提供的一种内置的集群模式，采用内置的资源管理器进行管理。下面按照如图所示演示 1 个 Mater 和 2 个 Worker 节点的集群配置，这里使用两台主机进行演示： hadoop001： 由于只有两台主机，所以 hadoop001 既是 Master 节点，也是 Worker 节点; hadoop002 ： Worker 节点。 3.1 环境配置首先需要保证 Spark 已经解压在两台主机的相同路径上。然后进入 hadoop001 的 ${SPARK_HOME}/conf/ 目录下，拷贝配置样本并进行相关配置： 1# cp spark-env.sh.template spark-env.sh 在 spark-env.sh 中配置 JDK 的目录，完成后将该配置使用 scp 命令分发到 hadoop002 上： 12# JDK安装位置JAVA_HOME=/usr/java/jdk1.8.0_201 3.2 集群配置在 ${SPARK_HOME}/conf/ 目录下，拷贝集群配置样本并进行相关配置： 1# cp slaves.template slaves 指定所有 Worker 节点的主机名： 123# A Spark Worker will be started on each of the machines listed below.hadoop001hadoop002 这里需要注意以下三点： 主机名与 IP 地址的映射必须在 /etc/hosts 文件中已经配置，否则就直接使用 IP 地址； 每个主机名必须独占一行； Spark 的 Master 主机是通过 SSH 访问所有的 Worker 节点，所以需要预先配置免密登录。 3.3 启动使用 start-all.sh 代表启动 Master 和所有 Worker 服务。 1./sbin/start-master.sh 访问 8080 端口，查看 Spark 的 Web-UI 界面,，此时应该显示有两个有效的工作节点： 3.4 提交作业12345678910111213141516171819# 以client模式提交到standalone集群 spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://hadoop001:7077 \--executor-memory 2G \--total-executor-cores 10 \/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \100# 以cluster模式提交到standalone集群 spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://207.184.161.138:7077 \--deploy-mode cluster \--supervise \ # 配置此参数代表开启监督，如果主应用程序异常退出，则自动重启 Driver--executor-memory 2G \--total-executor-cores 10 \/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \100 3.5 可选配置在虚拟机上提交作业时经常出现一个的问题是作业无法申请到足够的资源： 12Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources 这时候可以查看 Web UI，我这里是内存空间不足：提交命令中要求作业的 executor-memory 是 2G，但是实际的工作节点的 Memory 只有 1G，这时候你可以修改 --executor-memory，也可以修改 Woker 的 Memory，其默认值为主机所有可用内存值减去 1G。 关于 Master 和 Woker 节点的所有可选配置如下，可以在 spark-env.sh 中进行对应的配置： Environment Variable（环境变量） Meaning（含义） SPARK_MASTER_HOST master 节点地址 SPARK_MASTER_PORT master 节点地址端口（默认：7077） SPARK_MASTER_WEBUI_PORT master 的 web UI 的端口（默认：8080） SPARK_MASTER_OPTS 仅用于 master 的配置属性，格式是 “-Dx=y”（默认：none）,所有属性可以参考官方文档：spark-standalone-mode SPARK_LOCAL_DIRS spark 的临时存储的目录，用于暂存 map 的输出和持久化存储 RDDs。多个目录用逗号分隔 SPARK_WORKER_CORES spark worker 节点可以使用 CPU Cores 的数量。（默认：全部可用） SPARK_WORKER_MEMORY spark worker 节点可以使用的内存数量（默认：全部的内存减去 1GB）； SPARK_WORKER_PORT spark worker 节点的端口（默认： random（随机）） SPARK_WORKER_WEBUI_PORT worker 的 web UI 的 Port（端口）（默认：8081） SPARK_WORKER_DIR worker 运行应用程序的目录，这个目录中包含日志和暂存空间（default：SPARK_HOME/work） SPARK_WORKER_OPTS 仅用于 worker 的配置属性，格式是 “-Dx=y”（默认：none）。所有属性可以参考官方文档：spark-standalone-mode SPARK_DAEMON_MEMORY 分配给 spark master 和 worker 守护进程的内存。（默认： 1G） SPARK_DAEMON_JAVA_OPTS spark master 和 worker 守护进程的 JVM 选项，格式是 “-Dx=y”（默认：none） SPARK_PUBLIC_DNS spark master 和 worker 的公开 DNS 名称。（默认：none） 三、Spark on Yarn模式Spark 支持将作业提交到 Yarn 上运行，此时不需要启动 Master 节点，也不需要启动 Worker 节点。 3.1 配置在 spark-env.sh 中配置 hadoop 的配置目录的位置，可以使用 YARN_CONF_DIR 或 HADOOP_CONF_DIR 进行指定： 123YARN_CONF_DIR=/usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop# JDK安装位置JAVA_HOME=/usr/java/jdk1.8.0_201 3.2 启动必须要保证 Hadoop 已经启动，这里包括 YARN 和 HDFS 都需要启动，因为在计算过程中 Spark 会使用 HDFS 存储临时文件，如果 HDFS 没有启动，则会抛出异常。 12# start-yarn.sh# start-dfs.sh 3.3 提交应用12345678910111213141516171819# 以client模式提交到yarn集群 spark-submit \--class org.apache.spark.examples.SparkPi \--master yarn \--deploy-mode client \--executor-memory 2G \--num-executors 10 \/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \100# 以cluster模式提交到yarn集群 spark-submit \--class org.apache.spark.examples.SparkPi \--master yarn \--deploy-mode cluster \--executor-memory 2G \--num-executors 10 \/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \100]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkSQL常用聚合函数]]></title>
    <url>%2F2019%2F11%2F07%2FSparkSQL%E5%B8%B8%E7%94%A8%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[聚合函数Aggregations一、简单聚合1.1 数据准备1234567891011121314&#123;"EMPNO": 7369,"ENAME": "SMITH","JOB": "CLERK","MGR": 7902,"HIREDATE": "1980-12-17 00:00:00","SAL": 800.00,"COMM": null,"DEPTNO": 20&#125;&#123;"EMPNO": 7499,"ENAME": "ALLEN","JOB": "SALESMAN","MGR": 7698,"HIREDATE": "1981-02-20 00:00:00","SAL": 1600.00,"COMM": 300.00,"DEPTNO": 30&#125;&#123;"EMPNO": 7521,"ENAME": "WARD","JOB": "SALESMAN","MGR": 7698,"HIREDATE": "1981-02-22 00:00:00","SAL": 1250.00,"COMM": 500.00,"DEPTNO": 30&#125;&#123;"EMPNO": 7566,"ENAME": "JONES","JOB": "MANAGER","MGR": 7839,"HIREDATE": "1981-04-02 00:00:00","SAL": 2975.00,"COMM": null,"DEPTNO": 20&#125;&#123;"EMPNO": 7654,"ENAME": "MARTIN","JOB": "SALESMAN","MGR": 7698,"HIREDATE": "1981-09-28 00:00:00","SAL": 1250.00,"COMM": 1400.00,"DEPTNO": 30&#125;&#123;"EMPNO": 7698,"ENAME": "BLAKE","JOB": "MANAGER","MGR": 7839,"HIREDATE": "1981-05-01 00:00:00","SAL": 2850.00,"COMM": null,"DEPTNO": 30&#125;&#123;"EMPNO": 7782,"ENAME": "CLARK","JOB": "MANAGER","MGR": 7839,"HIREDATE": "1981-06-09 00:00:00","SAL": 2450.00,"COMM": null,"DEPTNO": 10&#125;&#123;"EMPNO": 7788,"ENAME": "SCOTT","JOB": "ANALYST","MGR": 7566,"HIREDATE": "1987-04-19 00:00:00","SAL": 1500.00,"COMM": null,"DEPTNO": 20&#125;&#123;"EMPNO": 7839,"ENAME": "KING","JOB": "PRESIDENT","MGR": null,"HIREDATE": "1981-11-17 00:00:00","SAL": 5000.00,"COMM": null,"DEPTNO": 10&#125;&#123;"EMPNO": 7844,"ENAME": "TURNER","JOB": "SALESMAN","MGR": 7698,"HIREDATE": "1981-09-08 00:00:00","SAL": 1500.00,"COMM": 0.00,"DEPTNO": 30&#125;&#123;"EMPNO": 7876,"ENAME": "ADAMS","JOB": "CLERK","MGR": 7788,"HIREDATE": "1987-05-23 00:00:00","SAL": 1100.00,"COMM": null,"DEPTNO": 20&#125;&#123;"EMPNO": 7900,"ENAME": "JAMES","JOB": "CLERK","MGR": 7698,"HIREDATE": "1981-12-03 00:00:00","SAL": 950.00,"COMM": null,"DEPTNO": 30&#125;&#123;"EMPNO": 7902,"ENAME": "FORD","JOB": "ANALYST","MGR": 7566,"HIREDATE": "1981-12-03 00:00:00","SAL": 3000.00,"COMM": null,"DEPTNO": 20&#125;&#123;"EMPNO": 7934,"ENAME": "MILLER","JOB": "CLERK","MGR": 7782,"HIREDATE": "1982-01-23 00:00:00","SAL": 1300.00,"COMM": null,"DEPTNO": 10&#125; 12345678// 需要导入 spark sql 内置的函数包import org.apache.spark.sql.functions._val spark = SparkSession.builder().appName("aggregations").master("local[2]").getOrCreate()val empDF = spark.read.json("/usr/file/json/emp.json")// 注册为临时视图，用于后面演示 SQL 查询empDF.createOrReplaceTempView("emp")empDF.show() 1.2 count12// 计算员工人数empDF.select(count("ename")).show() 1.3 countDistinct12// 计算姓名不重复的员工人数empDF.select(countDistinct("deptno")).show() 1.4 approx_count_distinct通常在使用大型数据集时，你可能关注的只是近似值而不是准确值，这时可以使用 approx_count_distinct 函数，并可以使用第二个参数指定最大允许误差。 1empDF.select(approx_count_distinct ("ename",0.1)).show() 1.5 first &amp; last获取 DataFrame 中指定列的第一个值或者最后一个值。 1empDF.select(first("ename"),last("job")).show() 1.6 min &amp; max获取 DataFrame 中指定列的最小值或者最大值。 1empDF.select(min("sal"),max("sal")).show() 1.7 sum &amp; sumDistinct求和以及求指定列所有不相同的值的和。 12empDF.select(sum("sal")).show()empDF.select(sumDistinct("sal")).show() 1.8 avg内置的求平均数的函数。 1empDF.select(avg("sal")).show() 1.9 数学函数Spark SQL 中还支持多种数学聚合函数，用于通常的数学计算，以下是一些常用的例子： 12345678// 1.计算总体方差、均方差、总体标准差、样本标准差empDF.select(var_pop("sal"), var_samp("sal"), stddev_pop("sal"), stddev_samp("sal")).show()// 2.计算偏度和峰度empDF.select(skewness("sal"), kurtosis("sal")).show()// 3. 计算两列的皮尔逊相关系数、样本协方差、总体协方差。(这里只是演示，员工编号和薪资两列实际上并没有什么关联关系)empDF.select(corr("empno", "sal"), covar_samp("empno", "sal"),covar_pop("empno", "sal")).show() 1.10 聚合数据到集合12345678scala&gt; empDF.agg(collect_set("job"), collect_list("ename")).show()输出：+--------------------+--------------------+| collect_set(job)| collect_list(ename)|+--------------------+--------------------+|[MANAGER, SALESMA...|[SMITH, ALLEN, WA...|+--------------------+--------------------+ 二、分组聚合2.1 简单分组123456789101112131415161718empDF.groupBy("deptno", "job").count().show()//等价 SQLspark.sql("SELECT deptno, job, count(*) FROM emp GROUP BY deptno, job").show()输出：+------+---------+-----+|deptno| job|count|+------+---------+-----+| 10|PRESIDENT| 1|| 30| CLERK| 1|| 10| MANAGER| 1|| 30| MANAGER| 1|| 20| CLERK| 2|| 30| SALESMAN| 4|| 20| ANALYST| 2|| 10| CLERK| 1|| 20| MANAGER| 1|+------+---------+-----+ 2.2 分组聚合1234567891011121314empDF.groupBy("deptno").agg(count("ename").alias("人数"), sum("sal").alias("总工资")).show()// 等价语法empDF.groupBy("deptno").agg("ename"-&gt;"count","sal"-&gt;"sum").show()// 等价 SQLspark.sql("SELECT deptno, count(ename) ,sum(sal) FROM emp GROUP BY deptno").show()输出：+------+----+------+|deptno|人数|总工资|+------+----+------+| 10| 3|8750.0|| 30| 6|9400.0|| 20| 5|9375.0|+------+----+------+ 三、自定义聚合函数Scala 提供了两种自定义聚合函数的方法，分别如下： 有类型的自定义聚合函数，主要适用于 DataSet； 无类型的自定义聚合函数，主要适用于 DataFrame。 以下分别使用两种方式来自定义一个求平均值的聚合函数，这里以计算员工平均工资为例。两种自定义方式分别如下： 3.1 有类型的自定义函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import org.apache.spark.sql.expressions.Aggregatorimport org.apache.spark.sql.&#123;Encoder, Encoders, SparkSession, functions&#125;// 1.定义员工类,对于可能存在 null 值的字段需要使用 Option 进行包装case class Emp(ename: String, comm: scala.Option[Double], deptno: Long, empno: Long, hiredate: String, job: String, mgr: scala.Option[Long], sal: Double)// 2.定义聚合操作的中间输出类型case class SumAndCount(var sum: Double, var count: Long)/* 3.自定义聚合函数 * @IN 聚合操作的输入类型 * @BUF reduction 操作输出值的类型 * @OUT 聚合操作的输出类型 */object MyAverage extends Aggregator[Emp, SumAndCount, Double] &#123; // 4.用于聚合操作的的初始零值 override def zero: SumAndCount = SumAndCount(0, 0) // 5.同一分区中的 reduce 操作 override def reduce(avg: SumAndCount, emp: Emp): SumAndCount = &#123; avg.sum += emp.sal avg.count += 1 avg &#125; // 6.不同分区中的 merge 操作 override def merge(avg1: SumAndCount, avg2: SumAndCount): SumAndCount = &#123; avg1.sum += avg2.sum avg1.count += avg2.count avg1 &#125; // 7.定义最终的输出类型 override def finish(reduction: SumAndCount): Double = reduction.sum / reduction.count // 8.中间类型的编码转换 override def bufferEncoder: Encoder[SumAndCount] = Encoders.product // 9.输出类型的编码转换 override def outputEncoder: Encoder[Double] = Encoders.scalaDouble&#125;object SparkSqlApp &#123; // 测试方法 def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName("Spark-SQL").master("local[2]").getOrCreate() import spark.implicits._ val ds = spark.read.json("file/emp.json").as[Emp] // 10.使用内置 avg() 函数和自定义函数分别进行计算，验证自定义函数是否正确 val myAvg = ds.select(MyAverage.toColumn.name("average_sal")).first() val avg = ds.select(functions.avg(ds.col("sal"))).first().get(0) println("自定义 average 函数 : " + myAvg) println("内置的 average 函数 : " + avg) &#125;&#125; 自定义聚合函数需要实现的方法比较多，这里以绘图的方式来演示其执行流程，以及每个方法的作用： 关于 zero,reduce,merge,finish 方法的作用在上图都有说明，这里解释一下中间类型和输出类型的编码转换，这个写法比较固定，基本上就是两种情况： 自定义类型 Case Class 或者元组就使用 Encoders.product 方法； 基本类型就使用其对应名称的方法，如 scalaByte，scalaFloat，scalaShort 等，示例如下： 12override def bufferEncoder: Encoder[SumAndCount] = Encoders.productoverride def outputEncoder: Encoder[Double] = Encoders.scalaDouble 3.2 无类型的自定义聚合函数理解了有类型的自定义聚合函数后，无类型的定义方式也基本相同，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;import org.apache.spark.sql.types._import org.apache.spark.sql.&#123;Row, SparkSession&#125;object MyAverage extends UserDefinedAggregateFunction &#123; // 1.聚合操作输入参数的类型,字段名称可以自定义 def inputSchema: StructType = StructType(StructField("MyInputColumn", LongType) :: Nil) // 2.聚合操作中间值的类型,字段名称可以自定义 def bufferSchema: StructType = &#123; StructType(StructField("sum", LongType) :: StructField("MyCount", LongType) :: Nil) &#125; // 3.聚合操作输出参数的类型 def dataType: DataType = DoubleType // 4.此函数是否始终在相同输入上返回相同的输出,通常为 true def deterministic: Boolean = true // 5.定义零值 def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer(0) = 0L buffer(1) = 0L &#125; // 6.同一分区中的 reduce 操作 def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; if (!input.isNullAt(0)) &#123; buffer(0) = buffer.getLong(0) + input.getLong(0) buffer(1) = buffer.getLong(1) + 1 &#125; &#125; // 7.不同分区中的 merge 操作 def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0) buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1) &#125; // 8.计算最终的输出值 def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)&#125;object SparkSqlApp &#123; // 测试方法 def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName("Spark-SQL").master("local[2]").getOrCreate() // 9.注册自定义的聚合函数 spark.udf.register("myAverage", MyAverage) val df = spark.read.json("file/emp.json") df.createOrReplaceTempView("emp") // 10.使用自定义函数和内置函数分别进行计算 val myAvg = spark.sql("SELECT myAverage(sal) as avg_sal FROM emp").first() val avg = spark.sql("SELECT avg(sal) as avg_sal FROM emp").first() println("自定义 average 函数 : " + myAvg) println("内置的 average 函数 : " + avg) &#125;&#125; 参考资料 Matei Zaharia, Bill Chambers . Spark: The Definitive Guide[M] . 2018-02]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DataFrame和Dataset简介]]></title>
    <url>%2F2019%2F10%2F21%2FSparkSQL_Dataset%E5%92%8CDataFrame%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[DataFrame和Dataset简介一、Spark SQL简介Spark SQL 是 Spark 中的一个子模块，主要用于操作结构化数据。它具有以下特点： 能够将 SQL 查询与 Spark 程序无缝混合，允许您使用 SQL 或 DataFrame API 对结构化数据进行查询； 支持多种开发语言； 支持多达上百种的外部数据源，包括 Hive，Avro，Parquet，ORC，JSON 和 JDBC 等； 支持 HiveQL 语法以及 Hive SerDes 和 UDF，允许你访问现有的 Hive 仓库； 支持标准的 JDBC 和 ODBC 连接； 支持优化器，列式存储和代码生成等特性； 支持扩展并能保证容错。 二、DataFrame &amp; DataSet2.1 DataFrame为了支持结构化数据的处理，Spark SQL 提供了新的数据结构 DataFrame。DataFrame 是一个由具名列组成的数据集。它在概念上等同于关系数据库中的表或 R/Python 语言中的 data frame。 由于 Spark SQL 支持多种语言的开发，所以每种语言都定义了 DataFrame 的抽象，主要如下： 语言 主要抽象 Scala Dataset[T] &amp; DataFrame (Dataset[Row] 的别名) Java Dataset[T] Python DataFrame R DataFrame 2.2 DataFrame 对比 RDDsDataFrame 和 RDDs 最主要的区别在于一个面向的是结构化数据，一个面向的是非结构化数据，它们内部的数据结构如下： DataFrame 内部的有明确 Scheme 结构，即列名、列字段类型都是已知的，这带来的好处是可以减少数据读取以及更好地优化执行计划，从而保证查询效率。 DataFrame 和 RDDs 应该如何选择？ 如果你想使用函数式编程而不是 DataFrame API，则使用 RDDs； 如果你的数据是非结构化的 (比如流媒体或者字符流)，则使用 RDDs， 如果你的数据是结构化的 (如 RDBMS 中的数据) 或者半结构化的 (如日志)，出于性能上的考虑，应优先使用 DataFrame。 2.3 DataSetDataset 也是分布式的数据集合，在 Spark 1.6 版本被引入，它集成了 RDD 和 DataFrame 的优点，具备强类型的特点，同时支持 Lambda 函数，但只能在 Scala 和 Java 语言中使用。在 Spark 2.0 后，为了方便开发者，Spark 将 DataFrame 和 Dataset 的 API 融合到一起，提供了结构化的 API(Structured API)，即用户可以通过一套标准的 API 就能完成对两者的操作。 这里注意一下：DataFrame 被标记为 Untyped API，而 DataSet 被标记为 Typed API，后文会对两者做出解释。 2.4 静态类型与运行时类型安全静态类型 (Static-typing) 与运行时类型安全 (runtime type-safety) 主要表现如下: 在实际使用中，如果你用的是 Spark SQL 的查询语句，则直到运行时你才会发现有语法错误，而如果你用的是 DataFrame 和 Dataset，则在编译时就可以发现错误 (这节省了开发时间和整体代价)。DataFrame 和 Dataset 主要区别在于： 在 DataFrame 中，当你调用了 API 之外的函数，编译器就会报错，但如果你使用了一个不存在的字段名字，编译器依然无法发现。而 Dataset 的 API 都是用 Lambda 函数和 JVM 类型对象表示的，所有不匹配的类型参数在编译时就会被发现。 以上这些最终都被解释成关于类型安全图谱，对应开发中的语法和分析错误。在图谱中，Dataset 最严格，但对于开发者来说效率最高。 上面的描述可能并没有那么直观，下面的给出一个 IDEA 中代码编译的示例： 这里一个可能的疑惑是 DataFrame 明明是有确定的 Scheme 结构 (即列名、列字段类型都是已知的)，但是为什么还是无法对列名进行推断和错误判断，这是因为 DataFrame 是 Untyped 的。 2.5 Untyped &amp; Typed在上面我们介绍过 DataFrame API 被标记为 Untyped API，而 DataSet API 被标记为 Typed API。DataFrame 的 Untyped 是相对于语言或 API 层面而言，它确实有明确的 Scheme 结构，即列名，列类型都是确定的，但这些信息完全由 Spark 来维护，Spark 只会在运行时检查这些类型和指定类型是否一致。这也就是为什么在 Spark 2.0 之后，官方推荐把 DataFrame 看做是 DatSet[Row]，Row 是 Spark 中定义的一个 trait，其子类中封装了列字段的信息。 相对而言，DataSet 是 Typed 的，即强类型。如下面代码，DataSet 的类型由 Case Class(Scala) 或者 Java Bean(Java) 来明确指定的，在这里即每一行数据代表一个 Person，这些信息由 JVM 来保证正确性，所以字段名错误和类型错误在编译的时候就会被 IDE 所发现。 12case class Person(name: String, age: Long)val dataSet: Dataset[Person] = spark.read.json("people.json").as[Person] 三、DataFrame &amp; DataSet &amp; RDDs 总结这里对三者做一下简单的总结： RDDs 适合非结构化数据的处理，而 DataFrame &amp; DataSet 更适合结构化数据和半结构化的处理； DataFrame &amp; DataSet 可以通过统一的 Structured API 进行访问，而 RDDs 则更适合函数式编程的场景； 相比于 DataFrame 而言，DataSet 是强类型的 (Typed)，有着更为严格的静态类型检查； DataSets、DataFrames、SQL 的底层都依赖了 RDDs API，并对外提供结构化的访问接口。 四、Spark SQL的运行原理DataFrame、DataSet 和 Spark SQL 的实际执行流程都是相同的： 进行 DataFrame/Dataset/SQL 编程； 如果是有效的代码，即代码没有编译错误，Spark 会将其转换为一个逻辑计划； Spark 将此逻辑计划转换为物理计划，同时进行代码优化； Spark 然后在集群上执行这个物理计划 (基于 RDD 操作) 。 4.1 逻辑计划(Logical Plan)执行的第一个阶段是将用户代码转换成一个逻辑计划。它首先将用户代码转换成 unresolved logical plan(未解决的逻辑计划)，之所以这个计划是未解决的，是因为尽管您的代码在语法上是正确的，但是它引用的表或列可能不存在。 Spark 使用 analyzer(分析器) 基于 catalog(存储的所有表和 DataFrames 的信息) 进行解析。解析失败则拒绝执行，解析成功则将结果传给 Catalyst 优化器 (Catalyst Optimizer)，优化器是一组规则的集合，用于优化逻辑计划，通过谓词下推等方式进行优化，最终输出优化后的逻辑执行计划。 4.2 物理计划(Physical Plan)得到优化后的逻辑计划后，Spark 就开始了物理计划过程。 它通过生成不同的物理执行策略，并通过成本模型来比较它们，从而选择一个最优的物理计划在集群上面执行的。物理规划的输出结果是一系列的 RDDs 和转换关系 (transformations)。 4.3 执行在选择一个物理计划后，Spark 运行其 RDDs 代码，并在运行时执行进一步的优化，生成本地 Java 字节码，最后将运行结果返回给用户。 参考资料 Matei Zaharia, Bill Chambers . Spark: The Definitive Guide[M] . 2018-02 Spark SQL, DataFrames and Datasets Guide 且谈 Apache Spark 的 API 三剑客：RDD、DataFrame 和 Dataset(译文) A Tale of Three Apache Spark APIs: RDDs vs DataFrames and Datasets(原文)]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[弹性式数据集RDD]]></title>
    <url>%2F2019%2F10%2F18%2FSpark_RDD%2F</url>
    <content type="text"><![CDATA[弹性式数据集RDD一、RDD简介RDD 全称为 Resilient Distributed Datasets，是 Spark 最基本的数据抽象，它是只读的、分区记录的集合，支持并行操作，可以由外部数据集或其他 RDD 转换而来，它具有以下特性： 一个 RDD 由一个或者多个分区（Partitions）组成。对于 RDD 来说，每个分区会被一个计算任务所处理，用户可以在创建 RDD 时指定其分区个数，如果没有指定，则默认采用程序所分配到的 CPU 的核心数； RDD 拥有一个用于计算分区的函数 compute； RDD 会保存彼此间的依赖关系，RDD 的每次转换都会生成一个新的依赖关系，这种 RDD 之间的依赖关系就像流水线一样。在部分分区数据丢失后，可以通过这种依赖关系重新计算丢失的分区数据，而不是对 RDD 的所有分区进行重新计算； Key-Value 型的 RDD 还拥有 Partitioner(分区器)，用于决定数据被存储在哪个分区中，目前 Spark 中支持 HashPartitioner(按照哈希分区) 和 RangeParationer(按照范围进行分区)； 一个优先位置列表 (可选)，用于存储每个分区的优先位置 (prefered location)。对于一个 HDFS 文件来说，这个列表保存的就是每个分区所在的块的位置，按照“移动数据不如移动计算“的理念，Spark 在进行任务调度的时候，会尽可能的将计算任务分配到其所要处理数据块的存储位置。 RDD[T] 抽象类的部分相关代码如下： 1234567891011121314// 由子类实现以计算给定分区def compute(split: Partition, context: TaskContext): Iterator[T]// 获取所有分区protected def getPartitions: Array[Partition]// 获取所有依赖关系protected def getDependencies: Seq[Dependency[_]] = deps// 获取优先位置列表protected def getPreferredLocations(split: Partition): Seq[String] = Nil// 分区器 由子类重写以指定它们的分区方式@transient val partitioner: Option[Partitioner] = None 二、创建RDDRDD 有两种创建方式，分别介绍如下： 2.1 由现有集合创建这里使用 spark-shell 进行测试，启动命令如下： 1spark-shell --master local[4] 启动 spark-shell 后，程序会自动创建应用上下文，相当于执行了下面的 Scala 语句： 12val conf = new SparkConf().setAppName("Spark shell").setMaster("local[4]")val sc = new SparkContext(conf) 由现有集合创建 RDD，你可以在创建时指定其分区个数，如果没有指定，则采用程序所分配到的 CPU 的核心数： 1234567val data = Array(1, 2, 3, 4, 5)// 由现有集合创建 RDD,默认分区数为程序所分配到的 CPU 的核心数val dataRDD = sc.parallelize(data) // 查看分区数dataRDD.getNumPartitions// 明确指定分区数val dataRDD = sc.parallelize(data,2) 执行结果如下： 2.2 引用外部存储系统中的数据集引用外部存储系统中的数据集，例如本地文件系统，HDFS，HBase 或支持 Hadoop InputFormat 的任何数据源。 123val fileRDD = sc.textFile("/usr/file/emp.txt")// 获取第一行文本fileRDD.take(1) 使用外部存储系统时需要注意以下两点： 如果在集群环境下从本地文件系统读取数据，则要求该文件必须在集群中所有机器上都存在，且路径相同； 支持目录路径，支持压缩文件，支持使用通配符。 2.3 textFile &amp; wholeTextFiles两者都可以用来读取外部文件，但是返回格式是不同的： textFile：其返回格式是 RDD[String] ，返回的是就是文件内容，RDD 中每一个元素对应一行数据； wholeTextFiles：其返回格式是 RDD[(String, String)]，元组中第一个参数是文件路径，第二个参数是文件内容； 两者都提供第二个参数来控制最小分区数； 从 HDFS 上读取文件时，Spark 会为每个块创建一个分区。 12def textFile(path: String,minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123;...&#125;def wholeTextFiles(path: String,minPartitions: Int = defaultMinPartitions): RDD[(String, String)]=&#123;..&#125; 三、操作RDDRDD 支持两种类型的操作：transformations（转换，从现有数据集创建新数据集）和 actions（在数据集上运行计算后将值返回到驱动程序）。RDD 中的所有转换操作都是惰性的，它们只是记住这些转换操作，但不会立即执行，只有遇到 action 操作后才会真正的进行计算，这类似于函数式编程中的惰性求值。 1234val list = List(1, 2, 3)// map 是一个 transformations 操作，而 foreach 是一个 actions 操作sc.parallelize(list).map(_ * 10).foreach(println)// 输出： 10 20 30 四、缓存RDD4.1 缓存级别Spark 速度非常快的一个原因是 RDD 支持缓存。成功缓存后，如果之后的操作使用到了该数据集，则直接从缓存中获取。虽然缓存也有丢失的风险，但是由于 RDD 之间的依赖关系，如果某个分区的缓存数据丢失，只需要重新计算该分区即可。 Spark 支持多种缓存级别 ： Storage Level（存储级别） Meaning（含义） MEMORY_ONLY 默认的缓存级别，将 RDD 以反序列化的 Java 对象的形式存储在 JVM 中。如果内存空间不够，则部分分区数据将不再缓存。 MEMORY_AND_DISK 将 RDD 以反序列化的 Java 对象的形式存储 JVM 中。如果内存空间不够，将未缓存的分区数据存储到磁盘，在需要使用这些分区时从磁盘读取。 MEMORY_ONLY_SER 将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式比反序列化对象节省存储空间，但在读取时会增加 CPU 的计算负担。仅支持 Java 和 Scala 。 MEMORY_AND_DISK_SER 类似于 MEMORY_ONLY_SER，但是溢出的分区数据会存储到磁盘，而不是在用到它们时重新计算。仅支持 Java 和 Scala。 DISK_ONLY 只在磁盘上缓存 RDD MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc 与上面的对应级别功能相同，但是会为每个分区在集群中的两个节点上建立副本。 OFF_HEAP 与 MEMORY_ONLY_SER 类似，但将数据存储在堆外内存中。这需要启用堆外内存。 启动堆外内存需要配置两个参数： spark.memory.offHeap.enabled ：是否开启堆外内存，默认值为 false，需要设置为 true； spark.memory.offHeap.size : 堆外内存空间的大小，默认值为 0，需要设置为正值。 4.2 使用缓存缓存数据的方法有两个：persist 和 cache 。cache 内部调用的也是 persist，它是 persist 的特殊化形式，等价于 persist(StorageLevel.MEMORY_ONLY)。示例如下： 123// 所有存储级别均定义在 StorageLevel 对象中fileRDD.persist(StorageLevel.MEMORY_AND_DISK)fileRDD.cache() 4.3 移除缓存Spark 会自动监视每个节点上的缓存使用情况，并按照最近最少使用（LRU）的规则删除旧数据分区。当然，你也可以使用 RDD.unpersist() 方法进行手动删除。 五、理解shuffle5.1 shuffle介绍在 Spark 中，一个任务对应一个分区，通常不会跨分区操作数据。但如果遇到 reduceByKey 等操作，Spark 必须从所有分区读取数据，并查找所有键的所有值，然后汇总在一起以计算每个键的最终结果 ，这称为 Shuffle。 5.2 Shuffle的影响Shuffle 是一项昂贵的操作，因为它通常会跨节点操作数据，这会涉及磁盘 I/O，网络 I/O，和数据序列化。某些 Shuffle 操作还会消耗大量的堆内存，因为它们使用堆内存来临时存储需要网络传输的数据。Shuffle 还会在磁盘上生成大量中间文件，从 Spark 1.3 开始，这些文件将被保留，直到相应的 RDD 不再使用并进行垃圾回收，这样做是为了避免在计算时重复创建 Shuffle 文件。如果应用程序长期保留对这些 RDD 的引用，则垃圾回收可能在很长一段时间后才会发生，这意味着长时间运行的 Spark 作业可能会占用大量磁盘空间，通常可以使用 spark.local.dir 参数来指定这些临时文件的存储目录。 5.3 导致Shuffle的操作由于 Shuffle 操作对性能的影响比较大，所以需要特别注意使用，以下操作都会导致 Shuffle： 涉及到重新分区操作： 如 repartition 和 coalesce； 所有涉及到 ByKey 的操作：如 groupByKey 和 reduceByKey，但 countByKey 除外； 联结操作：如 cogroup 和 join。 五、宽依赖和窄依赖RDD 和它的父 RDD(s) 之间的依赖关系分为两种不同的类型： 窄依赖 (narrow dependency)：父 RDDs 的一个分区最多被子 RDDs 一个分区所依赖； 宽依赖 (wide dependency)：父 RDDs 的一个分区可以被子 RDDs 的多个子分区所依赖。 如下图，每一个方框表示一个 RDD，带有颜色的矩形表示分区： 区分这两种依赖是非常有用的： 首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）对父分区数据进行计算，例如先执行 map 操作，然后执行 filter 操作。而宽依赖则需要计算好所有父分区的数据，然后再在节点之间进行 Shuffle，这与 MapReduce 类似。 窄依赖能够更有效地进行数据恢复，因为只需重新对丢失分区的父分区进行计算，且不同节点之间可以并行计算；而对于宽依赖而言，如果数据丢失，则需要对所有父分区数据进行计算并再次 Shuffle。 六、DAG的生成RDD(s) 及其之间的依赖关系组成了 DAG(有向无环图)，DAG 定义了这些 RDD(s) 之间的 Lineage(血统) 关系，通过血统关系，如果一个 RDD 的部分或者全部计算结果丢失了，也可以重新进行计算。那么 Spark 是如何根据 DAG 来生成计算任务呢？主要是根据依赖关系的不同将 DAG 划分为不同的计算阶段 (Stage)： 对于窄依赖，由于分区的依赖关系是确定的，其转换操作可以在同一个线程执行，所以可以划分到同一个执行阶段； 对于宽依赖，由于 Shuffle 的存在，只能在父 RDD(s) 被 Shuffle 处理完成后，才能开始接下来的计算，因此遇到宽依赖就需要重新划分阶段。 参考资料 张安站 . Spark 技术内幕：深入解析 Spark 内核架构设计与实现原理[M] . 机械工业出版社 . 2015-09-01 RDD Programming Guide RDD：基于内存的集群计算容错抽象]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark简介]]></title>
    <url>%2F2019%2F10%2F12%2FSpark%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Spark简介一、简介Spark 于 2009 年诞生于加州大学伯克利分校 AMPLab，2013 年被捐赠给 Apache 软件基金会，2014 年 2 月成为 Apache 的顶级项目。相对于 MapReduce 的批处理计算，Spark 可以带来上百倍的性能提升，因此它成为继 MapReduce 之后，最为广泛使用的分布式计算框架。 二、特点Apache Spark 具有以下特点： 使用先进的 DAG 调度程序，查询优化器和物理执行引擎，以实现性能上的保证； 多语言支持，目前支持的有 Java，Scala，Python 和 R； 提供了 80 多个高级 API，可以轻松地构建应用程序； 支持批处理，流处理和复杂的业务分析； 丰富的类库支持：包括 SQL，MLlib，GraphX 和 Spark Streaming 等库，并且可以将它们无缝地进行组合； 丰富的部署模式：支持本地模式和自带的集群模式，也支持在 Hadoop，Mesos，Kubernetes 上运行； 多数据源支持：支持访问 HDFS，Alluxio，Cassandra，HBase，Hive 以及数百个其他数据源中的数据。 三、集群架构 Term（术语） Meaning（含义） Application Spark 应用程序，由集群上的一个 Driver 节点和多个 Executor 节点组成。 Driver program 主运用程序，该进程运行应用的 main() 方法并且创建 SparkContext Cluster manager 集群资源管理器（例如，Standlone Manager，Mesos，YARN） Worker node 执行计算任务的工作节点 Executor 位于工作节点上的应用进程，负责执行计算任务并且将输出数据保存到内存或者磁盘中 Task 被发送到 Executor 中的工作单元 执行过程： 用户程序创建 SparkContext 后，它会连接到集群资源管理器，集群资源管理器会为用户程序分配计算资源，并启动 Executor； Dirver 将计算程序划分为不同的执行阶段和多个 Task，之后将 Task 发送给 Executor； Executor 负责执行 Task，并将执行状态汇报给 Driver，同时也会将当前节点资源的使用情况汇报给集群资源管理器。 四、核心组件Spark 基于 Spark Core 扩展了四个核心组件，分别用于满足不同领域的计算需求。 3.1 Spark SQLSpark SQL 主要用于结构化数据的处理。其具有以下特点： 能够将 SQL 查询与 Spark 程序无缝混合，允许您使用 SQL 或 DataFrame API 对结构化数据进行查询； 支持多种数据源，包括 Hive，Avro，Parquet，ORC，JSON 和 JDBC； 支持 HiveQL 语法以及用户自定义函数 (UDF)，允许你访问现有的 Hive 仓库； 支持标准的 JDBC 和 ODBC 连接； 支持优化器，列式存储和代码生成等特性，以提高查询效率。 3.2 Spark StreamingSpark Streaming 主要用于快速构建可扩展，高吞吐量，高容错的流处理程序。支持从 HDFS，Flume，Kafka，Twitter 和 ZeroMQ 读取数据，并进行处理。 Spark Streaming 的本质是微批处理，它将数据流进行极小粒度的拆分，拆分为多个批处理，从而达到接近于流处理的效果。 3.3 MLlibMLlib 是 Spark 的机器学习库。其设计目标是使得机器学习变得简单且可扩展。它提供了以下工具： 常见的机器学习算法：如分类，回归，聚类和协同过滤； 特征化：特征提取，转换，降维和选择； 管道：用于构建，评估和调整 ML 管道的工具； 持久性：保存和加载算法，模型，管道数据； 实用工具：线性代数，统计，数据处理等。 3.4 GraphxGraphX 是 Spark 中用于图形计算和图形并行计算的新组件。在高层次上，GraphX 通过引入一个新的图形抽象来扩展 RDD(一种具有附加到每个顶点和边缘的属性的定向多重图形)。为了支持图计算，GraphX 提供了一组基本运算符（如： subgraph，joinVertices 和 aggregateMessages）以及优化后的 Pregel API。此外，GraphX 还包括越来越多的图形算法和构建器，以简化图形分析任务。]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sqoop基本使用]]></title>
    <url>%2F2019%2F10%2F02%2FSqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Sqoop基本使用一、Sqoop 基本命令1. 查看所有命令1# sqoop help 2. 查看某条命令的具体使用方法1# sqoop help 命令名 二、Sqoop 与 MySQL1. 查询MySQL所有数据库通常用于 Sqoop 与 MySQL 连通测试： 1234sqoop list-databases \--connect jdbc:mysql://hadoop001:3306/ \--username root \--password root 2. 查询指定数据库中所有数据表1234sqoop list-tables \--connect jdbc:mysql://hadoop001:3306/mysql \--username root \--password root 三、Sqoop 与 HDFS3.1 MySQL数据导入到HDFS1. 导入命令示例：导出 MySQL 数据库中的 help_keyword 表到 HDFS 的 /sqoop 目录下，如果导入目录存在则先删除再导入，使用 3 个 map tasks 并行导入。 注：help_keyword 是 MySQL 内置的一张字典表，之后的示例均使用这张表。 123456789sqoop import \--connect jdbc:mysql://hadoop001:3306/mysql \ --username root \--password root \--table help_keyword \ # 待导入的表--delete-target-dir \ # 目标目录存在则先删除--target-dir /sqoop \ # 导入的目标目录--fields-terminated-by '\t' \ # 指定导出数据的分隔符-m 3 # 指定并行执行的 map tasks 数量 日志输出如下，可以看到输入数据被平均 split 为三份，分别由三个 map task 进行处理。数据默认以表的主键列作为拆分依据，如果你的表没有主键，有以下两种方案： 添加 -- autoreset-to-one-mapper 参数，代表只启动一个 map task，即不并行执行； 若仍希望并行执行，则可以使用 --split-by &lt;column-name&gt; 指明拆分数据的参考列。 2. 导入验证1234# 查看导入后的目录hadoop fs -ls -R /sqoop# 查看导入内容hadoop fs -text /sqoop/part-m-00000 查看 HDFS 导入目录,可以看到表中数据被分为 3 部分进行存储，这是由指定的并行度决定的。 3.2 HDFS数据导出到MySQL12345678sqoop export \ --connect jdbc:mysql://hadoop001:3306/mysql \ --username root \ --password root \ --table help_keyword_from_hdfs \ # 导出数据存储在 MySQL 的 help_keyword_from_hdf 的表中 --export-dir /sqoop \ --input-fields-terminated-by '\t'\ --m 3 表必须预先创建，建表语句如下： 1CREATE TABLE help_keyword_from_hdfs LIKE help_keyword ; 四、Sqoop 与 Hive4.1 MySQL数据导入到HiveSqoop 导入数据到 Hive 是通过先将数据导入到 HDFS 上的临时目录，然后再将数据从 HDFS 上 Load 到 Hive 中，最后将临时目录删除。可以使用 target-dir 来指定临时目录。 1. 导入命令1234567891011sqoop import \ --connect jdbc:mysql://hadoop001:3306/mysql \ --username root \ --password root \ --table help_keyword \ # 待导入的表 --delete-target-dir \ # 如果临时目录存在删除 --target-dir /sqoop_hive \ # 临时目录位置 --hive-database sqoop_test \ # 导入到 Hive 的 sqoop_test 数据库，数据库需要预先创建。不指定则默认为 default 库 --hive-import \ # 导入到 Hive --hive-overwrite \ # 如果 Hive 表中有数据则覆盖，这会清除表中原有的数据，然后再写入 -m 3 # 并行度 导入到 Hive 中的 sqoop_test 数据库需要预先创建，不指定则默认使用 Hive 中的 default 库。 1234# 查看 hive 中的所有数据库hive&gt; SHOW DATABASES;# 创建 sqoop_test 数据库hive&gt; CREATE DATABASE sqoop_test; 2. 导入验证1234# 查看 sqoop_test 数据库的所有表 hive&gt; SHOW TABLES IN sqoop_test;# 查看表中数据 hive&gt; SELECT * FROM sqoop_test.help_keyword; 3. 可能出现的问题 如果执行报错 java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf，则需将 Hive 安装目录下 lib 下的 hive-exec-**.jar 放到 sqoop 的 lib 。 123[root@hadoop001 lib]# ll hive-exec-*-rw-r--r--. 1 1106 4001 19632031 11 月 13 21:45 hive-exec-1.1.0-cdh5.15.2.jar[root@hadoop001 lib]# cp hive-exec-1.1.0-cdh5.15.2.jar $&#123;SQOOP_HOME&#125;/lib 4.2 Hive 导出数据到MySQL由于 Hive 的数据是存储在 HDFS 上的，所以 Hive 导入数据到 MySQL，实际上就是 HDFS 导入数据到 MySQL。 1. 查看Hive表在HDFS的存储位置1234# 进入对应的数据库hive&gt; use sqoop_test;# 查看表信息hive&gt; desc formatted help_keyword; Location 属性为其存储位置： 这里可以查看一下这个目录，文件结构如下： 3.2 执行导出命令12345678sqoop export \ --connect jdbc:mysql://hadoop001:3306/mysql \ --username root \ --password root \ --table help_keyword_from_hive \ --export-dir /user/hive/warehouse/sqoop_test.db/help_keyword \ -input-fields-terminated-by '\001' \ # 需要注意的是 hive 中默认的分隔符为 \001 --m 3 MySQL 中的表需要预先创建： 1CREATE TABLE help_keyword_from_hive LIKE help_keyword ; 五、Sqoop 与 HBase 本小节只讲解从 RDBMS 导入数据到 HBase，因为暂时没有命令能够从 HBase 直接导出数据到 RDBMS。 5.1 MySQL导入数据到HBase1. 导入数据将 help_keyword 表中数据导入到 HBase 上的 help_keyword_hbase 表中，使用原表的主键 help_keyword_id 作为 RowKey，原表的所有列都会在 keywordInfo 列族下，目前只支持全部导入到一个列族下，不支持分别指定列族。 12345678sqoop import \ --connect jdbc:mysql://hadoop001:3306/mysql \ --username root \ --password root \ --table help_keyword \ # 待导入的表 --hbase-table help_keyword_hbase \ # hbase 表名称，表需要预先创建 --column-family keywordInfo \ # 所有列导入到 keywordInfo 列族下 --hbase-row-key help_keyword_id # 使用原表的 help_keyword_id 作为 RowKey 导入的 HBase 表需要预先创建： 123456# 查看所有表hbase&gt; list# 创建表hbase&gt; create 'help_keyword_hbase', 'keywordInfo'# 查看表信息hbase&gt; desc 'help_keyword_hbase' 2. 导入验证使用 scan 查看表数据： 六、全库导出Sqoop 支持通过 import-all-tables 命令进行全库导出到 HDFS/Hive，但需要注意有以下两个限制： 所有表必须有主键；或者使用 --autoreset-to-one-mapper，代表只启动一个 map task; 你不能使用非默认的分割列，也不能通过 WHERE 子句添加任何限制。 第二点解释得比较拗口，这里列出官方原本的说明： You must not intend to use non-default splitting column, nor impose any conditions via a WHERE clause. 全库导出到 HDFS： 1234567sqoop import-all-tables \ --connect jdbc:mysql://hadoop001:3306/数据库名 \ --username root \ --password root \ --warehouse-dir /sqoop_all \ # 每个表会单独导出到一个目录，需要用此参数指明所有目录的父目录 --fields-terminated-by '\t' \ -m 3 全库导出到 Hive： 12345678sqoop import-all-tables -Dorg.apache.sqoop.splitter.allow_text_splitter=true \ --connect jdbc:mysql://hadoop001:3306/数据库名 \ --username root \ --password root \ --hive-database sqoop_test \ # 导出到 Hive 对应的库 --hive-import \ --hive-overwrite \ -m 3 七、Sqoop 数据过滤7.1 query参数Sqoop 支持使用 query 参数定义查询 SQL，从而可以导出任何想要的结果集。使用示例如下： 12345678910111213sqoop import \ --connect jdbc:mysql://hadoop001:3306/mysql \ --username root \ --password root \ --query 'select * from help_keyword where $CONDITIONS and help_keyword_id &lt; 50' \ --delete-target-dir \ --target-dir /sqoop_hive \ --hive-database sqoop_test \ # 指定导入目标数据库 不指定则默认使用 Hive 中的 default 库 --hive-table filter_help_keyword \ # 指定导入目标表 --split-by help_keyword_id \ # 指定用于 split 的列 --hive-import \ # 导入到 Hive --hive-overwrite \ 、 -m 3 在使用 query 进行数据过滤时，需要注意以下三点： 必须用 --hive-table 指明目标表； 如果并行度 -m 不为 1 或者没有指定 --autoreset-to-one-mapper，则需要用 --split-by 指明参考列； SQL 的 where 字句必须包含 $CONDITIONS，这是固定写法，作用是动态替换。 7.2 增量导入123456789101112sqoop import \ --connect jdbc:mysql://hadoop001:3306/mysql \ --username root \ --password root \ --table help_keyword \ --target-dir /sqoop_hive \ --hive-database sqoop_test \ --incremental append \ # 指明模式 --check-column help_keyword_id \ # 指明用于增量导入的参考列 --last-value 300 \ # 指定参考列上次导入的最大值 --hive-import \ -m 3 incremental 参数有以下两个可选的选项： append：要求参考列的值必须是递增的，所有大于 last-value 的值都会被导入； lastmodified：要求参考列的值必须是 timestamp 类型，且插入数据时候要在参考列插入当前时间戳，更新数据时也要更新参考列的时间戳，所有时间晚于 last-value 的数据都会被导入。 通过上面的解释我们可以看出来，其实 Sqoop 的增量导入并没有太多神器的地方，就是依靠维护的参考列来判断哪些是增量数据。当然我们也可以使用上面介绍的 query 参数来进行手动的增量导出，这样反而更加灵活。 八、类型支持Sqoop 默认支持数据库的大多数字段类型，但是某些特殊类型是不支持的。遇到不支持的类型，程序会抛出异常 Hive does not support the SQL type for column xxx 异常，此时可以通过下面两个参数进行强制类型转换： –map-column-java&lt;mapping&gt; ：重写 SQL 到 Java 类型的映射； –map-column-hive &lt;mapping&gt; ： 重写 Hive 到 Java 类型的映射。 示例如下，将原先 id 字段强制转为 String 类型，value 字段强制转为 Integer 类型： 1$ sqoop import ... --map-column-java id=String,value=Integer 参考资料Sqoop User Guide (v1.4.7)]]></content>
      <categories>
        <category>大数据</category>
        <category>Sqoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase Java API 的基本使用]]></title>
    <url>%2F2019%2F09%2F24%2FHbase_Java_API%2F</url>
    <content type="text"><![CDATA[HBase Java API 的基本使用一、简述截至到目前 ，HBase 有两个主要的版本，分别是 1.x 和 2.x ，两个版本的 Java API 有所不同，1.x 中某些方法在 2.x 中被标识为 @deprecated 过时。 同时你使用的客户端的版本必须与服务端版本保持一致，如果用 2.x 版本的客户端代码去连接 1.x 版本的服务端，会抛出 NoSuchColumnFamilyException 等异常。 二、Java API 1.x 基本使用2.1 新建Maven工程，导入项目依赖要使用 Java API 操作 HBase，需要引入 hbase-client。这里选取的 HBase Client 的版本为 1.2.0。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt;&lt;/dependency&gt; 2.2 API 基本使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252public class HBaseUtils &#123; private static Connection connection; static &#123; Configuration configuration = HBaseConfiguration.create(); configuration.set("hbase.zookeeper.property.clientPort", "2181"); // 如果是集群 则主机名用逗号分隔 configuration.set("hbase.zookeeper.quorum", "hadoop001"); try &#123; connection = ConnectionFactory.createConnection(configuration); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 创建 HBase 表 * * @param tableName 表名 * @param columnFamilies 列族的数组 */ public static boolean createTable(String tableName, List&lt;String&gt; columnFamilies) &#123; try &#123; HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); if (admin.tableExists(tableName)) &#123; return false; &#125; HTableDescriptor tableDescriptor = new HTableDescriptor(TableName.valueOf(tableName)); columnFamilies.forEach(columnFamily -&gt; &#123; HColumnDescriptor columnDescriptor = new HColumnDescriptor(columnFamily); columnDescriptor.setMaxVersions(1); tableDescriptor.addFamily(columnDescriptor); &#125;); admin.createTable(tableDescriptor); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 删除 hBase 表 * * @param tableName 表名 */ public static boolean deleteTable(String tableName) &#123; try &#123; HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); // 删除表前需要先禁用表 admin.disableTable(tableName); admin.deleteTable(tableName); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 插入数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamilyName 列族名 * @param qualifier 列标识 * @param value 数据 */ public static boolean putRow(String tableName, String rowKey, String columnFamilyName, String qualifier, String value) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Put put = new Put(Bytes.toBytes(rowKey)); put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(qualifier), Bytes.toBytes(value)); table.put(put); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 插入数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamilyName 列族名 * @param pairList 列标识和值的集合 */ public static boolean putRow(String tableName, String rowKey, String columnFamilyName, List&lt;Pair&lt;String, String&gt;&gt; pairList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Put put = new Put(Bytes.toBytes(rowKey)); pairList.forEach(pair -&gt; put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(pair.getKey()), Bytes.toBytes(pair.getValue()))); table.put(put); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 根据 rowKey 获取指定行的数据 * * @param tableName 表名 * @param rowKey 唯一标识 */ public static Result getRow(String tableName, String rowKey) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Get get = new Get(Bytes.toBytes(rowKey)); return table.get(get); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 获取指定行指定列 (cell) 的最新版本的数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamily 列族 * @param qualifier 列标识 */ public static String getCell(String tableName, String rowKey, String columnFamily, String qualifier) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Get get = new Get(Bytes.toBytes(rowKey)); if (!get.isCheckExistenceOnly()) &#123; get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier)); Result result = table.get(get); byte[] resultValue = result.getValue(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier)); return Bytes.toString(resultValue); &#125; else &#123; return null; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索全表 * * @param tableName 表名 */ public static ResultScanner getScanner(String tableName) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索表中指定数据 * * @param tableName 表名 * @param filterList 过滤器 */ public static ResultScanner getScanner(String tableName, FilterList filterList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); scan.setFilter(filterList); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索表中指定数据 * * @param tableName 表名 * @param startRowKey 起始 RowKey * @param endRowKey 终止 RowKey * @param filterList 过滤器 */ public static ResultScanner getScanner(String tableName, String startRowKey, String endRowKey, FilterList filterList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); scan.setStartRow(Bytes.toBytes(startRowKey)); scan.setStopRow(Bytes.toBytes(endRowKey)); scan.setFilter(filterList); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 删除指定行记录 * * @param tableName 表名 * @param rowKey 唯一标识 */ public static boolean deleteRow(String tableName, String rowKey) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Delete delete = new Delete(Bytes.toBytes(rowKey)); table.delete(delete); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 删除指定行的指定列 * * @param tableName 表名 * @param rowKey 唯一标识 * @param familyName 列族 * @param qualifier 列标识 */ public static boolean deleteColumn(String tableName, String rowKey, String familyName, String qualifier) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Delete delete = new Delete(Bytes.toBytes(rowKey)); delete.addColumn(Bytes.toBytes(familyName), Bytes.toBytes(qualifier)); table.delete(delete); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125;&#125; 2.3 单元测试以单元测试的方式对上面封装的 API 进行测试。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293public class HBaseUtilsTest &#123; private static final String TABLE_NAME = "class"; private static final String TEACHER = "teacher"; private static final String STUDENT = "student"; @Test public void createTable() &#123; // 新建表 List&lt;String&gt; columnFamilies = Arrays.asList(TEACHER, STUDENT); boolean table = HBaseUtils.createTable(TABLE_NAME, columnFamilies); System.out.println("表创建结果:" + table); &#125; @Test public void insertData() &#123; List&lt;Pair&lt;String, String&gt;&gt; pairs1 = Arrays.asList(new Pair&lt;&gt;("name", "Tom"), new Pair&lt;&gt;("age", "22"), new Pair&lt;&gt;("gender", "1")); HBaseUtils.putRow(TABLE_NAME, "rowKey1", STUDENT, pairs1); List&lt;Pair&lt;String, String&gt;&gt; pairs2 = Arrays.asList(new Pair&lt;&gt;("name", "Jack"), new Pair&lt;&gt;("age", "33"), new Pair&lt;&gt;("gender", "2")); HBaseUtils.putRow(TABLE_NAME, "rowKey2", STUDENT, pairs2); List&lt;Pair&lt;String, String&gt;&gt; pairs3 = Arrays.asList(new Pair&lt;&gt;("name", "Mike"), new Pair&lt;&gt;("age", "44"), new Pair&lt;&gt;("gender", "1")); HBaseUtils.putRow(TABLE_NAME, "rowKey3", STUDENT, pairs3); &#125; @Test public void getRow() &#123; Result result = HBaseUtils.getRow(TABLE_NAME, "rowKey1"); if (result != null) &#123; System.out.println(Bytes .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes("name")))); &#125; &#125; @Test public void getCell() &#123; String cell = HBaseUtils.getCell(TABLE_NAME, "rowKey2", STUDENT, "age"); System.out.println("cell age :" + cell); &#125; @Test public void getScanner() &#123; ResultScanner scanner = HBaseUtils.getScanner(TABLE_NAME); if (scanner != null) &#123; scanner.forEach(result -&gt; System.out.println(Bytes.toString(result.getRow()) + "-&gt;" + Bytes .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes("name"))))); scanner.close(); &#125; &#125; @Test public void getScannerWithFilter() &#123; FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ALL); SingleColumnValueFilter nameFilter = new SingleColumnValueFilter(Bytes.toBytes(STUDENT), Bytes.toBytes("name"), CompareOperator.EQUAL, Bytes.toBytes("Jack")); filterList.addFilter(nameFilter); ResultScanner scanner = HBaseUtils.getScanner(TABLE_NAME, filterList); if (scanner != null) &#123; scanner.forEach(result -&gt; System.out.println(Bytes.toString(result.getRow()) + "-&gt;" + Bytes .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes("name"))))); scanner.close(); &#125; &#125; @Test public void deleteColumn() &#123; boolean b = HBaseUtils.deleteColumn(TABLE_NAME, "rowKey2", STUDENT, "age"); System.out.println("删除结果: " + b); &#125; @Test public void deleteRow() &#123; boolean b = HBaseUtils.deleteRow(TABLE_NAME, "rowKey2"); System.out.println("删除结果: " + b); &#125; @Test public void deleteTable() &#123; boolean b = HBaseUtils.deleteTable(TABLE_NAME); System.out.println("删除结果: " + b); &#125;&#125; 三、Java API 2.x 基本使用3.1 新建Maven工程，导入项目依赖这里选取的 HBase Client 的版本为最新的 2.1.4。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;2.1.4&lt;/version&gt;&lt;/dependency&gt; 3.2 API 的基本使用2.x 版本相比于 1.x 废弃了一部分方法，关于废弃的方法在源码中都会指明新的替代方法，比如，在 2.x 中创建表时：HTableDescriptor 和 HColumnDescriptor 等类都标识为废弃，取而代之的是使用 TableDescriptorBuilder 和 ColumnFamilyDescriptorBuilder 来定义表和列族。 以下为 HBase 2.x 版本 Java API 的使用示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253public class HBaseUtils &#123; private static Connection connection; static &#123; Configuration configuration = HBaseConfiguration.create(); configuration.set("hbase.zookeeper.property.clientPort", "2181"); // 如果是集群 则主机名用逗号分隔 configuration.set("hbase.zookeeper.quorum", "hadoop001"); try &#123; connection = ConnectionFactory.createConnection(configuration); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 创建 HBase 表 * * @param tableName 表名 * @param columnFamilies 列族的数组 */ public static boolean createTable(String tableName, List&lt;String&gt; columnFamilies) &#123; try &#123; HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); if (admin.tableExists(TableName.valueOf(tableName))) &#123; return false; &#125; TableDescriptorBuilder tableDescriptor = TableDescriptorBuilder.newBuilder(TableName.valueOf(tableName)); columnFamilies.forEach(columnFamily -&gt; &#123; ColumnFamilyDescriptorBuilder cfDescriptorBuilder = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(columnFamily)); cfDescriptorBuilder.setMaxVersions(1); ColumnFamilyDescriptor familyDescriptor = cfDescriptorBuilder.build(); tableDescriptor.setColumnFamily(familyDescriptor); &#125;); admin.createTable(tableDescriptor.build()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 删除 hBase 表 * * @param tableName 表名 */ public static boolean deleteTable(String tableName) &#123; try &#123; HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); // 删除表前需要先禁用表 admin.disableTable(TableName.valueOf(tableName)); admin.deleteTable(TableName.valueOf(tableName)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 插入数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamilyName 列族名 * @param qualifier 列标识 * @param value 数据 */ public static boolean putRow(String tableName, String rowKey, String columnFamilyName, String qualifier, String value) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Put put = new Put(Bytes.toBytes(rowKey)); put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(qualifier), Bytes.toBytes(value)); table.put(put); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 插入数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamilyName 列族名 * @param pairList 列标识和值的集合 */ public static boolean putRow(String tableName, String rowKey, String columnFamilyName, List&lt;Pair&lt;String, String&gt;&gt; pairList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Put put = new Put(Bytes.toBytes(rowKey)); pairList.forEach(pair -&gt; put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(pair.getKey()), Bytes.toBytes(pair.getValue()))); table.put(put); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 根据 rowKey 获取指定行的数据 * * @param tableName 表名 * @param rowKey 唯一标识 */ public static Result getRow(String tableName, String rowKey) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Get get = new Get(Bytes.toBytes(rowKey)); return table.get(get); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 获取指定行指定列 (cell) 的最新版本的数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamily 列族 * @param qualifier 列标识 */ public static String getCell(String tableName, String rowKey, String columnFamily, String qualifier) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Get get = new Get(Bytes.toBytes(rowKey)); if (!get.isCheckExistenceOnly()) &#123; get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier)); Result result = table.get(get); byte[] resultValue = result.getValue(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier)); return Bytes.toString(resultValue); &#125; else &#123; return null; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索全表 * * @param tableName 表名 */ public static ResultScanner getScanner(String tableName) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索表中指定数据 * * @param tableName 表名 * @param filterList 过滤器 */ public static ResultScanner getScanner(String tableName, FilterList filterList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); scan.setFilter(filterList); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索表中指定数据 * * @param tableName 表名 * @param startRowKey 起始 RowKey * @param endRowKey 终止 RowKey * @param filterList 过滤器 */ public static ResultScanner getScanner(String tableName, String startRowKey, String endRowKey, FilterList filterList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); scan.withStartRow(Bytes.toBytes(startRowKey)); scan.withStopRow(Bytes.toBytes(endRowKey)); scan.setFilter(filterList); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 删除指定行记录 * * @param tableName 表名 * @param rowKey 唯一标识 */ public static boolean deleteRow(String tableName, String rowKey) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Delete delete = new Delete(Bytes.toBytes(rowKey)); table.delete(delete); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 删除指定行指定列 * * @param tableName 表名 * @param rowKey 唯一标识 * @param familyName 列族 * @param qualifier 列标识 */ public static boolean deleteColumn(String tableName, String rowKey, String familyName, String qualifier) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Delete delete = new Delete(Bytes.toBytes(rowKey)); delete.addColumn(Bytes.toBytes(familyName), Bytes.toBytes(qualifier)); table.delete(delete); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125;&#125; 四、正确连接Hbase在上面的代码中，在类加载时就初始化了 Connection 连接，并且之后的方法都是复用这个 Connection，这时我们可能会考虑是否可以使用自定义连接池来获取更好的性能表现？实际上这是没有必要的。 首先官方对于 Connection 的使用说明如下： 1234567891011121314Connection Pooling For applications which require high-end multithreaded access (e.g., web-servers or application servers that may serve many application threads in a single JVM), you can pre-create a Connection, as shown in the following example:对于高并发多线程访问的应用程序（例如，在单个 JVM 中存在的为多个线程服务的 Web 服务器或应用程序服务器）， 您只需要预先创建一个 Connection。例子如下：// Create a connection to the cluster.Configuration conf = HBaseConfiguration.create();try (Connection connection = ConnectionFactory.createConnection(conf); Table table = connection.getTable(TableName.valueOf(tablename))) &#123; // use table as needed, the table returned is lightweight&#125; 之所以能这样使用，这是因为 Connection 并不是一个简单的 socket 连接，接口文档 中对 Connection 的表述是： 1234567A cluster connection encapsulating lower level individual connections to actual servers and a connection to zookeeper. Connections are instantiated through the ConnectionFactory class. The lifecycle of the connection is managed by the caller, who has to close() the connection to release the resources. Connection 是一个集群连接，封装了与多台服务器（Matser/Region Server）的底层连接以及与 zookeeper 的连接。 连接通过 ConnectionFactory 类实例化。连接的生命周期由调用者管理，调用者必须使用 close() 关闭连接以释放资源。 之所以封装这些连接，是因为 HBase 客户端需要连接三个不同的服务角色： Zookeeper ：主要用于获取 meta 表的位置信息，Master 的信息； HBase Master ：主要用于执行 HBaseAdmin 接口的一些操作，例如建表等； HBase RegionServer ：用于读、写数据。 Connection 对象和实际的 Socket 连接之间的对应关系如下图： 上面两张图片引用自博客：连接 HBase 的正确姿势 在 HBase 客户端代码中，真正对应 Socket 连接的是 RpcConnection 对象。HBase 使用 PoolMap 这种数据结构来存储客户端到 HBase 服务器之间的连接。PoolMap 的内部有一个 ConcurrentHashMap 实例，其 key 是 ConnectionId(封装了服务器地址和用户 ticket)，value 是一个 RpcConnection 对象的资源池。当 HBase 需要连接一个服务器时，首先会根据 ConnectionId 找到对应的连接池，然后从连接池中取出一个连接对象。 123456789101112@InterfaceAudience.Privatepublic class PoolMap&lt;K, V&gt; implements Map&lt;K, V&gt; &#123; private PoolType poolType; private int poolMaxSize; private Map&lt;K, Pool&lt;V&gt;&gt; pools = new ConcurrentHashMap&lt;&gt;(); public PoolMap(PoolType poolType) &#123; this.poolType = poolType; &#125; ..... HBase 中提供了三种资源池的实现，分别是 Reusable，RoundRobin 和 ThreadLocal。具体实现可以通 hbase.client.ipc.pool.type 配置项指定，默认为 Reusable。连接池的大小也可以通过 hbase.client.ipc.pool.size 配置项指定，默认为 1，即每个 Server 1 个连接。也可以通过修改配置实现： 123config.set("hbase.client.ipc.pool.type",...);config.set("hbase.client.ipc.pool.size",...);connection = ConnectionFactory.createConnection(config); 由此可以看出 HBase 中 Connection 类已经实现了对连接的管理功能，所以我们不必在 Connection 上在做额外的管理。 另外，Connection 是线程安全的，但 Table 和 Admin 却不是线程安全的，因此正确的做法是一个进程共用一个 Connection 对象，而在不同的线程中使用单独的 Table 和 Admin 对象。Table 和 Admin 的获取操作 getTable() 和 getAdmin() 都是轻量级，所以不必担心性能的消耗，同时建议在使用完成后显示的调用 close() 方法来关闭它们。 参考资料 连接 HBase 的正确姿势 Apache HBase ™ Reference Guide]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase 常用 Shell 命令]]></title>
    <url>%2F2019%2F09%2F17%2FHbase_Shell%2F</url>
    <content type="text"><![CDATA[Hbase 常用 Shell 命令一、基本命令打开 Hbase Shell： 1# hbase shell 1.1 获取帮助1234# 获取帮助help# 获取命令的详细信息help 'status' 1.2 查看服务器状态1status 1.3 查看版本信息1version 二、关于表的操作2.1 查看所有表1list 2.2 创建表 命令格式： create ‘表名称’, ‘列族名称 1’,’列族名称 2’,’列名称 N’ 12# 创建一张名为Student的表,包含基本信息（baseInfo）、学校信息（schoolInfo）两个列族create 'Student','baseInfo','schoolInfo' 2.3 查看表的基本信息 命令格式：desc ‘表名’ 1describe 'Student' 2.4 表的启用/禁用enable 和 disable 可以启用/禁用这个表,is_enabled 和 is_disabled 来检查表是否被禁用 12345678# 禁用表disable 'Student'# 检查表是否被禁用is_disabled 'Student'# 启用表enable 'Student'# 检查表是否被启用is_enabled 'Student' 2.5 检查表是否存在1exists 'Student' 2.6 删除表1234# 删除表前需要先禁用表disable 'Student'# 删除表drop 'Student' 三、增删改3.1 添加列族 命令格式： alter ‘表名’, ‘列族名’ 1alter 'Student', 'teacherInfo' 3.2 删除列族 命令格式：alter ‘表名’, {NAME =&gt; ‘列族名’, METHOD =&gt; ‘delete’} 1alter 'Student', &#123;NAME =&gt; 'teacherInfo', METHOD =&gt; 'delete'&#125; 3.3 更改列族存储版本的限制默认情况下，列族只存储一个版本的数据，如果需要存储多个版本的数据，则需要修改列族的属性。修改后可通过 desc 命令查看。 1alter 'Student',&#123;NAME=&gt;'baseInfo',VERSIONS=&gt;3&#125; 3.4 插入数据 命令格式：put ‘表名’, ‘行键’,’列族:列’,’值’ 注意：如果新增数据的行键值、列族名、列名与原有数据完全相同，则相当于更新操作 12345678910111213141516171819put 'Student', 'rowkey1','baseInfo:name','tom'put 'Student', 'rowkey1','baseInfo:birthday','1990-01-09'put 'Student', 'rowkey1','baseInfo:age','29'put 'Student', 'rowkey1','schoolInfo:name','Havard'put 'Student', 'rowkey1','schoolInfo:localtion','Boston'put 'Student', 'rowkey2','baseInfo:name','jack'put 'Student', 'rowkey2','baseInfo:birthday','1998-08-22'put 'Student', 'rowkey2','baseInfo:age','21'put 'Student', 'rowkey2','schoolInfo:name','yale'put 'Student', 'rowkey2','schoolInfo:localtion','New Haven'put 'Student', 'rowkey3','baseInfo:name','maike'put 'Student', 'rowkey3','baseInfo:birthday','1995-01-22'put 'Student', 'rowkey3','baseInfo:age','24'put 'Student', 'rowkey3','schoolInfo:name','yale'put 'Student', 'rowkey3','schoolInfo:localtion','New Haven'put 'Student', 'wrowkey4','baseInfo:name','maike-jack' 3.5 获取指定行、指定行中的列族、列的信息123456# 获取指定行中所有列的数据信息get 'Student','rowkey3'# 获取指定行中指定列族下所有列的数据信息get 'Student','rowkey3','baseInfo'# 获取指定行中指定列的数据信息get 'Student','rowkey3','baseInfo:name' 3.6 删除指定行、指定行中的列1234# 删除指定行delete 'Student','rowkey3'# 删除指定行中指定列的数据delete 'Student','rowkey3','baseInfo:name' 四、查询hbase 中访问数据有两种基本的方式： 按指定 rowkey 获取数据：get 方法； 按指定条件获取数据：scan 方法。 scan 可以设置 begin 和 end 参数来访问一个范围内所有的数据。get 本质上就是 begin 和 end 相等的一种特殊的 scan。 4.1Get查询123456# 获取指定行中所有列的数据信息get 'Student','rowkey3'# 获取指定行中指定列族下所有列的数据信息get 'Student','rowkey3','baseInfo'# 获取指定行中指定列的数据信息get 'Student','rowkey3','baseInfo:name' 4.2 查询整表数据1scan 'Student' 4.3 查询指定列簇的数据1scan 'Student', &#123;COLUMN=&gt;'baseInfo'&#125; 4.4 条件查询12# 查询指定列的数据scan 'Student', &#123;COLUMNS=&gt; 'baseInfo:birthday'&#125; 除了列 （COLUMNS） 修饰词外，HBase 还支持 Limit（限制查询结果行数），STARTROW（ROWKEY 起始行，会先根据这个 key 定位到 region，再向后扫描）、STOPROW(结束行)、TIMERANGE（限定时间戳范围）、VERSIONS（版本数）、和 FILTER（按条件过滤行）等。 如下代表从 rowkey2 这个 rowkey 开始，查找下两个行的最新 3 个版本的 name 列的数据： 1scan 'Student', &#123;COLUMNS=&gt; 'baseInfo:name',STARTROW =&gt; 'rowkey2',STOPROW =&gt; 'wrowkey4',LIMIT=&gt;2, VERSIONS=&gt;3&#125; 4.5 条件过滤Filter 可以设定一系列条件来进行过滤。如我们要查询值等于 24 的所有数据： 1scan 'Student', FILTER=&gt;"ValueFilter(=,'binary:24')" 值包含 yale 的所有数据： 1scan 'Student', FILTER=&gt;"ValueFilter(=,'substring:yale')" 列名中的前缀为 birth 的： 1scan 'Student', FILTER=&gt;"ColumnPrefixFilter('birth')" FILTER 中支持多个过滤条件通过括号、AND 和 OR 进行组合： 12# 列名中的前缀为birth且列值中包含1998的数据scan 'Student', FILTER=&gt;"ColumnPrefixFilter('birth') AND ValueFilter ValueFilter(=,'substring:1998')" PrefixFilter 用于对 Rowkey 的前缀进行判断： 1scan 'Student', FILTER=&gt;"PrefixFilter('wr')"]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase系统架构及数据结构]]></title>
    <url>%2F2019%2F09%2F11%2FHbase%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Hbase系统架构及数据结构一、基本概念一个典型的 Hbase Table 表如下： 1.1 Row Key (行键)Row Key 是用来检索记录的主键。想要访问 HBase Table 中的数据，只有以下三种方式： 通过指定的 Row Key 进行访问； 通过 Row Key 的 range 进行访问，即访问指定范围内的行； 进行全表扫描。 Row Key 可以是任意字符串，存储时数据按照 Row Key 的字典序进行排序。这里需要注意以下两点： 因为字典序对 Int 排序的结果是 1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…,9,91,92,93,94,95,96,97,98,99。如果你使用整型的字符串作为行键，那么为了保持整型的自然序，行键必须用 0 作左填充。 行的一次读写操作时原子性的 (不论一次读写多少列)。 1.2 Column Family（列族）HBase 表中的每个列，都归属于某个列族。列族是表的 Schema 的一部分，所以列族需要在创建表时进行定义。列族的所有列都以列族名作为前缀，例如 courses:history，courses:math 都属于 courses 这个列族。 1.3 Column Qualifier (列限定符)列限定符，你可以理解为是具体的列名，例如 courses:history，courses:math 都属于 courses 这个列族，它们的列限定符分别是 history 和 math。需要注意的是列限定符不是表 Schema 的一部分，你可以在插入数据的过程中动态创建列。 1.4 Column(列)HBase 中的列由列族和列限定符组成，它们由 :(冒号) 进行分隔，即一个完整的列名应该表述为 列族名 ：列限定符。 1.5 CellCell 是行，列族和列限定符的组合，并包含值和时间戳。你可以等价理解为关系型数据库中由指定行和指定列确定的一个单元格，但不同的是 HBase 中的一个单元格是由多个版本的数据组成的，每个版本的数据用时间戳进行区分。 1.6 Timestamp(时间戳)HBase 中通过 row key 和 column 确定的为一个存储单元称为 Cell。每个 Cell 都保存着同一份数据的多个版本。版本通过时间戳来索引，时间戳的类型是 64 位整型，时间戳可以由 HBase 在数据写入时自动赋值，也可以由客户显式指定。每个 Cell 中，不同版本的数据按照时间戳倒序排列，即最新的数据排在最前面。 二、存储结构2.1 RegionsHBase Table 中的所有行按照 Row Key 的字典序排列。HBase Tables 通过行键的范围 (row key range) 被水平切分成多个 Region, 一个 Region 包含了在 start key 和 end key 之间的所有行。 每个表一开始只有一个 Region，随着数据不断增加，Region 会不断增大，当增大到一个阀值的时候，Region 就会等分为两个新的 Region。当 Table 中的行不断增多，就会有越来越多的 Region。 Region 是 HBase 中分布式存储和负载均衡的最小单元。这意味着不同的 Region 可以分布在不同的 Region Server 上。但一个 Region 是不会拆分到多个 Server 上的。 2.2 Region ServerRegion Server 运行在 HDFS 的 DataNode 上。它具有以下组件： WAL(Write Ahead Log，预写日志)：用于存储尚未进持久化存储的数据记录，以便在发生故障时进行恢复。 BlockCache：读缓存。它将频繁读取的数据存储在内存中，如果存储不足，它将按照 最近最少使用原则 清除多余的数据。 MemStore：写缓存。它存储尚未写入磁盘的新数据，并会在数据写入磁盘之前对其进行排序。每个 Region 上的每个列族都有一个 MemStore。 HFile ：将行数据按照 Key\Values 的形式存储在文件系统上。 Region Server 存取一个子表时，会创建一个 Region 对象，然后对表的每个列族创建一个 Store 实例，每个 Store 会有 0 个或多个 StoreFile 与之对应，每个 StoreFile 则对应一个 HFile，HFile 就是实际存储在 HDFS 上的文件。 三、Hbase系统架构3.1 系统架构HBase 系统遵循 Master/Salve 架构，由三种不同类型的组件组成： Zookeeper 保证任何时候，集群中只有一个 Master； 存贮所有 Region 的寻址入口； 实时监控 Region Server 的状态，将 Region Server 的上线和下线信息实时通知给 Master； 存储 HBase 的 Schema，包括有哪些 Table，每个 Table 有哪些 Column Family 等信息。 Master 为 Region Server 分配 Region ； 负责 Region Server 的负载均衡 ； 发现失效的 Region Server 并重新分配其上的 Region； GFS 上的垃圾文件回收； 处理 Schema 的更新请求。 Region Server Region Server 负责维护 Master 分配给它的 Region ，并处理发送到 Region 上的 IO 请求； Region Server 负责切分在运行过程中变得过大的 Region。 3.2 组件间的协作 HBase 使用 ZooKeeper 作为分布式协调服务来维护集群中的服务器状态。 Zookeeper 负责维护可用服务列表，并提供服务故障通知等服务： 每个 Region Server 都会在 ZooKeeper 上创建一个临时节点，Master 通过 Zookeeper 的 Watcher 机制对节点进行监控，从而可以发现新加入的 Region Server 或故障退出的 Region Server； 所有 Masters 会竞争性地在 Zookeeper 上创建同一个临时节点，由于 Zookeeper 只能有一个同名节点，所以必然只有一个 Master 能够创建成功，此时该 Master 就是主 Master，主 Master 会定期向 Zookeeper 发送心跳。备用 Masters 则通过 Watcher 机制对主 HMaster 所在节点进行监听； 如果主 Master 未能定时发送心跳，则其持有的 Zookeeper 会话会过期，相应的临时节点也会被删除，这会触发定义在该节点上的 Watcher 事件，使得备用的 Master Servers 得到通知。所有备用的 Master Servers 在接到通知后，会再次去竞争性地创建临时节点，完成主 Master 的选举。 四、数据的读写流程简述4.1 写入数据的流程 Client 向 Region Server 提交写请求； Region Server 找到目标 Region； Region 检查数据是否与 Schema 一致； 如果客户端没有指定版本，则获取当前系统时间作为数据版本； 将更新写入 WAL Log； 将更新写入 Memstore； 判断 Memstore 存储是否已满，如果存储已满则需要 flush 为 Store Hfile 文件。 更为详细写入流程可以参考：HBase － 数据写入流程解析 4.2 读取数据的流程以下是客户端首次读写 HBase 上数据的流程： 客户端从 Zookeeper 获取 META 表所在的 Region Server； 客户端访问 META 表所在的 Region Server，从 META 表中查询到访问行键所在的 Region Server，之后客户端将缓存这些信息以及 META 表的位置； 客户端从行键所在的 Region Server 上获取数据。 如果再次读取，客户端将从缓存中获取行键所在的 Region Server。这样客户端就不需要再次查询 META 表，除非 Region 移动导致缓存失效，这样的话，则将会重新查询并更新缓存。 注：META 表是 HBase 中一张特殊的表，它保存了所有 Region 的位置信息，META 表自己的位置信息则存储在 ZooKeeper 上。 更为详细读取数据流程参考： HBase 原理－数据读取流程解析 HBase 原理－迟到的‘数据读取流程部分细节 参考资料本篇文章内容主要参考自官方文档和以下两篇博客，图片也主要引用自以下两篇博客： HBase Architectural Components Hbase 系统架构及数据结构 官方文档： Apache HBase ™ Reference Guide]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka消费者详解]]></title>
    <url>%2F2019%2F09%2F07%2FKafka%E6%B6%88%E8%B4%B9%E8%80%85%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Kafka消费者详解一、消费者和消费者群组在 Kafka 中，消费者通常是消费者群组的一部分，多个消费者群组共同读取同一个主题时，彼此之间互不影响。Kafka 之所以要引入消费者群组这个概念是因为 Kafka 消费者经常会做一些高延迟的操作，比如把数据写到数据库或 HDFS ，或者进行耗时的计算，在这些情况下，单个消费者无法跟上数据生成的速度。此时可以增加更多的消费者，让它们分担负载，分别处理部分分区的消息，这就是 Kafka 实现横向伸缩的主要手段。 需要注意的是：同一个分区只能被同一个消费者群组里面的一个消费者读取，不可能存在同一个分区被同一个消费者群里多个消费者共同读取的情况，如图： 可以看到即便消费者 Consumer5 空闲了，但是也不会去读取任何一个分区的数据，这同时也提醒我们在使用时应该合理设置消费者的数量，以免造成闲置和额外开销。 二、分区再均衡因为群组里的消费者共同读取主题的分区，所以当一个消费者被关闭或发生崩溃时，它就离开了群组，原本由它读取的分区将由群组里的其他消费者来读取。同时在主题发生变化时 ， 比如添加了新的分区，也会发生分区与消费者的重新分配，分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为再均衡。正是因为再均衡，所以消费费者群组才能保证高可用性和伸缩性。 消费者通过向群组协调器所在的 broker 发送心跳来维持它们和群组的从属关系以及它们对分区的所有权。只要消费者以正常的时间间隔发送心跳，就被认为是活跃的，说明它还在读取分区里的消息。消费者会在轮询消息或提交偏移量时发送心跳。如果消费者停止发送心跳的时间足够长，会话就会过期，群组协调器认为它已经死亡，就会触发再均衡。 三、创建Kafka消费者在创建消费者的时候以下以下三个选项是必选的： bootstrap.servers ：指定 broker 的地址清单，清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找 broker 的信息。不过建议至少要提供两个 broker 的信息作为容错； key.deserializer ：指定键的反序列化器； value.deserializer ：指定值的反序列化器。 除此之外你还需要指明你需要想订阅的主题，可以使用如下两个 API : consumer.subscribe(Collection&lt;String&gt; topics) ：指明需要订阅的主题的集合； consumer.subscribe(Pattern pattern) ：使用正则来匹配需要订阅的集合。 最后只需要通过轮询 API(poll) 向服务器定时请求数据。一旦消费者订阅了主题，轮询就会处理所有的细节，包括群组协调、分区再均衡、发送心跳和获取数据，这使得开发者只需要关注从分区返回的数据，然后进行业务处理。 示例如下： 12345678910111213141516171819202122232425String topic = "Hello-Kafka";String group = "group1";Properties props = new Properties();props.put("bootstrap.servers", "hadoop001:9092");/*指定分组 ID*/props.put("group.id", group);props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);/*订阅主题 (s)*/consumer.subscribe(Collections.singletonList(topic));try &#123; while (true) &#123; /*轮询获取数据*/ ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("topic = %s,partition = %d, key = %s, value = %s, offset = %d,\n", record.topic(), record.partition(), record.key(), record.value(), record.offset()); &#125; &#125;&#125; finally &#123; consumer.close();&#125; 三、 自动提交偏移量3.1 偏移量的重要性Kafka 的每一条消息都有一个偏移量属性，记录了其在分区中的位置，偏移量是一个单调递增的整数。消费者通过往一个叫作 ＿consumer_offset 的特殊主题发送消息，消息里包含每个分区的偏移量。 如果消费者一直处于运行状态，那么偏移量就没有什么用处。不过，如果有消费者退出或者新分区加入，此时就会触发再均衡。完成再均衡之后，每个消费者可能分配到新的分区，而不是之前处理的那个。为了能够继续之前的工作，消费者需要读取每个分区最后一次提交的偏移量，然后从偏移量指定的地方继续处理。 因为这个原因，所以如果不能正确提交偏移量，就可能会导致数据丢失或者重复出现消费，比如下面情况： 如果提交的偏移量小于客户端处理的最后一个消息的偏移量 ，那么处于两个偏移量之间的消息就会被重复消费； 如果提交的偏移量大于客户端处理的最后一个消息的偏移量，那么处于两个偏移量之间的消息将会丢失。 3.2 自动提交偏移量Kafka 支持自动提交和手动提交偏移量两种方式。这里先介绍比较简单的自动提交： 只需要将消费者的 enable.auto.commit 属性配置为 true 即可完成自动提交的配置。 此时每隔固定的时间，消费者就会把 poll() 方法接收到的最大偏移量进行提交，提交间隔由 auto.commit.interval.ms 属性进行配置，默认值是 5s。 使用自动提交是存在隐患的，假设我们使用默认的 5s 提交时间间隔，在最近一次提交之后的 3s 发生了再均衡，再均衡之后，消费者从最后一次提交的偏移量位置开始读取消息。这个时候偏移量已经落后了 3s ，所以在这 3s 内到达的消息会被重复处理。可以通过修改提交时间间隔来更频繁地提交偏移量，减小可能出现重复消息的时间窗，不过这种情况是无法完全避免的。基于这个原因，Kafka 也提供了手动提交偏移量的 API，使得用户可以更为灵活的提交偏移量。 四、手动提交偏移量用户可以通过将 enable.auto.commit 设为 false，然后手动提交偏移量。基于用户需求手动提交偏移量可以分为两大类： 手动提交当前偏移量：即手动提交当前轮询的最大偏移量； 手动提交固定偏移量：即按照业务需求，提交某一个固定的偏移量。 而按照 Kafka API，手动提交偏移量又可以分为同步提交和异步提交。 4.1 同步提交通过调用 consumer.commitSync() 来进行同步提交，不传递任何参数时提交的是当前轮询的最大偏移量。 12345678while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record); &#125; /*同步提交*/ consumer.commitSync();&#125; 如果某个提交失败，同步提交还会进行重试，这可以保证数据能够最大限度提交成功，但是同时也会降低程序的吞吐量。基于这个原因，Kafka 还提供了异步提交的 API。 4.2 异步提交异步提交可以提高程序的吞吐量，因为此时你可以尽管请求数据，而不用等待 Broker 的响应。代码如下： 1234567891011121314151617while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record); &#125; /*异步提交并定义回调*/ consumer.commitAsync(new OffsetCommitCallback() &#123; @Override public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception) &#123; if (exception != null) &#123; System.out.println("错误处理"); offsets.forEach((x, y) -&gt; System.out.printf("topic = %s,partition = %d, offset = %s \n", x.topic(), x.partition(), y.offset())); &#125; &#125; &#125;);&#125; 异步提交存在的问题是，在提交失败的时候不会进行自动重试，实际上也不能进行自动重试。假设程序同时提交了 200 和 300 的偏移量，此时 200 的偏移量失败的，但是紧随其后的 300 的偏移量成功了，此时如果重试就会存在 200 覆盖 300 偏移量的可能。同步提交就不存在这个问题，因为在同步提交的情况下，300 的提交请求必须等待服务器返回 200 提交请求的成功反馈后才会发出。基于这个原因，某些情况下，需要同时组合同步和异步两种提交方式。 注：虽然程序不能在失败时候进行自动重试，但是我们是可以手动进行重试的，你可以通过一个 Map&lt;TopicPartition, Integer&gt; offsets 来维护你提交的每个分区的偏移量，然后当失败时候，你可以判断失败的偏移量是否小于你维护的同主题同分区的最后提交的偏移量，如果小于则代表你已经提交了更大的偏移量请求，此时不需要重试，否则就可以进行手动重试。 4.3 同步加异步提交下面这种情况，在正常的轮询中使用异步提交来保证吞吐量，但是因为在最后即将要关闭消费者了，所以此时需要用同步提交来保证最大限度的提交成功。 12345678910111213141516171819try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record); &#125; // 异步提交 consumer.commitAsync(); &#125;&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; // 因为即将要关闭消费者，所以要用同步提交保证提交成功 consumer.commitSync(); &#125; finally &#123; consumer.close(); &#125;&#125; 4.4 提交特定偏移量在上面同步和异步提交的 API 中，实际上我们都没有对 commit 方法传递参数，此时默认提交的是当前轮询的最大偏移量，如果你需要提交特定的偏移量，可以调用它们的重载方法。 1234/*同步提交特定偏移量*/commitSync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets) /*异步提交特定偏移量*/ commitAsync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback) 需要注意的是，因为你可以订阅多个主题，所以 offsets 中必须要包含所有主题的每个分区的偏移量，示例代码如下： 1234567891011121314151617try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record); /*记录每个主题的每个分区的偏移量*/ TopicPartition topicPartition = new TopicPartition(record.topic(), record.partition()); OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(record.offset()+1, "no metaData"); /*TopicPartition 重写过 hashCode 和 equals 方法，所以能够保证同一主题和分区的实例不会被重复添加*/ offsets.put(topicPartition, offsetAndMetadata); &#125; /*提交特定偏移量*/ consumer.commitAsync(offsets, null); &#125;&#125; finally &#123; consumer.close();&#125; 五、监听分区再均衡因为分区再均衡会导致分区与消费者的重新划分，有时候你可能希望在再均衡前执行一些操作：比如提交已经处理但是尚未提交的偏移量，关闭数据库连接等。此时可以在订阅主题时候，调用 subscribe 的重载方法传入自定义的分区再均衡监听器。 1234 /*订阅指定集合内的所有主题*/subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) /*使用正则匹配需要订阅的主题*/ subscribe(Pattern pattern, ConsumerRebalanceListener listener) 代码示例如下： 123456789101112131415161718192021222324252627282930313233Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets = new HashMap&lt;&gt;();consumer.subscribe(Collections.singletonList(topic), new ConsumerRebalanceListener() &#123; /*该方法会在消费者停止读取消息之后，再均衡开始之前就调用*/ @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; System.out.println("再均衡即将触发"); // 提交已经处理的偏移量 consumer.commitSync(offsets); &#125; /*该方法会在重新分配分区之后，消费者开始读取消息之前被调用*/ @Override public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; &#125;&#125;);try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record); TopicPartition topicPartition = new TopicPartition(record.topic(), record.partition()); OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(record.offset() + 1, "no metaData"); /*TopicPartition 重写过 hashCode 和 equals 方法，所以能够保证同一主题和分区的实例不会被重复添加*/ offsets.put(topicPartition, offsetAndMetadata); &#125; consumer.commitAsync(offsets, null); &#125;&#125; finally &#123; consumer.close();&#125; 六 、退出轮询Kafka 提供了 consumer.wakeup() 方法用于退出轮询，它通过抛出 WakeupException 异常来跳出循环。需要注意的是，在退出线程时最好显示的调用 consumer.close() , 此时消费者会提交任何还没有提交的东西，并向群组协调器发送消息，告知自己要离开群组，接下来就会触发再均衡 ，而不需要等待会话超时。 下面的示例代码为监听控制台输出，当输入 exit 时结束轮询，关闭消费者并退出程序： 1234567891011121314151617181920212223242526272829303132/*调用 wakeup 优雅的退出*/final Thread mainThread = Thread.currentThread();new Thread(() -&gt; &#123; Scanner sc = new Scanner(System.in); while (sc.hasNext()) &#123; if ("exit".equals(sc.next())) &#123; consumer.wakeup(); try &#123; /*等待主线程完成提交偏移量、关闭消费者等操作*/ mainThread.join(); break; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;).start();try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; rd : records) &#123; System.out.printf("topic = %s,partition = %d, key = %s, value = %s, offset = %d,\n", rd.topic(), rd.partition(), rd.key(), rd.value(), rd.offset()); &#125; &#125;&#125; catch (WakeupException e) &#123; //对于 wakeup() 调用引起的 WakeupException 异常可以不必处理&#125; finally &#123; consumer.close(); System.out.println("consumer 关闭");&#125; 七、独立的消费者因为 Kafka 的设计目标是高吞吐和低延迟，所以在 Kafka 中，消费者通常都是从属于某个群组的，这是因为单个消费者的处理能力是有限的。但是某些时候你的需求可能很简单，比如可能只需要一个消费者从一个主题的所有分区或者某个特定的分区读取数据，这个时候就不需要消费者群组和再均衡了， 只需要把主题或者分区分配给消费者，然后开始读取消息井提交偏移量即可。 在这种情况下，就不需要订阅主题， 取而代之的是消费者为自己分配分区。 一个消费者可以订阅主题（井加入消费者群组），或者为自己分配分区，但不能同时做这两件事情。 分配分区的示例代码如下： 12345678910111213141516171819202122List&lt;TopicPartition&gt; partitions = new ArrayList&lt;&gt;();List&lt;PartitionInfo&gt; partitionInfos = consumer.partitionsFor(topic);/*可以指定读取哪些分区 如这里假设只读取主题的 0 分区*/for (PartitionInfo partition : partitionInfos) &#123; if (partition.partition()==0)&#123; partitions.add(new TopicPartition(partition.topic(), partition.partition())); &#125;&#125;// 为消费者指定分区consumer.assign(partitions);while (true) &#123; ConsumerRecords&lt;Integer, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;Integer, String&gt; record : records) &#123; System.out.printf("partition = %s, key = %d, value = %s\n", record.partition(), record.key(), record.value()); &#125; consumer.commitSync();&#125; 附录 : Kafka消费者可选属性1. fetch.min.byte消费者从服务器获取记录的最小字节数。如果可用的数据量小于设置值，broker 会等待有足够的可用数据时才会把它返回给消费者。 2. fetch.max.wait.msbroker 返回给消费者数据的等待时间，默认是 500ms。 3. max.partition.fetch.bytes该属性指定了服务器从每个分区返回给消费者的最大字节数，默认为 1MB。 4. session.timeout.ms消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 3s。 5. auto.offset.reset该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理： latest (默认值) ：在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的最新记录）; earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。 6. enable.auto.commit是否自动提交偏移量，默认值是 true。为了避免出现重复消费和数据丢失，可以把它设置为 false。 7. client.id客户端 id，服务器用来识别消息的来源。 8. max.poll.records单次调用 poll() 方法能够返回的记录数量。 9. receive.buffer.bytes &amp; send.buffer.byte这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群同步脚本xsync]]></title>
    <url>%2F2019%2F09%2F05%2F%E9%9B%86%E7%BE%A4%E5%90%8C%E6%AD%A5%E8%84%9A%E6%9C%ACxsync%2F</url>
    <content type="text"><![CDATA[该脚本用于在所有主机上同步文件。在 /usr/local/bin目录下,创建文件xsync，向里面添加如下代码： 12345678910111213141516171819202122232425#!/bin/bash#1 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi#2 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname#3 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir#4 获取当前用户名称user=`whoami`#5 循环for((host=102; host&lt;104; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone 使用chmod a+x xsync给文件添加执行权限]]></content>
      <categories>
        <category>集群管理</category>
      </categories>
      <tags>
        <tag>集群管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群批量命令脚本xcall]]></title>
    <url>%2F2019%2F09%2F05%2F%E9%9B%86%E7%BE%A4%E6%89%B9%E9%87%8F%E5%91%BD%E4%BB%A4%E8%84%9A%E6%9C%ACxcall%2F</url>
    <content type="text"><![CDATA[该脚本用于在所有主机上同时执行相同的命令。在 /usr/local/bin目录下,创建文件xcall，向里面添加： 1234567891011#!/bin/shpcount=$#if((pcount==0));then echo no args...; exit;fifor((host=101; host&lt;=103; host++)); do echo ==================hadoop$host================== ssh hadoop$host $@done#Note:这里的hadoop是对应自己主机名，需要做相应修改。另外，for循环中的host的边界值由自己的主机编号决定。 最后chmod a+x xcall给文件添加执行权限即可。 但是在使用xcall执行jps时，经常出现 bash: jsp: 未找到命令 需要在 /usr/local/bin目录下创建jps软连接 1`cd` `/usr/local/bin` `ln` `-s ``/opt/jdk/bin/jps` `jps`]]></content>
      <categories>
        <category>集群管理</category>
      </categories>
      <tags>
        <tag>集群管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume 整合 Kafka]]></title>
    <url>%2F2019%2F08%2F25%2FFlume%E6%95%B4%E5%90%88Kafka%2F</url>
    <content type="text"><![CDATA[Flume 整合 Kafka一、背景先说一下，为什么要使用 Flume + Kafka？ 以实时流处理项目为例，由于采集的数据量可能存在峰值和峰谷，假设是一个电商项目，那么峰值通常出现在秒杀时，这时如果直接将 Flume 聚合后的数据输入到 Storm 等分布式计算框架中，可能就会超过集群的处理能力，这时采用 Kafka 就可以起到削峰的作用。Kafka 天生为大数据场景而设计，具有高吞吐的特性，能很好地抗住峰值数据的冲击。 二、整合流程Flume 发送数据到 Kafka 上主要是通过 KafkaSink 来实现的，主要步骤如下： 1. 启动Zookeeper和Kafka这里启动一个单节点的 Kafka 作为测试： 12345# 启动ZookeeperzkServer.sh start# 启动kafkabin/kafka-server-start.sh config/server.properties 2. 创建主题创建一个主题 flume-kafka，之后 Flume 收集到的数据都会发到这个主题上： 12345678# 创建主题bin/kafka-topics.sh --create \--zookeeper hadoop001:2181 \--replication-factor 1 \--partitions 1 --topic flume-kafka# 查看创建的主题bin/kafka-topics.sh --zookeeper hadoop001:2181 --list 3. 启动kafka消费者启动一个消费者，监听我们刚才创建的 flume-kafka 主题： 1# bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic flume-kafka 4. 配置Flume新建配置文件 exec-memory-kafka.properties，文件内容如下。这里我们监听一个名为 kafka.log 的文件，当文件内容有变化时，将新增加的内容发送到 Kafka 的 flume-kafka 主题上。 123456789101112131415161718192021a1.sources = s1a1.channels = c1a1.sinks = k1 a1.sources.s1.type=execa1.sources.s1.command=tail -F /tmp/kafka.loga1.sources.s1.channels=c1 #设置Kafka接收器a1.sinks.k1.type= org.apache.flume.sink.kafka.KafkaSink#设置Kafka地址a1.sinks.k1.brokerList=hadoop001:9092#设置发送到Kafka上的主题a1.sinks.k1.topic=flume-kafka#设置序列化方式a1.sinks.k1.serializer.class=kafka.serializer.StringEncodera1.sinks.k1.channel=c1 a1.channels.c1.type=memorya1.channels.c1.capacity=10000a1.channels.c1.transactionCapacity=100 5. 启动Flume1234flume-ng agent \--conf conf \--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/exec-memory-kafka.properties \--name a1 -Dflume.root.logger=INFO,console 6. 测试向监听的 /tmp/kafka.log 文件中追加内容，查看 Kafka 消费者的输出： 可以看到 flume-kafka 主题的消费端已经收到了对应的消息：]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解Kafka副本机制]]></title>
    <url>%2F2019%2F08%2F21%2FKafka%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%88%86%E5%8C%BA%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[深入理解Kafka副本机制一、Kafka集群Kafka 使用 Zookeeper 来维护集群成员 (brokers) 的信息。每个 broker 都有一个唯一标识 broker.id，用于标识自己在集群中的身份，可以在配置文件 server.properties 中进行配置，或者由程序自动生成。下面是 Kafka brokers 集群自动创建的过程： 每一个 broker 启动的时候，它会在 Zookeeper 的 /brokers/ids 路径下创建一个 临时节点，并将自己的 broker.id 写入，从而将自身注册到集群； 当有多个 broker 时，所有 broker 会竞争性地在 Zookeeper 上创建 /controller 节点，由于 Zookeeper 上的节点不会重复，所以必然只会有一个 broker 创建成功，此时该 broker 称为 controller broker。它除了具备其他 broker 的功能外，还负责管理主题分区及其副本的状态。 当 broker 出现宕机或者主动退出从而导致其持有的 Zookeeper 会话超时时，会触发注册在 Zookeeper 上的 watcher 事件，此时 Kafka 会进行相应的容错处理；如果宕机的是 controller broker 时，还会触发新的 controller 选举。 二、副本机制为了保证高可用，kafka 的分区是多副本的，如果一个副本丢失了，那么还可以从其他副本中获取分区数据。但是这要求对应副本的数据必须是完整的，这是 Kafka 数据一致性的基础，所以才需要使用 controller broker 来进行专门的管理。下面将详解介绍 Kafka 的副本机制。 2.1 分区和副本Kafka 的主题被分为多个分区 ，分区是 Kafka 最基本的存储单位。每个分区可以有多个副本 (可以在创建主题时使用 replication-factor 参数进行指定)。其中一个副本是首领副本 (Leader replica)，所有的事件都直接发送给首领副本；其他副本是跟随者副本 (Follower replica)，需要通过复制来保持与首领副本数据一致，当首领副本不可用时，其中一个跟随者副本将成为新首领。 2.2 ISR机制每个分区都有一个 ISR(in-sync Replica) 列表，用于维护所有同步的、可用的副本。首领副本必然是同步副本，而对于跟随者副本来说，它需要满足以下条件才能被认为是同步副本： 与 Zookeeper 之间有一个活跃的会话，即必须定时向 Zookeeper 发送心跳； 在规定的时间内从首领副本那里低延迟地获取过消息。 如果副本不满足上面条件的话，就会被从 ISR 列表中移除，直到满足条件才会被再次加入。 这里给出一个主题创建的示例：使用 --replication-factor 指定副本系数为 3，创建成功后使用 --describe 命令可以看到分区 0 的有 0,1,2 三个副本，且三个副本都在 ISR 列表中，其中 1 为首领副本。 2.3 不完全的首领选举对于副本机制，在 broker 级别有一个可选的配置参数 unclean.leader.election.enable，默认值为 fasle，代表禁止不完全的首领选举。这是针对当首领副本挂掉且 ISR 中没有其他可用副本时，是否允许某个不完全同步的副本成为首领副本，这可能会导致数据丢失或者数据不一致，在某些对数据一致性要求较高的场景 (如金融领域)，这可能无法容忍的，所以其默认值为 false，如果你能够允许部分数据不一致的话，可以配置为 true。 2.4 最少同步副本ISR 机制的另外一个相关参数是 min.insync.replicas , 可以在 broker 或者主题级别进行配置，代表 ISR 列表中至少要有几个可用副本。这里假设设置为 2，那么当可用副本数量小于该值时，就认为整个分区处于不可用状态。此时客户端再向分区写入数据时候就会抛出异常 org.apache.kafka.common.errors.NotEnoughReplicasExceptoin: Messages are rejected since there are fewer in-sync replicas than required。 2.5 发送确认Kafka 在生产者上有一个可选的参数 ack，该参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入成功： acks=0 ：消息发送出去就认为已经成功了，不会等待任何来自服务器的响应； acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应； acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。 三、数据请求3.1 元数据请求机制在所有副本中，只有领导副本才能进行消息的读写处理。由于不同分区的领导副本可能在不同的 broker 上，如果某个 broker 收到了一个分区请求，但是该分区的领导副本并不在该 broker 上，那么它就会向客户端返回一个 Not a Leader for Partition 的错误响应。 为了解决这个问题，Kafka 提供了元数据请求机制。 首先集群中的每个 broker 都会缓存所有主题的分区副本信息，客户端会定期发送发送元数据请求，然后将获取的元数据进行缓存。定时刷新元数据的时间间隔可以通过为客户端配置 metadata.max.age.ms 来进行指定。有了元数据信息后，客户端就知道了领导副本所在的 broker，之后直接将读写请求发送给对应的 broker 即可。 如果在定时请求的时间间隔内发生的分区副本的选举，则意味着原来缓存的信息可能已经过时了，此时还有可能会收到 Not a Leader for Partition 的错误响应，这种情况下客户端会再次求发出元数据请求，然后刷新本地缓存，之后再去正确的 broker 上执行对应的操作，过程如下图： 3.2 数据可见性需要注意的是，并不是所有保存在分区首领上的数据都可以被客户端读取到，为了保证数据一致性，只有被所有同步副本 (ISR 中所有副本) 都保存了的数据才能被客户端读取到。 3.3 零拷贝Kafka 所有数据的写入和读取都是通过零拷贝来实现的。传统拷贝与零拷贝的区别如下： 传统模式下的四次拷贝与四次上下文切换以将磁盘文件通过网络发送为例。传统模式下，一般使用如下伪代码所示的方法先将文件数据读入内存，然后通过 Socket 将内存中的数据发送出去。 12buffer = File.readSocket.send(buffer) 这一过程实际上发生了四次数据拷贝。首先通过系统调用将文件数据读入到内核态 Buffer（DMA 拷贝），然后应用程序将内存态 Buffer 数据读入到用户态 Buffer（CPU 拷贝），接着用户程序通过 Socket 发送数据时将用户态 Buffer 数据拷贝到内核态 Buffer（CPU 拷贝），最后通过 DMA 拷贝将数据拷贝到 NIC Buffer。同时，还伴随着四次上下文切换，如下图所示： sendfile和transferTo实现零拷贝Linux 2.4+ 内核通过 sendfile 系统调用，提供了零拷贝。数据通过 DMA 拷贝到内核态 Buffer 后，直接通过 DMA 拷贝到 NIC Buffer，无需 CPU 拷贝。这也是零拷贝这一说法的来源。除了减少数据拷贝外，因为整个读文件到网络发送由一个 sendfile 调用完成，整个过程只有两次上下文切换，因此大大提高了性能。零拷贝过程如下图所示： 从具体实现来看，Kafka 的数据传输通过 TransportLayer 来完成，其子类 PlaintextTransportLayer 的 transferFrom 方法通过调用 Java NIO 中 FileChannel 的 transferTo 方法实现零拷贝，如下所示： 1234@Overridepublic long transferFrom(FileChannel fileChannel, long position, long count) throws IOException &#123; return fileChannel.transferTo(position, count, socketChannel);&#125; 注： transferTo 和 transferFrom 并不保证一定能使用零拷贝。实际上是否能使用零拷贝与操作系统相关，如果操作系统提供 sendfile 这样的零拷贝系统调用，则这两个方法会通过这样的系统调用充分利用零拷贝的优势，否则并不能通过这两个方法本身实现零拷贝。 四、物理存储4.1 分区分配在创建主题时，Kafka 会首先决定如何在 broker 间分配分区副本，它遵循以下原则： 在所有 broker 上均匀地分配分区副本； 确保分区的每个副本分布在不同的 broker 上； 如果使用了 broker.rack 参数为 broker 指定了机架信息，那么会尽可能的把每个分区的副本分配到不同机架的 broker 上，以避免一个机架不可用而导致整个分区不可用。 基于以上原因，如果你在一个单节点上创建一个 3 副本的主题，通常会抛出下面的异常： 12Error while executing topic command : org.apache.kafka.common.errors.InvalidReplicationFactor Exception: Replication factor: 3 larger than available brokers: 1. 4.2 分区数据保留规则保留数据是 Kafka 的一个基本特性， 但是 Kafka 不会一直保留数据，也不会等到所有消费者都读取了消息之后才删除消息。相反， Kafka 为每个主题配置了数据保留期限，规定数据被删除之前可以保留多长时间，或者清理数据之前可以保留的数据量大小。分别对应以下四个参数： log.retention.bytes ：删除数据前允许的最大数据量；默认值-1，代表没有限制； log.retention.ms：保存数据文件的毫秒数，如果未设置，则使用 log.retention.minutes 中的值，默认为 null； log.retention.minutes：保留数据文件的分钟数，如果未设置，则使用 log.retention.hours 中的值，默认为 null； log.retention.hours：保留数据文件的小时数，默认值为 168，也就是一周。 因为在一个大文件里查找和删除消息是很费时的，也很容易出错，所以 Kafka 把分区分成若干个片段，当前正在写入数据的片段叫作活跃片段。活动片段永远不会被删除。如果按照默认值保留数据一周，而且每天使用一个新片段，那么你就会看到，在每天使用一个新片段的同时会删除一个最老的片段，所以大部分时间该分区会有 7 个片段存在。 4.3 文件格式通常保存在磁盘上的数据格式与生产者发送过来消息格式是一样的。 如果生产者发送的是压缩过的消息，那么同一个批次的消息会被压缩在一起，被当作“包装消息”进行发送 (格式如下所示) ，然后保存到磁盘上。之后消费者读取后再自己解压这个包装消息，获取每条消息的具体信息。 参考资料 Neha Narkhede, Gwen Shapira ,Todd Palino(著) , 薛命灯 (译) . Kafka 权威指南 . 人民邮电出版社 . 2017-12-26 Kafka 高性能架构之道]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka生产者详解]]></title>
    <url>%2F2019%2F08%2F17%2FKafka%E7%94%9F%E4%BA%A7%E8%80%85%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Kafka生产者详解一、生产者发送消息的过程首先介绍一下 Kafka 生产者发送消息的过程： Kafka 会将发送消息包装为 ProducerRecord 对象， ProducerRecord 对象包含了目标主题和要发送的内容，同时还可以指定键和分区。在发送 ProducerRecord 对象前，生产者会先把键和值对象序列化成字节数组，这样它们才能够在网络上传输。 接下来，数据被传给分区器。如果之前已经在 ProducerRecord 对象里指定了分区，那么分区器就不会再做任何事情。如果没有指定分区 ，那么分区器会根据 ProducerRecord 对象的键来选择一个分区，紧接着，这条记录被添加到一个记录批次里，这个批次里的所有消息会被发送到相同的主题和分区上。有一个独立的线程负责把这些记录批次发送到相应的 broker 上。 服务器在收到这些消息时会返回一个响应。如果消息成功写入 Kafka，就返回一个 RecordMetaData 对象，它包含了主题和分区信息，以及记录在分区里的偏移量。如果写入失败，则会返回一个错误。生产者在收到错误之后会尝试重新发送消息，如果达到指定的重试次数后还没有成功，则直接抛出异常，不再重试。 二、创建生产者2.1 项目依赖本项目采用 Maven 构建，想要调用 Kafka 生产者 API，需要导入 kafka-clients 依赖，如下： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt; 2.2 创建生产者创建 Kafka 生产者时，以下三个属性是必须指定的： bootstrap.servers ：指定 broker 的地址清单，清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找 broker 的信息。不过建议至少要提供两个 broker 的信息作为容错； key.serializer ：指定键的序列化器； value.serializer ：指定值的序列化器。 创建的示例代码如下： 1234567891011121314151617181920212223public class SimpleProducer &#123; public static void main(String[] args) &#123; String topicName = "Hello-Kafka"; Properties props = new Properties(); props.put("bootstrap.servers", "hadoop001:9092"); props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); /*创建生产者*/ Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 10; i++) &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topicName, "hello" + i, "world" + i); /* 发送消息*/ producer.send(record); &#125; /*关闭生产者*/ producer.close(); &#125;&#125; 2.3 测试1. 启动KakfaKafka 的运行依赖于 zookeeper，需要预先启动，可以启动 Kafka 内置的 zookeeper，也可以启动自己安装的： 12345# zookeeper启动命令bin/zkServer.sh start# 内置zookeeper启动命令bin/zookeeper-server-start.sh config/zookeeper.properties 启动单节点 kafka 用于测试： 1# bin/kafka-server-start.sh config/server.properties 2. 创建topic12345678# 创建用于测试主题bin/kafka-topics.sh --create \ --bootstrap-server hadoop001:9092 \ --replication-factor 1 --partitions 1 \ --topic Hello-Kafka# 查看所有主题 bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092 3. 启动消费者 启动一个控制台消费者用于观察写入情况，启动命令如下： 1# bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic Hello-Kafka --from-beginning 4. 运行项目此时可以看到消费者控制台，输出如下，这里 kafka-console-consumer 只会打印出值信息，不会打印出键信息。 2.4 可能出现的问题在这里可能出现的一个问题是：生产者程序在启动后，一直处于等待状态。这通常出现在你使用默认配置启动 Kafka 的情况下，此时需要对 server.properties 文件中的 listeners 配置进行更改： 12# hadoop001 为我启动kafka服务的主机名，你可以换成自己的主机名或者ip地址listeners=PLAINTEXT://hadoop001:9092 二、发送消息上面的示例程序调用了 send 方法发送消息后没有做任何操作，在这种情况下，我们没有办法知道消息发送的结果。想要知道消息发送的结果，可以使用同步发送或者异步发送来实现。 2.1 同步发送在调用 send 方法后可以接着调用 get() 方法，send 方法的返回值是一个 Future&lt;RecordMetadata&gt;对象，RecordMetadata 里面包含了发送消息的主题、分区、偏移量等信息。改写后的代码如下： 1234567891011for (int i = 0; i &lt; 10; i++) &#123; try &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topicName, "k" + i, "world" + i); /*同步发送消息*/ RecordMetadata metadata = producer.send(record).get(); System.out.printf("topic=%s, partition=%d, offset=%s \n", metadata.topic(), metadata.partition(), metadata.offset()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125;&#125; 此时得到的输出如下：偏移量和调用次数有关，所有记录都分配到了 0 分区，这是因为在创建 Hello-Kafka 主题时候，使用 --partitions 指定其分区数为 1，即只有一个分区。 12345678910topic=Hello-Kafka, partition=0, offset=40 topic=Hello-Kafka, partition=0, offset=41 topic=Hello-Kafka, partition=0, offset=42 topic=Hello-Kafka, partition=0, offset=43 topic=Hello-Kafka, partition=0, offset=44 topic=Hello-Kafka, partition=0, offset=45 topic=Hello-Kafka, partition=0, offset=46 topic=Hello-Kafka, partition=0, offset=47 topic=Hello-Kafka, partition=0, offset=48 topic=Hello-Kafka, partition=0, offset=49 2.2 异步发送通常我们并不关心发送成功的情况，更多关注的是失败的情况，因此 Kafka 提供了异步发送和回调函数。 代码如下： 123456789101112131415for (int i = 0; i &lt; 10; i++) &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topicName, "k" + i, "world" + i); /*异步发送消息，并监听回调*/ producer.send(record, new Callback() &#123; @Override public void onCompletion(RecordMetadata metadata, Exception exception) &#123; if (exception != null) &#123; System.out.println("进行异常处理"); &#125; else &#123; System.out.printf("topic=%s, partition=%d, offset=%s \n", metadata.topic(), metadata.partition(), metadata.offset()); &#125; &#125; &#125;);&#125; 三、自定义分区器Kafka 有着默认的分区机制： 如果键值为 null， 则使用轮询 (Round Robin) 算法将消息均衡地分布到各个分区上； 如果键值不为 null，那么 Kafka 会使用内置的散列算法对键进行散列，然后分布到各个分区上。 某些情况下，你可能有着自己的分区需求，这时候可以采用自定义分区器实现。这里给出一个自定义分区器的示例： 3.1 自定义分区器12345678910111213141516171819202122232425/** * 自定义分区器 */public class CustomPartitioner implements Partitioner &#123; private int passLine; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; /*从生产者配置中获取分数线*/ passLine = (Integer) configs.get("pass.line"); &#125; @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; /*key 值为分数，当分数大于分数线时候，分配到 1 分区，否则分配到 0 分区*/ return (Integer) key &gt;= passLine ? 1 : 0; &#125; @Override public void close() &#123; System.out.println("分区器关闭"); &#125;&#125; 需要在创建生产者时指定分区器，和分区器所需要的配置参数： 1234567891011121314151617181920212223242526272829public class ProducerWithPartitioner &#123; public static void main(String[] args) &#123; String topicName = "Kafka-Partitioner-Test"; Properties props = new Properties(); props.put("bootstrap.servers", "hadoop001:9092"); props.put("key.serializer", "org.apache.kafka.common.serialization.IntegerSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); /*传递自定义分区器*/ props.put("partitioner.class", "com.heibaiying.producers.partitioners.CustomPartitioner"); /*传递分区器所需的参数*/ props.put("pass.line", 6); Producer&lt;Integer, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt;= 10; i++) &#123; String score = "score:" + i; ProducerRecord&lt;Integer, String&gt; record = new ProducerRecord&lt;&gt;(topicName, i, score); /*异步发送消息*/ producer.send(record, (metadata, exception) -&gt; System.out.printf("%s, partition=%d, \n", score, metadata.partition())); &#125; producer.close(); &#125;&#125; 3.2 测试需要创建一个至少有两个分区的主题： 1234bin/kafka-topics.sh --create \ --bootstrap-server hadoop001:9092 \ --replication-factor 1 --partitions 2 \ --topic Kafka-Partitioner-Test 此时输入如下，可以看到分数大于等于 6 分的都被分到 1 分区，而小于 6 分的都被分到了 0 分区。 123456789101112score:6, partition=1, score:7, partition=1, score:8, partition=1, score:9, partition=1, score:10, partition=1, score:0, partition=0, score:1, partition=0, score:2, partition=0, score:3, partition=0, score:4, partition=0, score:5, partition=0, 分区器关闭 四、生产者其他属性上面生产者的创建都仅指定了服务地址，键序列化器、值序列化器，实际上 Kafka 的生产者还有很多可配置属性，如下： 1. acksacks 参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入是成功的： acks=0 ： 消息发送出去就认为已经成功了，不会等待任何来自服务器的响应； acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应； acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。 2. buffer.memory设置生产者内存缓冲区的大小。 3. compression.type默认情况下，发送的消息不会被压缩。如果想要进行压缩，可以配置此参数，可选值有 snappy，gzip，lz4。 4. retries发生错误后，消息重发的次数。如果达到设定值，生产者就会放弃重试并返回错误。 5. batch.size当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。 6. linger.ms该参数制定了生产者在发送批次之前等待更多消息加入批次的时间。 7. clent.id客户端 id,服务器用来识别消息的来源。 8. max.in.flight.requests.per.connection指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量，把它设置为 1 可以保证消息是按照发送的顺序写入服务器，即使发生了重试。 9. timeout.ms, request.timeout.ms &amp; metadata.fetch.timeout.ms timeout.ms 指定了 borker 等待同步副本返回消息的确认时间； request.timeout.ms 指定了生产者在发送数据时等待服务器返回响应的时间； metadata.fetch.timeout.ms 指定了生产者在获取元数据（比如分区首领是谁）时等待服务器返回响应的时间。 10. max.block.ms指定了在调用 send() 方法或使用 partitionsFor() 方法获取元数据时生产者的阻塞时间。当生产者的发送缓冲区已满，或者没有可用的元数据时，这些方法会阻塞。在阻塞时间达到 max.block.ms 时，生产者会抛出超时异常。 11. max.request.size该参数用于控制生产者发送的请求大小。它可以指发送的单个消息的最大值，也可以指单个请求里所有消息总的大小。例如，假设这个值为 1000K ，那么可以发送的单个最大消息为 1000K ，或者生产者可以在单个请求里发送一个批次，该批次包含了 1000 个消息，每个消息大小为 1K。 12. receive.buffer.bytes &amp; send.buffer.byte这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka入门]]></title>
    <url>%2F2019%2F08%2F11%2FKafka%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Kafka入门一、简介ApacheKafka 是一个分布式的流处理平台。它具有以下特点： 支持消息的发布和订阅，类似于 RabbtMQ、ActiveMQ 等消息队列； 支持数据实时处理； 能保证消息的可靠性投递； 支持消息的持久化存储，并通过多副本分布式的存储方案来保证消息的容错； 高吞吐率，单 Broker 可以轻松处理数千个分区以及每秒百万级的消息量。 二、基本概念2.1 Messages And BatchesKafka 的基本数据单元被称为 message(消息)，为减少网络开销，提高效率，多个消息会被放入同一批次 (Batch) 中后再写入。 2.2 Topics And PartitionsKafka 的消息通过 Topics(主题) 进行分类，一个主题可以被分为若干个 Partitions(分区)，一个分区就是一个提交日志 (commit log)。消息以追加的方式写入分区，然后以先入先出的顺序读取。Kafka 通过分区来实现数据的冗余和伸缩性，分区可以分布在不同的服务器上，这意味着一个 Topic 可以横跨多个服务器，以提供比单个服务器更强大的性能。 由于一个 Topic 包含多个分区，因此无法在整个 Topic 范围内保证消息的顺序性，但可以保证消息在单个分区内的顺序性。 2.3 Producers And Consumers1. 生产者生产者负责创建消息。一般情况下，生产者在把消息均衡地分布到在主题的所有分区上，而并不关心消息会被写到哪个分区。如果我们想要把消息写到指定的分区，可以通过自定义分区器来实现。 2. 消费者消费者是消费者群组的一部分，消费者负责消费消息。消费者可以订阅一个或者多个主题，并按照消息生成的顺序来读取它们。消费者通过检查消息的偏移量 (offset) 来区分读取过的消息。偏移量是一个不断递增的数值，在创建消息时，Kafka 会把它添加到其中，在给定的分区里，每个消息的偏移量都是唯一的。消费者把每个分区最后读取的偏移量保存在 Zookeeper 或 Kafka 上，如果消费者关闭或者重启，它还可以重新获取该偏移量，以保证读取状态不会丢失。 一个分区只能被同一个消费者群组里面的一个消费者读取，但可以被不同消费者群组中所组成的多个消费者共同读取。多个消费者群组中消费者共同读取同一个主题时，彼此之间互不影响。 2.4 Brokers And Clusters一个独立的 Kafka 服务器被称为 Broker。Broker 接收来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。Broker 为消费者提供服务，对读取分区的请求做出响应，返回已经提交到磁盘的消息。 Broker 是集群 (Cluster) 的组成部分。每一个集群都会选举出一个 Broker 作为集群控制器 (Controller)，集群控制器负责管理工作，包括将分区分配给 Broker 和监控 Broker。 在集群中，一个分区 (Partition) 从属一个 Broker，该 Broker 被称为分区的首领 (Leader)。一个分区可以分配给多个 Brokers，这个时候会发生分区复制。这种复制机制为分区提供了消息冗余，如果有一个 Broker 失效，其他 Broker 可以接管领导权。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume 简介及基本使用]]></title>
    <url>%2F2019%2F08%2F02%2FFlume%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Flume 简介及基本使用一、Flume简介Apache Flume 是一个分布式，高可用的数据收集系统。它可以从不同的数据源收集数据，经过聚合后发送到存储系统中，通常用于日志数据的收集。Flume 分为 NG 和 OG (1.0 之前) 两个版本，NG 在 OG 的基础上进行了完全的重构，是目前使用最为广泛的版本。下面的介绍均以 NG 为基础。 二、Flume架构和基本概念下图为 Flume 的基本架构图： 2.1 基本架构外部数据源以特定格式向 Flume 发送 events (事件)，当 source 接收到 events 时，它将其存储到一个或多个 channel，channe 会一直保存 events 直到它被 sink 所消费。sink 的主要功能从 channel 中读取 events，并将其存入外部存储系统或转发到下一个 source，成功后再从 channel 中移除 events。 2.2 基本概念1. Event Event 是 Flume NG 数据传输的基本单元。类似于 JMS 和消息系统中的消息。一个 Event 由标题和正文组成：前者是键/值映射，后者是任意字节数组。 2. Source 数据收集组件，从外部数据源收集数据，并存储到 Channel 中。 3. Channel Channel 是源和接收器之间的管道，用于临时存储数据。可以是内存或持久化的文件系统： Memory Channel : 使用内存，优点是速度快，但数据可能会丢失 (如突然宕机)； File Channel : 使用持久化的文件系统，优点是能保证数据不丢失，但是速度慢。 4. Sink Sink 的主要功能从 Channel 中读取 Event，并将其存入外部存储系统或将其转发到下一个 Source，成功后再从 Channel 中移除 Event。 5. Agent 是一个独立的 (JVM) 进程，包含 Source、 Channel、 Sink 等组件。 2.3 组件种类Flume 中的每一个组件都提供了丰富的类型，适用于不同场景： Source 类型 ：内置了几十种类型，如 Avro Source，Thrift Source，Kafka Source，JMS Source； Sink 类型 ：HDFS Sink，Hive Sink，HBaseSinks，Avro Sink 等； Channel 类型 ：Memory Channel，JDBC Channel，Kafka Channel，File Channel 等。 对于 Flume 的使用，除非有特别的需求，否则通过组合内置的各种类型的 Source，Sink 和 Channel 就能满足大多数的需求。在 Flume 官网 上对所有类型组件的配置参数均以表格的方式做了详尽的介绍，并附有配置样例；同时不同版本的参数可能略有所不同，所以使用时建议选取官网对应版本的 User Guide 作为主要参考资料。 三、Flume架构模式Flume 支持多种架构模式，分别介绍如下 3.1 multi-agent flow Flume 支持跨越多个 Agent 的数据传递，这要求前一个 Agent 的 Sink 和下一个 Agent 的 Source 都必须是 Avro 类型，Sink 指向 Source 所在主机名 (或 IP 地址) 和端口（详细配置见下文案例三）。 3.2 Consolidation 日志收集中常常存在大量的客户端（比如分布式 web 服务），Flume 支持使用多个 Agent 分别收集日志，然后通过一个或者多个 Agent 聚合后再存储到文件系统中。 3.3 Multiplexing the flow Flume 支持从一个 Source 向多个 Channel，也就是向多个 Sink 传递事件，这个操作称之为 Fan Out(扇出)。默认情况下 Fan Out 是向所有的 Channel 复制 Event，即所有 Channel 收到的数据都是相同的。同时 Flume 也支持在 Source 上自定义一个复用选择器 (multiplexing selector) 来实现自定义的路由规则。 四、Flume配置格式Flume 配置通常需要以下两个步骤： 分别定义好 Agent 的 Sources，Sinks，Channels，然后将 Sources 和 Sinks 与通道进行绑定。需要注意的是一个 Source 可以配置多个 Channel，但一个 Sink 只能配置一个 Channel。基本格式如下： 123456789&lt;Agent&gt;.sources = &lt;Source&gt;&lt;Agent&gt;.sinks = &lt;Sink&gt;&lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt;# set channel for source&lt;Agent&gt;.sources.&lt;Source&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; ...# set channel for sink&lt;Agent&gt;.sinks.&lt;Sink&gt;.channel = &lt;Channel1&gt; 分别定义 Source，Sink，Channel 的具体属性。基本格式如下： 12345678&lt;Agent&gt;.sources.&lt;Source&gt;.&lt;someProperty&gt; = &lt;someValue&gt;# properties for channels&lt;Agent&gt;.channel.&lt;Channel&gt;.&lt;someProperty&gt; = &lt;someValue&gt;# properties for sinks&lt;Agent&gt;.sources.&lt;Sink&gt;.&lt;someProperty&gt; = &lt;someValue&gt; 五、Flume的安装部署一、前置条件Flume 需要依赖 JDK 1.8+ 二 、安装步骤2.1 下载并解压下载所需版本的 Flume，这里我下载的是 CDH 版本的 Flume。下载地址为：http://archive.cloudera.com/cdh5/cdh/5/ 12# 下载后进行解压tar -zxvf flume-ng-1.6.0-cdh5.15.2.tar.gz 2.2 配置环境变量1# vim /etc/profile 添加环境变量： 12export FLUME_HOME=/usr/app/apache-flume-1.6.0-cdh5.15.2-binexport PATH=$FLUME_HOME/bin:$PATH 使得配置的环境变量立即生效： 1# source /etc/profile 2.3 修改配置进入安装目录下的 conf/ 目录，拷贝 Flume 的环境配置模板 flume-env.sh.template： 1# cp flume-env.sh.template flume-env.sh 修改 flume-env.sh,指定 JDK 的安装路径： 12# Enviroment variables can be set here.export JAVA_HOME=/usr/java/jdk1.8.0_201 2.4 验证由于已经将 Flume 的 bin 目录配置到环境变量，直接使用以下命令验证是否配置成功： 1# flume-ng version 出现对应的版本信息则代表配置成功。 六、Flume使用案例介绍几个 Flume 的使用案例： 案例一：使用 Flume 监听文件内容变动，将新增加的内容输出到控制台。 案例二：使用 Flume 监听指定目录，将目录下新增加的文件存储到 HDFS。 案例三：使用 Avro 将本服务器收集到的日志数据发送到另外一台服务器。 6.1 案例一需求： 监听文件内容变动，将新增加的内容输出到控制台。 实现： 主要使用 Exec Source 配合 tail 命令实现。 1. 配置新建配置文件 exec-memory-logger.properties,其内容如下： 123456789101112131415161718192021#指定agent的sources,sinks,channelsa1.sources = s1 a1.sinks = k1 a1.channels = c1 #配置sources属性a1.sources.s1.type = execa1.sources.s1.command = tail -F /tmp/log.txta1.sources.s1.shell = /bin/bash -c#将sources与channels进行绑定a1.sources.s1.channels = c1 #配置sink a1.sinks.k1.type = logger#将sinks与channels进行绑定 a1.sinks.k1.channel = c1 #配置channel类型a1.channels.c1.type = memory 2. 启动 12345flume-ng agent \--conf conf \--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/exec-memory-logger.properties \--name a1 \-Dflume.root.logger=INFO,console 3. 测试向文件中追加数据： 控制台的显示： 6.2 案例二需求： 监听指定目录，将目录下新增加的文件存储到 HDFS。 实现：使用 Spooling Directory Source 和 HDFS Sink。 1. 配置1234567891011121314151617181920212223242526#指定agent的sources,sinks,channelsa1.sources = s1 a1.sinks = k1 a1.channels = c1 #配置sources属性a1.sources.s1.type =spooldir a1.sources.s1.spoolDir =/tmp/logsa1.sources.s1.basenameHeader = truea1.sources.s1.basenameHeaderKey = fileName #将sources与channels进行绑定 a1.sources.s1.channels =c1 #配置sink a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H/a1.sinks.k1.hdfs.filePrefix = %&#123;fileName&#125;#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本a1.sinks.k1.hdfs.fileType = DataStream a1.sinks.k1.hdfs.useLocalTimeStamp = true#将sinks与channels进行绑定 a1.sinks.k1.channel = c1 #配置channel类型a1.channels.c1.type = memory 2. 启动1234flume-ng agent \--conf conf \--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/spooling-memory-hdfs.properties \--name a1 -Dflume.root.logger=INFO,console 3. 测试拷贝任意文件到监听目录下，可以从日志看到文件上传到 HDFS 的路径： 1# cp log.txt logs/ 查看上传到 HDFS 上的文件内容与本地是否一致： 1# hdfs dfs -cat /flume/events/19-04-09/13/log.txt.1554788567801 6.3 案例三需求： 将本服务器收集到的数据发送到另外一台服务器。 实现：使用 avro sources 和 avro Sink 实现。 1. 配置日志收集Flume新建配置 netcat-memory-avro.properties，监听文件内容变化，然后将新的文件内容通过 avro sink 发送到 hadoop001 这台服务器的 8888 端口： 12345678910111213141516171819202122#指定agent的sources,sinks,channelsa1.sources = s1a1.sinks = k1a1.channels = c1#配置sources属性a1.sources.s1.type = execa1.sources.s1.command = tail -F /tmp/log.txta1.sources.s1.shell = /bin/bash -ca1.sources.s1.channels = c1#配置sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = hadoop001a1.sinks.k1.port = 8888a1.sinks.k1.batch-size = 1a1.sinks.k1.channel = c1#配置channel类型a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 2. 配置日志聚合Flume使用 avro source 监听 hadoop001 服务器的 8888 端口，将获取到内容输出到控制台： 1234567891011121314151617181920212223#指定agent的sources,sinks,channelsa2.sources = s2a2.sinks = k2a2.channels = c2#配置sources属性a2.sources.s2.type = avroa2.sources.s2.bind = hadoop001a2.sources.s2.port = 8888#将sources与channels进行绑定a2.sources.s2.channels = c2#配置sinka2.sinks.k2.type = logger#将sinks与channels进行绑定a2.sinks.k2.channel = c2#配置channel类型a2.channels.c2.type = memorya2.channels.c2.capacity = 1000a2.channels.c2.transactionCapacity = 100 3. 启动启动日志聚集 Flume： 1234flume-ng agent \--conf conf \--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/avro-memory-logger.properties \--name a2 -Dflume.root.logger=INFO,console 在启动日志收集 Flume: 1234flume-ng agent \--conf conf \--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/netcat-memory-avro.properties \--name a1 -Dflume.root.logger=INFO,console 这里建议按以上顺序启动，原因是 avro.source 会先与端口进行绑定，这样 avro sink 连接时才不会报无法连接的异常。但是即使不按顺序启动也是没关系的，sink 会一直重试，直至建立好连接。 4.测试向文件 tmp/log.txt 中追加内容： 可以看到已经从 8888 端口监听到内容，并成功输出到控制台：]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive CLI和Beeline命令行的基本使用]]></title>
    <url>%2F2019%2F07%2F23%2FHiveCLI%E5%92%8CBeeline%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Hive CLI和Beeline命令行的基本使用一、Hive CLI1.1 Help使用 hive -H 或者 hive --help 命令可以查看所有命令的帮助，显示如下： 12345678910111213usage: hive -d,--define &lt;key=value&gt; Variable subsitution to apply to hive commands. e.g. -d A=B or --define A=B --定义用户自定义变量 --database &lt;databasename&gt; Specify the database to use -- 指定使用的数据库 -e &lt;quoted-query-string&gt; SQL from command line -- 执行指定的 SQL -f &lt;filename&gt; SQL from files --执行 SQL 脚本 -H,--help Print help information -- 打印帮助信息 --hiveconf &lt;property=value&gt; Use value for given property --自定义配置 --hivevar &lt;key=value&gt; Variable subsitution to apply to hive --自定义变量 commands. e.g. --hivevar A=B -i &lt;filename&gt; Initialization SQL file --在进入交互模式之前运行初始化脚本 -S,--silent Silent mode in interactive shell --静默模式 -v,--verbose Verbose mode (echo executed SQL to the console) --详细模式 1.2 交互式命令行直接使用 Hive 命令，不加任何参数，即可进入交互式命令行。 1.3 执行SQL命令在不进入交互式命令行的情况下，可以使用 hive -e 执行 SQL 命令。 1hive -e 'select * from emp'; 1.4 执行SQL脚本用于执行的 sql 脚本可以在本地文件系统，也可以在 HDFS 上。 12345# 本地文件系统hive -f /usr/file/simple.sql;# HDFS文件系统hive -f hdfs://hadoop001:8020/tmp/simple.sql; 其中 simple.sql 内容如下： 1select * from emp; 1.5 配置Hive变量可以使用 --hiveconf 设置 Hive 运行时的变量。 123hive -e 'select * from emp' \--hiveconf hive.exec.scratchdir=/tmp/hive_scratch \--hiveconf mapred.reduce.tasks=4; hive.exec.scratchdir：指定 HDFS 上目录位置，用于存储不同 map/reduce 阶段的执行计划和这些阶段的中间输出结果。 1.6 配置文件启动使用 -i 可以在进入交互模式之前运行初始化脚本，相当于指定配置文件启动。 1hive -i /usr/file/hive-init.conf; 其中 hive-init.conf 的内容如下： 1set hive.exec.mode.local.auto = true; hive.exec.mode.local.auto 默认值为 false，这里设置为 true ，代表开启本地模式。 1.7 用户自定义变量--define &lt;key=value&gt; 和 --hivevar &lt;key=value&gt; 在功能上是等价的，都是用来实现自定义变量，这里给出一个示例: 定义变量： 1hive --define n=ename --hiveconf --hivevar j=job; 在查询中引用自定义变量： 1234567# 以下两条语句等价hive &gt; select $&#123;n&#125; from emp;hive &gt; select $&#123;hivevar:n&#125; from emp;# 以下两条语句等价hive &gt; select $&#123;j&#125; from emp;hive &gt; select $&#123;hivevar:j&#125; from emp; 结果如下： 二、Beeline2.1 HiveServer2Hive 内置了 HiveServer 和 HiveServer2 服务，两者都允许客户端使用多种编程语言进行连接，但是 HiveServer 不能处理多个客户端的并发请求，所以产生了 HiveServer2。 HiveServer2（HS2）允许远程客户端可以使用各种编程语言向 Hive 提交请求并检索结果，支持多客户端并发访问和身份验证。HS2 是由多个服务组成的单个进程，其包括基于 Thrift 的 Hive 服务（TCP 或 HTTP）和用于 Web UI 的 Jetty Web 服务器。 HiveServer2 拥有自己的 CLI(Beeline)，Beeline 是一个基于 SQLLine 的 JDBC 客户端。由于 HiveServer2 是 Hive 开发维护的重点 (Hive0.15 后就不再支持 hiveserver)，所以 Hive CLI 已经不推荐使用了，官方更加推荐使用 Beeline。 2.1 BeelineBeeline 拥有更多可使用参数，可以使用 beeline --help 查看，完整参数如下： 1234567891011121314151617181920212223242526272829303132333435363738394041Usage: java org.apache.hive.cli.beeline.BeeLine -u &lt;database url&gt; the JDBC URL to connect to -r reconnect to last saved connect url (in conjunction with !save) -n &lt;username&gt; the username to connect as -p &lt;password&gt; the password to connect as -d &lt;driver class&gt; the driver class to use -i &lt;init file&gt; script file for initialization -e &lt;query&gt; query that should be executed -f &lt;exec file&gt; script file that should be executed -w (or) --password-file &lt;password file&gt; the password file to read password from --hiveconf property=value Use value for given property --hivevar name=value hive variable name and value This is Hive specific settings in which variables can be set at session level and referenced in Hive commands or queries. --property-file=&lt;property-file&gt; the file to read connection properties (url, driver, user, password) from --color=[true/false] control whether color is used for display --showHeader=[true/false] show column names in query results --headerInterval=ROWS; the interval between which heades are displayed --fastConnect=[true/false] skip building table/column list for tab-completion --autoCommit=[true/false] enable/disable automatic transaction commit --verbose=[true/false] show verbose error messages and debug info --showWarnings=[true/false] display connection warnings --showNestedErrs=[true/false] display nested errors --numberFormat=[pattern] format numbers using DecimalFormat pattern --force=[true/false] continue running script even after errors --maxWidth=MAXWIDTH the maximum width of the terminal --maxColumnWidth=MAXCOLWIDTH the maximum width to use when displaying columns --silent=[true/false] be more silent --autosave=[true/false] automatically save preferences --outputformat=[table/vertical/csv2/tsv2/dsv/csv/tsv] format mode for result display --incrementalBufferRows=NUMROWS the number of rows to buffer when printing rows on stdout, defaults to 1000; only applicable if --incremental=true and --outputformat=table --truncateTable=[true/false] truncate table column when it exceeds length --delimiterForDSV=DELIMITER specify the delimiter for delimiter-separated values output format (default: |) --isolation=LEVEL set the transaction isolation level --nullemptystring=[true/false] set to true to get historic behavior of printing null as empty string --maxHistoryRows=MAXHISTORYROWS The maximum number of rows to store beeline history. --convertBinaryArrayToString=[true/false] display binary column data as string or as byte array --help display this message 2.3 常用参数在 Hive CLI 中支持的参数，Beeline 都支持，常用的参数如下。更多参数说明可以参见官方文档 Beeline Command Options 参数 说明 -u &lt;database URL&gt; 数据库地址 -n &lt;username&gt; 用户名 -p &lt;password&gt; 密码 -d &lt;driver class&gt; 驱动 (可选) -e &lt;query&gt; 执行 SQL 命令 -f &lt;file&gt; 执行 SQL 脚本 -i (or)–init &lt;file or files&gt; 在进入交互模式之前运行初始化脚本 –property-file &lt;file&gt; 指定配置文件 –hiveconf property=value 指定配置属性 –hivevar name=value 用户自定义属性，在会话级别有效 示例： 使用用户名和密码连接 Hive 1$ beeline -u jdbc:hive2://localhost:10000 -n username -p password ​ 三、Hive配置可以通过三种方式对 Hive 的相关属性进行配置，分别介绍如下： 3.1 配置文件方式一为使用配置文件，使用配置文件指定的配置是永久有效的。Hive 有以下三个可选的配置文件： hive-site.xml ：Hive 的主要配置文件； hivemetastore-site.xml： 关于元数据的配置； hiveserver2-site.xml：关于 HiveServer2 的配置。 示例如下,在 hive-site.xml 配置 hive.exec.scratchdir： 12345&lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/mydir&lt;/value&gt; &lt;description&gt;Scratch space for Hive jobs&lt;/description&gt; &lt;/property&gt; 3.2 hiveconf方式二为在启动命令行 (Hive CLI / Beeline) 的时候使用 --hiveconf 指定配置，这种方式指定的配置作用于整个 Session。 1hive --hiveconf hive.exec.scratchdir=/tmp/mydir 3.3 set方式三为在交互式环境下 (Hive CLI / Beeline)，使用 set 命令指定。这种设置的作用范围也是 Session 级别的，配置对于执行该命令后的所有命令生效。set 兼具设置参数和查看参数的功能。如下： 123456780: jdbc:hive2://hadoop001:10000&gt; set hive.exec.scratchdir=/tmp/mydir;No rows affected (0.025 seconds)0: jdbc:hive2://hadoop001:10000&gt; set hive.exec.scratchdir;+----------------------------------+--+| set |+----------------------------------+--+| hive.exec.scratchdir=/tmp/mydir |+----------------------------------+--+ 3.4 配置优先级配置的优先顺序如下 (由低到高)：hive-site.xml - &gt;hivemetastore-site.xml- &gt; hiveserver2-site.xml - &gt;-- hiveconf- &gt; set 3.5 配置参数Hive 可选的配置参数非常多，在用到时查阅官方文档即可AdminManual Configuration 参考资料 HiveServer2 Clients LanguageManual Cli AdminManual Configuration]]></content>
      <categories>
        <category>HIVE</category>
      </categories>
      <tags>
        <tag>HIVE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive分区表和分桶表]]></title>
    <url>%2F2019%2F07%2F21%2FHive%E5%88%86%E5%8C%BA%E8%A1%A8%E5%92%8C%E5%88%86%E6%A1%B6%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[Hive分区表和分桶表一、分区表1.1 概念Hive 中的表对应为 HDFS 上的指定目录，在查询数据时候，默认会对全表进行扫描，这样时间和性能的消耗都非常大。 分区为 HDFS 上表目录的子目录，数据按照分区存储在子目录中。如果查询的 where 字句的中包含分区条件，则直接从该分区去查找，而不是扫描整个表目录，合理的分区设计可以极大提高查询速度和性能。 这里说明一下分区表并 Hive 独有的概念，实际上这个概念非常常见。比如在我们常用的 Oracle 数据库中，当表中的数据量不断增大，查询数据的速度就会下降，这时也可以对表进行分区。表进行分区后，逻辑上表仍然是一张完整的表，只是将表中的数据存放到多个表空间（物理文件上），这样查询数据时，就不必要每次都扫描整张表，从而提升查询性能。 1.2 使用场景通常，在管理大规模数据集的时候都需要进行分区，比如将日志文件按天进行分区，从而保证数据细粒度的划分，使得查询性能得到提升。 1.3 创建分区表在 Hive 中可以使用 PARTITIONED BY 子句创建分区表。表可以包含一个或多个分区列，程序会为分区列中的每个不同值组合创建单独的数据目录。下面的我们创建一张雇员表作为测试： 123456789101112CREATE EXTERNAL TABLE emp_partition( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2) ) PARTITIONED BY (deptno INT) -- 按照部门编号进行分区 ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t" LOCATION '/hive/emp_partition'; 1.4 加载数据到分区表加载数据到分区表时候必须要指定数据所处的分区： 1234# 加载部门编号为20的数据到表中LOAD DATA LOCAL INPATH "/usr/file/emp20.txt" OVERWRITE INTO TABLE emp_partition PARTITION (deptno=20)# 加载部门编号为30的数据到表中LOAD DATA LOCAL INPATH "/usr/file/emp30.txt" OVERWRITE INTO TABLE emp_partition PARTITION (deptno=30) 1.5 查看分区目录这时候我们直接查看表目录，可以看到表目录下存在两个子目录，分别是 deptno=20 和 deptno=30,这就是分区目录，分区目录下才是我们加载的数据文件。 1# hadoop fs -ls hdfs://hadoop001:8020/hive/emp_partition/ 这时候当你的查询语句的 where 包含 deptno=20，则就去对应的分区目录下进行查找，而不用扫描全表。 二、分桶表1.1 简介分区提供了一个隔离数据和优化查询的可行方案，但是并非所有的数据集都可以形成合理的分区，分区的数量也不是越多越好，过多的分区条件可能会导致很多分区上没有数据。同时 Hive 会限制动态分区可以创建的最大分区数，用来避免过多分区文件对文件系统产生负担。鉴于以上原因，Hive 还提供了一种更加细粒度的数据拆分方案：分桶表 (bucket Table)。 分桶表会将指定列的值进行哈希散列，并对 bucket（桶数量）取余，然后存储到对应的 bucket（桶）中。 1.2 理解分桶表单从概念上理解分桶表可能会比较晦涩，其实和分区一样，分桶这个概念同样不是 Hive 独有的，对于 Java 开发人员而言，这可能是一个每天都会用到的概念，因为 Hive 中的分桶概念和 Java 数据结构中的 HashMap 的分桶概念是一致的。 当调用 HashMap 的 put() 方法存储数据时，程序会先对 key 值调用 hashCode() 方法计算出 hashcode，然后对数组长度取模计算出 index，最后将数据存储在数组 index 位置的链表上，链表达到一定阈值后会转换为红黑树 (JDK1.8+)。下图为 HashMap 的数据结构图： 图片引用自：HashMap vs. Hashtable 1.3 创建分桶表在 Hive 中，我们可以通过 CLUSTERED BY 指定分桶列，并通过 SORTED BY 指定桶中数据的排序参考列。下面为分桶表建表语句示例： 123456789101112CREATE EXTERNAL TABLE emp_bucket( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2), deptno INT) CLUSTERED BY(empno) SORTED BY(empno ASC) INTO 4 BUCKETS --按照员工编号散列到四个 bucket 中 ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t" LOCATION '/hive/emp_bucket'; 1.4 加载数据到分桶表这里直接使用 Load 语句向分桶表加载数据，数据时可以加载成功的，但是数据并不会分桶。 这是由于分桶的实质是对指定字段做了 hash 散列然后存放到对应文件中，这意味着向分桶表中插入数据是必然要通过 MapReduce，且 Reducer 的数量必须等于分桶的数量。由于以上原因，分桶表的数据通常只能使用 CTAS(CREATE TABLE AS SELECT) 方式插入，因为 CTAS 操作会触发 MapReduce。加载数据步骤如下： 1. 设置强制分桶1set hive.enforce.bucketing = true; --Hive 2.x 不需要这一步 在 Hive 0.x and 1.x 版本，必须使用设置 hive.enforce.bucketing = true，表示强制分桶，允许程序根据表结构自动选择正确数量的 Reducer 和 cluster by column 来进行分桶。 2. CTAS导入数据1INSERT INTO TABLE emp_bucket SELECT * FROM emp; --这里的 emp 表就是一张普通的雇员表 可以从执行日志看到 CTAS 触发 MapReduce 操作，且 Reducer 数量和建表时候指定 bucket 数量一致： 1.5 查看分桶文件bucket(桶) 本质上就是表目录下的具体文件： 三、分区表和分桶表结合使用分区表和分桶表的本质都是将数据按照不同粒度进行拆分，从而使得在查询时候不必扫描全表，只需要扫描对应的分区或分桶，从而提升查询效率。两者可以结合起来使用，从而保证表数据在不同粒度上都能得到合理的拆分。下面是 Hive 官方给出的示例： 12345678910111213CREATE TABLE page_view_bucketed( viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING ) PARTITIONED BY(dt STRING) CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' COLLECTION ITEMS TERMINATED BY '\002' MAP KEYS TERMINATED BY '\003' STORED AS SEQUENCEFILE; 此时导入数据时需要指定分区： 123INSERT OVERWRITE page_view_bucketedPARTITION (dt='2009-02-25')SELECT * FROM page_view WHERE dt='2009-02-25';]]></content>
      <categories>
        <category>HIVE</category>
      </categories>
      <tags>
        <tag>HIVE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive数据查询详解]]></title>
    <url>%2F2019%2F07%2F19%2FHive%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Hive数据查询详解一、数据准备为了演示查询操作，这里需要预先创建三张表，并加载测试数据。 1.1 员工表1234567891011121314 -- 建表语句 CREATE TABLE emp( empno INT, -- 员工表编号 ename STRING, -- 员工姓名 job STRING, -- 职位类型 mgr INT, hiredate TIMESTAMP, --雇佣日期 sal DECIMAL(7,2), --工资 comm DECIMAL(7,2), deptno INT) --部门编号 ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"; --加载数据LOAD DATA LOCAL INPATH "/usr/file/emp.txt" OVERWRITE INTO TABLE emp; 1.2 部门表12345678910-- 建表语句CREATE TABLE dept( deptno INT, --部门编号 dname STRING, --部门名称 loc STRING --部门所在的城市)ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t";--加载数据LOAD DATA LOCAL INPATH "/usr/file/dept.txt" OVERWRITE INTO TABLE dept; 1.3 分区表这里需要额外创建一张分区表，主要是为了演示分区查询： 123456789101112131415161718CREATE EXTERNAL TABLE emp_ptn( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2) ) PARTITIONED BY (deptno INT) -- 按照部门编号进行分区 ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t";--加载数据LOAD DATA LOCAL INPATH "/usr/file/emp.txt" OVERWRITE INTO TABLE emp_ptn PARTITION (deptno=20)LOAD DATA LOCAL INPATH "/usr/file/emp.txt" OVERWRITE INTO TABLE emp_ptn PARTITION (deptno=30)LOAD DATA LOCAL INPATH "/usr/file/emp.txt" OVERWRITE INTO TABLE emp_ptn PARTITION (deptno=40)LOAD DATA LOCAL INPATH "/usr/file/emp.txt" OVERWRITE INTO TABLE emp_ptn PARTITION (deptno=50) 二、单表查询2.1 SELECT12-- 查询表中全部数据SELECT * FROM emp; 2.2 WHERE12-- 查询 10 号部门中员工编号大于 7782 的员工信息 SELECT * FROM emp WHERE empno &gt; 7782 AND deptno = 10; 2.3 DISTINCTHive 支持使用 DISTINCT 关键字去重。 12-- 查询所有工作类型SELECT DISTINCT job FROM emp; 2.4 分区查询分区查询 (Partition Based Queries)，可以指定某个分区或者分区范围。 123-- 查询分区表中部门编号在[20,40]之间的员工SELECT emp_ptn.* FROM emp_ptnWHERE emp_ptn.deptno &gt;= 20 AND emp_ptn.deptno &lt;= 40; 2.5 LIMIT12-- 查询薪资最高的 5 名员工SELECT * FROM emp ORDER BY sal DESC LIMIT 5; 2.6 GROUP BYHive 支持使用 GROUP BY 进行分组聚合操作。 1234set hive.map.aggr=true;-- 查询各个部门薪酬综合SELECT deptno,SUM(sal) FROM emp GROUP BY deptno; hive.map.aggr 控制程序如何进行聚合。默认值为 false。如果设置为 true，Hive 会在 map 阶段就执行一次聚合。这可以提高聚合效率，但需要消耗更多内存。 2.7 ORDER AND SORT可以使用 ORDER BY 或者 Sort BY 对查询结果进行排序，排序字段可以是整型也可以是字符串：如果是整型，则按照大小排序；如果是字符串，则按照字典序排序。ORDER BY 和 SORT BY 的区别如下： 使用 ORDER BY 时会有一个 Reducer 对全部查询结果进行排序，可以保证数据的全局有序性； 使用 SORT BY 时只会在每个 Reducer 中进行排序，这可以保证每个 Reducer 的输出数据是有序的，但不能保证全局有序。 由于 ORDER BY 的时间可能很长，如果你设置了严格模式 (hive.mapred.mode = strict)，则其后面必须再跟一个 limit 子句。 注 ：hive.mapred.mode 默认值是 nonstrict ，也就是非严格模式。 12-- 查询员工工资，结果按照部门升序，按照工资降序排列SELECT empno, deptno, sal FROM emp ORDER BY deptno ASC, sal DESC; 2.8 HAVING可以使用 HAVING 对分组数据进行过滤。 12-- 查询工资总和大于 9000 的所有部门SELECT deptno,SUM(sal) FROM emp GROUP BY deptno HAVING SUM(sal)&gt;9000; 2.9 DISTRIBUTE BY默认情况下，MapReduce 程序会对 Map 输出结果的 Key 值进行散列，并均匀分发到所有 Reducer 上。如果想要把具有相同 Key 值的数据分发到同一个 Reducer 进行处理，这就需要使用 DISTRIBUTE BY 字句。 需要注意的是，DISTRIBUTE BY 虽然能保证具有相同 Key 值的数据分发到同一个 Reducer，但是不能保证数据在 Reducer 上是有序的。情况如下： 把以下 5 个数据发送到两个 Reducer 上进行处理： 12345k1k2k4k3k1 Reducer1 得到如下乱序数据： 123k1k2k1 Reducer2 得到数据如下： 12k4k3 如果想让 Reducer 上的数据时有序的，可以结合 SORT BY 使用 (示例如下)，或者使用下面我们将要介绍的 CLUSTER BY。 12-- 将数据按照部门分发到对应的 Reducer 上处理SELECT empno, deptno, sal FROM emp DISTRIBUTE BY deptno SORT BY deptno ASC; 2.10 CLUSTER BY如果 SORT BY 和 DISTRIBUTE BY 指定的是相同字段，且 SORT BY 排序规则是 ASC，此时可以使用 CLUSTER BY 进行替换，同时 CLUSTER BY 可以保证数据在全局是有序的。 1SELECT empno, deptno, sal FROM emp CLUSTER BY deptno ; 三、多表联结查询Hive 支持内连接，外连接，左外连接，右外连接，笛卡尔连接，这和传统数据库中的概念是一致的，可以参见下图。 需要特别强调：JOIN 语句的关联条件必须用 ON 指定，不能用 WHERE 指定，否则就会先做笛卡尔积，再过滤，这会导致你得不到预期的结果 (下面的演示会有说明)。 3.1 INNER JOIN12345678-- 查询员工编号为 7369 的员工的详细信息SELECT e.*,d.* FROM emp e JOIN dept dON e.deptno = d.deptno WHERE empno=7369;--如果是三表或者更多表连接，语法如下SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1) 3.2 LEFT OUTER JOINLEFT OUTER JOIN 和 LEFT JOIN 是等价的。 1234-- 左连接SELECT e.*,d.*FROM emp e LEFT OUTER JOIN dept dON e.deptno = d.deptno; 3.3 RIGHT OUTER JOIN1234--右连接SELECT e.*,d.*FROM emp e RIGHT OUTER JOIN dept dON e.deptno = d.deptno; 执行右连接后，由于 40 号部门下没有任何员工，所以此时员工信息为 NULL。这个查询可以很好的复述上面提到的——JOIN 语句的关联条件必须用 ON 指定，不能用 WHERE 指定。你可以把 ON 改成 WHERE，你会发现无论如何都查不出 40 号部门这条数据，因为笛卡尔运算不会有 (NULL, 40) 这种情况。 ### 3.4 FULL OUTER JOIN 123SELECT e.*,d.*FROM emp e FULL OUTER JOIN dept dON e.deptno = d.deptno; 3.5 LEFT SEMI JOINLEFT SEMI JOIN （左半连接）是 IN/EXISTS 子查询的一种更高效的实现。 JOIN 子句中右边的表只能在 ON 子句中设置过滤条件; 查询结果只包含左边表的数据，所以只能 SELECT 左表中的列。 12345678-- 查询在纽约办公的所有员工信息SELECT emp.*FROM emp LEFT SEMI JOIN dept ON emp.deptno = dept.deptno AND dept.loc="NEW YORK";--上面的语句就等价于SELECT emp.* FROM empWHERE emp.deptno IN (SELECT deptno FROM dept WHERE loc="NEW YORK"); 3.6 JOIN笛卡尔积连接，这个连接日常的开发中可能很少遇到，且性能消耗比较大，基于这个原因，如果在严格模式下 (hive.mapred.mode = strict)，Hive 会阻止用户执行此操作。 1SELECT * FROM emp JOIN dept; 四、JOIN优化4.1 STREAMTABLE在多表进行联结的时候，如果每个 ON 字句都使用到共同的列（如下面的 b.key），此时 Hive 会进行优化，将多表 JOIN 在同一个 map / reduce 作业上进行。同时假定查询的最后一个表（如下面的 c 表）是最大的一个表，在对每行记录进行 JOIN 操作时，它将尝试将其他的表缓存起来，然后扫描最后那个表进行计算。因此用户需要保证查询的表的大小从左到右是依次增加的。 1`SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key) JOIN c ON (c.key = b.key)` 然后，用户并非需要总是把最大的表放在查询语句的最后面，Hive 提供了 /*+ STREAMTABLE() */ 标志，用于标识最大的表，示例如下： 1234SELECT /*+ STREAMTABLE(d) */ e.*,d.* FROM emp e JOIN dept dON e.deptno = d.deptnoWHERE job='CLERK'; 4.2 MAPJOIN如果所有表中只有一张表是小表，那么 Hive 把这张小表加载到内存中。这时候程序会在 map 阶段直接拿另外一个表的数据和内存中表数据做匹配，由于在 map 就进行了 JOIN 操作，从而可以省略 reduce 过程，这样效率可以提升很多。Hive 中提供了 /*+ MAPJOIN() */ 来标记小表，示例如下： 1234SELECT /*+ MAPJOIN(d) */ e.*,d.* FROM emp e JOIN dept dON e.deptno = d.deptnoWHERE job='CLERK'; 五、SELECT的其他用途查看当前数据库： 1SELECT current_database() 六、本地模式在上面演示的语句中，大多数都会触发 MapReduce, 少部分不会触发，比如 select * from emp limit 5 就不会触发 MR，此时 Hive 只是简单的读取数据文件中的内容，然后格式化后进行输出。在需要执行 MapReduce 的查询中，你会发现执行时间可能会很长，这时候你可以选择开启本地模式。 12--本地模式默认关闭，需要手动开启此功能SET hive.exec.mode.local.auto=true; 启用后，Hive 将分析查询中每个 map-reduce 作业的大小，如果满足以下条件，则可以在本地运行它： 作业的总输入大小低于：hive.exec.mode.local.auto.inputbytes.max（默认为 128MB）； map-tasks 的总数小于：hive.exec.mode.local.auto.tasks.max（默认为 4）； 所需的 reduce 任务总数为 1 或 0。 因为我们测试的数据集很小，所以你再次去执行上面涉及 MR 操作的查询，你会发现速度会有显著的提升。 参考资料 LanguageManual Select LanguageManual Joins LanguageManual GroupBy LanguageManual SortBy]]></content>
      <categories>
        <category>HIVE</category>
      </categories>
      <tags>
        <tag>HIVE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive简介及核心概念]]></title>
    <url>%2F2019%2F07%2F16%2FHive%E7%AE%80%E4%BB%8B%E5%8F%8A%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[Hive简介及核心概念一、简介Hive 是一个构建在 Hadoop 之上的数据仓库，它可以将结构化的数据文件映射成表，并提供类 SQL 查询功能，用于查询的 SQL 语句会被转化为 MapReduce 作业，然后提交到 Hadoop 上运行。 特点： 简单、容易上手 (提供了类似 sql 的查询语言 hql)，使得精通 sql 但是不了解 Java 编程的人也能很好地进行大数据分析； 灵活性高，可以自定义用户函数 (UDF) 和存储格式； 为超大的数据集设计的计算和存储能力，集群扩展容易; 统一的元数据管理，可与 presto／impala／sparksql 等共享数据； 执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理。 二、Hive的体系架构 Hive是典型C/S模式。Client端有JDBC/ODBC Client和Thrift Client两类。Server 端则分为如下几个部分： 用户接口：clientCLI（hive shell）、JDBC/ODBC（java访问hive），WEBUI（浏览器访问hive） 元数据：metastore元数据包括：表名、表所属数据库、表的拥有者、列/分区字段、表的类型、表数据所在目录。 驱动器：dirver包含：解析器、编译器、优化器、执行器 解析器：将SQL字符串转换成抽象语法树AST，这一步一般都是用第三方工具库完成，比如antlr；对AST语法树进行分析，比如表否存在、字段是否存在、SQL语义是否有误。 编译器：将AST编译生成逻辑执行计划。 优化器：对逻辑执行计划进行优化。 执行器：把逻辑执行计划转换成物理执行计划。对于hive来说，就是MR/TEZ/Spark； 2.1 command-line shell &amp; thrift/jdbc可以用 command-line shell 和 thrift／jdbc 两种方式来操作数据： command-line shell：通过 hive 命令行的的方式来操作数据； thrift／jdbc：通过 thrift 协议按照标准的 JDBC 的方式操作数据。 2.2 Metastore在 Hive 中，表名、表结构、字段名、字段类型、表的分隔符等统一被称为元数据。所有的元数据默认存储在 Hive 内置的 derby 数据库中，但由于 derby 只能有一个实例，也就是说不能有多个命令行客户端同时访问，所以在实际生产环境中，通常使用 MySQL 代替 derby。 Hive 进行的是统一的元数据管理，就是说你在 Hive 上创建了一张表，然后在 presto／impala／sparksql 中都是可以直接使用的，它们会从 Metastore 中获取统一的元数据信息，同样的你在 presto／impala／sparksql 中创建一张表，在 Hive 中也可以直接使用。 2.3 HQL的执行流程Hive 在执行一条 HQL 的时候，会经过以下步骤： 语法解析：Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象 语法树 AST Tree； 语义解析：遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock； 生成逻辑执行计划：遍历 QueryBlock，翻译为执行操作树 OperatorTree； 优化逻辑执行计划：逻辑层优化器进行 OperatorTree 变换，合并不必要的 ReduceSinkOperator，减少 shuffle 数据量； 生成物理执行计划：遍历 OperatorTree，翻译为 MapReduce 任务； 优化物理执行计划：物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划。 关于 Hive SQL 的详细执行流程可以参考美团技术团队的文章：Hive SQL 的编译过程 三、数据类型3.1 基本数据类型Hive 表中的列支持以下基本数据类型： 大类 类型 Integers（整型） TINYINT—1 字节的有符号整数 SMALLINT—2 字节的有符号整数 INT—4 字节的有符号整数 BIGINT—8 字节的有符号整数 Boolean（布尔型） BOOLEAN—TRUE/FALSE Floating point numbers（浮点型） FLOAT— 单精度浮点型 DOUBLE—双精度浮点型 Fixed point numbers（定点数） DECIMAL—用户自定义精度定点数，比如 DECIMAL(7,2) String types（字符串） STRING—指定字符集的字符序列 VARCHAR—具有最大长度限制的字符序列 CHAR—固定长度的字符序列 Date and time types（日期时间类型） TIMESTAMP — 时间戳 TIMESTAMP WITH LOCAL TIME ZONE — 时间戳，纳秒精度 DATE—日期类型 Binary types（二进制类型） BINARY—字节序列 TIMESTAMP 和 TIMESTAMP WITH LOCAL TIME ZONE 的区别如下： TIMESTAMP WITH LOCAL TIME ZONE：用户提交时间给数据库时，会被转换成数据库所在的时区来保存。查询时则按照查询客户端的不同，转换为查询客户端所在时区的时间。 TIMESTAMP ：提交什么时间就保存什么时间，查询时也不做任何转换。 3.2 隐式转换Hive 中基本数据类型遵循以下的层次结构，按照这个层次结构，子类型到祖先类型允许隐式转换。例如 INT 类型的数据允许隐式转换为 BIGINT 类型。额外注意的是：按照类型层次结构允许将 STRING 类型隐式转换为 DOUBLE 类型。 3.3 复杂类型 类型 描述 示例 STRUCT 类似于对象，是字段的集合，字段的类型可以不同，可以使用 名称.字段名 方式进行访问 STRUCT (‘xiaoming’, 12 , ‘2018-12-12’) MAP 键值对的集合，可以使用 名称[key] 的方式访问对应的值 map(‘a’, 1, ‘b’, 2) ARRAY 数组是一组具有相同类型和名称的变量的集合，可以使用 名称[index] 访问对应的值 ARRAY(‘a’, ‘b’, ‘c’, ‘d’) 3.4 示例如下给出一个基本数据类型和复杂数据类型的使用示例： 1234567CREATE TABLE students( name STRING, -- 姓名 age INT, -- 年龄 subject ARRAY&lt;STRING&gt;, --学科 score MAP&lt;STRING,FLOAT&gt;, --各个学科考试成绩 address STRUCT&lt;houseNumber:int, street:STRING, city:STRING, province：STRING&gt; --家庭居住地址) ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"; 四、内容格式当数据存储在文本文件中，必须按照一定格式区别行和列，如使用逗号作为分隔符的 CSV 文件 (Comma-Separated Values) 或者使用制表符作为分隔值的 TSV 文件 (Tab-Separated Values)。但此时也存在一个缺点，就是正常的文件内容中也可能出现逗号或者制表符。 所以 Hive 默认使用了几个平时很少出现的字符，这些字符一般不会作为内容出现在文件中。Hive 默认的行和列分隔符如下表所示。 分隔符 描述 \n 对于文本文件来说，每行是一条记录，所以可以使用换行符来分割记录 ^A (Ctrl+A) 分割字段 (列)，在 CREATE TABLE 语句中也可以使用八进制编码 \001 来表示 ^B 用于分割 ARRAY 或者 STRUCT 中的元素，或者用于 MAP 中键值对之间的分割，在 CREATE TABLE 语句中也可以使用八进制编码 \002 表示 ^C 用于 MAP 中键和值之间的分割，在 CREATE TABLE 语句中也可以使用八进制编码 \003 表示 使用示例如下： 123456CREATE TABLE page_view(viewTime INT, userid BIGINT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' COLLECTION ITEMS TERMINATED BY '\002' MAP KEYS TERMINATED BY '\003' STORED AS SEQUENCEFILE; 五、存储格式5.1 支持的存储格式Hive 会在 HDFS 为每个数据库上创建一个目录，数据库中的表是该目录的子目录，表中的数据会以文件的形式存储在对应的表目录下。Hive 支持以下几种文件存储格式： 格式 说明 TextFile 存储为纯文本文件。 这是 Hive 默认的文件存储格式。这种存储方式数据不做压缩，磁盘开销大，数据解析开销大。 SequenceFile SequenceFile 是 Hadoop API 提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中。这种二进制文件内部使用 Hadoop 的标准的 Writable 接口实现序列化和反序列化。它与 Hadoop API 中的 MapFile 是互相兼容的。Hive 中的 SequenceFile 继承自 Hadoop API 的 SequenceFile，不过它的 key 为空，使用 value 存放实际的值，这样是为了避免 MR 在运行 map 阶段进行额外的排序操作。 RCFile RCFile 文件格式是 FaceBook 开源的一种 Hive 的文件存储格式，首先将表分为几个行组，对每个行组内的数据按列存储，每一列的数据都是分开存储。 ORC Files ORC 是在一定程度上扩展了 RCFile，是对 RCFile 的优化。 Avro Files Avro 是一个数据序列化系统，设计用于支持大批量数据交换的应用。它的主要特点有：支持二进制序列化方式，可以便捷，快速地处理大量数据；动态语言友好，Avro 提供的机制使动态语言可以方便地处理 Avro 数据。 Parquet Parquet 是基于 Dremel 的数据模型和算法实现的，面向分析型业务的列式存储格式。它通过按列进行高效压缩和特殊的编码技术，从而在降低存储空间的同时提高了 IO 效率。 以上压缩格式中 ORC 和 Parquet 的综合性能突出，使用较为广泛，推荐使用这两种格式。 5.2 指定存储格式通常在创建表的时候使用 STORED AS 参数指定： 123456CREATE TABLE page_view(viewTime INT, userid BIGINT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' COLLECTION ITEMS TERMINATED BY '\002' MAP KEYS TERMINATED BY '\003' STORED AS SEQUENCEFILE; 各个存储文件类型指定方式如下： STORED AS TEXTFILE STORED AS SEQUENCEFILE STORED AS ORC STORED AS PARQUET STORED AS AVRO STORED AS RCFILE 六、内部表和外部表内部表又叫做管理表 (Managed/Internal Table)，创建表时不做任何指定，默认创建的就是内部表。想要创建外部表 (External Table)，则需要使用 External 进行修饰。 内部表和外部表主要区别如下： 内部表 外部表 数据存储位置 内部表数据存储的位置由 hive.metastore.warehouse.dir 参数指定，默认情况下表的数据存储在 HDFS 的 /user/hive/warehouse/数据库名.db/表名/ 目录下 外部表数据的存储位置创建表时由 Location 参数指定； 导入数据 在导入数据到内部表，内部表将数据移动到自己的数据仓库目录下，数据的生命周期由 Hive 来进行管理 外部表不会将数据移动到自己的数据仓库目录下，只是在元数据中存储了数据的位置 删除表 删除元数据（metadata）和文件 只删除元数据（metadata）]]></content>
      <categories>
        <category>HIVE</category>
      </categories>
      <tags>
        <tag>HIVE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群时间同步]]></title>
    <url>%2F2019%2F07%2F04%2FHadoop%E9%9B%86%E7%BE%A4%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[Hadoop集群时间同步测试环境 IP 节点角色 主机名 10.10.101.100 master hadoop100 10.10.101.101 node hadoop101 10.10.101.102 node hadoop102 设置master服务器时间（1）检查ntp是否安装,若没有安装则使用yum install -y ntp进行安装1234567[root@hadoop101 桌面]# rpm -qa|grep ntpntp-4.2.6p5-10.el6.centos.x86_64fontpackages-filesystem-1.41-1.1.el6.noarchntpdate-4.2.6p5-10.el6.centos.x86_64 （2）修改ntp配置文件1[root@hadoop101 桌面]# vim /etc/ntp.conf 修改内容如下 a）修改1（授权192.168.1.0网段上的所有机器可以从这台机器上查询和同步时间） 123#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap为restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap b）修改2（集群在局域网中，不使用其他的网络时间） 123456789server 0.centos.pool.ntp.org iburstserver 1.centos.pool.ntp.org iburstserver 2.centos.pool.ntp.org iburstserver 3.centos.pool.ntp.org iburst为#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst c）添加3（当该节点丢失网络连接，依然可以作为时间服务器为集群中的其他节点提供时间同步） 12server 127.127.1.0fudge 127.127.1.0 stratum 10 （3）修改/etc/sysconfig/ntpd 文件1[root@hadoop101 桌面]# vim /etc/sysconfig/ntpd 增加内容如下（让硬件时间与系统时间一起同步） 1SYNC_HWCLOCK=yes （4）重新启动ntpd1234567[root@hadoop101 桌面]# service ntpd statusntpd 已停[root@hadoop101 桌面]# service ntpd start正在启动 ntpd： [确定] （5）设置开机自启动：1[root@hadoop101 桌面]# chkconfig ntpd on node配置（必须root用户）（1）在node配置10分钟与时间服务器同步一次12345[root@hadoop102 hadoop-2.7.2]# crontab -e#添加如下任务*/10 * * * * /usr/sbin/ntpdate hadoop101 （2）修改任意node时间1[root@hadoop102 root]# date -s &quot;2017-9-11 11:11:11&quot; （3）十分钟后查看机器是否与时间服务器同步1[root@hadoop102 root]# date]]></content>
      <categories>
        <category>集群管理</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫从零开始(六)]]></title>
    <url>%2F2019%2F06%2F26%2FPython%E7%88%AC%E8%99%AB%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B(%E5%85%AD)%2F</url>
    <content type="text"><![CDATA[认识HTML什么是HTMLHTML 是用来描述网页的一种语言。 HTML 指的是超文本标记语言 (Hyper Text Markup Language) HTML 不是一种编程语言，而是一种标记语言 (markup language) 标记语言是一套标记标签 (markup tag) HTML 使用标记标签来描述网页 HTML 标签HTML 标记标签通常被称为 HTML 标签 (HTML tag)。 HTML 标签是由尖括号包围的关键词，比如 &lt;html&gt; HTML 标签通常是成对出现的，比如 &lt;b&gt; 和 &lt;/b&gt; 标签对中的第一个标签是开始标签，第二个标签是结束标签 开始和结束标签也被称为开放标签和闭合标签 HTML 文档 = 网页 HTML 文档描述网页 HTML 文档包含 HTML 标签和纯文本 HTML 文档也被称为网页Web 浏览器的作用是读取 HTML 文档，并以网页的形式显示出它们。浏览器不会显示 HTML 标签，而是使用标签来解释页面的内容：123456789&lt;html&gt;&lt;body&gt;&lt;h1&gt;我的第一个标题&lt;/h1&gt;&lt;p&gt;我的第一个段落。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 例子解释 &lt;html&gt; 与 &lt;/html&gt; 之间的文本描述网页 &lt;body&gt; 与 &lt;/body&gt; 之间的文本是可见的页面内容 &lt;h1&gt; 与 &lt;/h1&gt; 之间的文本被显示为标题 &lt;p&gt; 与 &lt;/p&gt; 之间的文本被显示为段落 以下是常用标签及其含义 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;页面标题&lt;/title&gt; &lt;style&gt; #div1 &#123; color: brown; &#125; .div2 &#123; color: chartreuse; &#125; div &#123; color: aqua; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;一级标题&lt;/h1&gt;&lt;h2&gt;二级标题&lt;/h2&gt;&lt;h3&gt;三级标题&lt;/h3&gt;&lt;h4&gt;四级标题&lt;/h4&gt;&lt;h5&gt;五级标题&lt;/h5&gt;&lt;h6&gt;六级标题&lt;/h6&gt;span标签没有任何样式，&lt;span style=&quot;color: aqua&quot;&gt;仅仅&lt;/span&gt;为了分割显示样式。&lt;br&gt;换行标签&lt;br&gt;&lt;p&gt; 这是一个段落这是一个段落这是一个段落这是一个段落这是一个段落这是一个段落 这是一个段落这是一个段落这是一个段落这是一个段落这是一个段落这是一个段落这是一个段落这是一个段落这是一个段落这是一个段落这是一个段落&lt;/p&gt;&lt;P&gt; 这是一个新的段落&lt;/P&gt;&lt;div style=&quot;color: brown;width: 100px;height: 100px&quot;&gt; 这是一个div&lt;/div&gt;&lt;div style=&quot;color: chartreuse;width: 100px;height: 100px&quot;&gt; 这是一个新的div&lt;/div&gt;&lt;hr&gt;&lt;hr&gt;&lt;hr&gt;&lt;b&gt;加粗&lt;/b&gt;&lt;i&gt;斜体&lt;/i&gt;&lt;u&gt;下划线&lt;/u&gt;&lt;del&gt;删除线&lt;/del&gt;&lt;sup&gt;上标&lt;/sup&gt;&lt;sub&gt;下标&lt;/sub&gt;&lt;!--无序列表--&gt;&lt;ul type=&quot;square&quot;&gt; &lt;li&gt;sunck is a good man&lt;/li&gt; &lt;li&gt;sunck is a nice man&lt;/li&gt; &lt;li&gt;sunck is a handsome man&lt;/li&gt;&lt;/ul&gt;&lt;!--有序列表--&gt;&lt;ol type=&quot;I&quot;&gt; &lt;li&gt;sunck is a good man&lt;/li&gt; &lt;li&gt;sunck is a nice man&lt;/li&gt; &lt;li&gt;sunck is a handsome man&lt;/li&gt;&lt;/ol&gt;&lt;!--定义列表--&gt;&lt;dl&gt; &lt;dt&gt;学院1&lt;/dt&gt; &lt;dd&gt;电子信息工程1&lt;/dd&gt; &lt;dd&gt;电子信息工程11&lt;/dd&gt; &lt;dt&gt;学院2&lt;/dt&gt; &lt;dd&gt;电子信息工程2&lt;/dd&gt;&lt;/dl&gt;&lt;!--表格--&gt;&lt;table border=&quot;1&quot; cellspacing=&quot;0&quot; cellpadding=&quot;0&quot; align=&quot;center&quot;&gt; &lt;tr&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;性别&lt;/th&gt; &lt;th&gt;年龄&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=&quot;2&quot;&gt;第1行第一列&lt;/td&gt; &lt;td&gt;第1行第二列&lt;/td&gt; &lt;!--&lt;td&gt;第1行第三列&lt;/td&gt;--&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;第2行第一列&lt;/td&gt; &lt;td&gt;第2行第二列&lt;/td&gt; &lt;td rowspan=&quot;2&quot;&gt;第2行第三列&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;第3行第一列&lt;/td&gt; &lt;td&gt;第3行第二列&lt;/td&gt; &lt;!--&lt;td&gt;第3行第三列&lt;/td&gt;--&gt; &lt;/tr&gt;&lt;/table&gt;&lt;div id=&quot;div1&quot; class=&quot;div1&quot;&gt; 这是一个块标签&lt;/div&gt;&lt;div id=&quot;div2&quot; class=&quot;div2&quot;&gt; 这是一个块&lt;/div&gt;&lt;a href=&quot;www.baidu.com&quot;&gt;百度&lt;/a&gt;&lt;img src=&quot;a.jpg&quot;&gt;&lt;form action=&quot;&quot; method=&quot;get&quot;&gt; First name: &lt;input type=&quot;text&quot; name=&quot;fname&quot;&gt;&lt;br&gt; Last name: &lt;input type=&quot;text&quot; name=&quot;lname&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot;&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 初始爬虫什么是爬虫网络爬虫就行一只虫子，在互联网上爬啊爬，将获取的信息收集起来。 百度百科的定义为： 网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。 爬虫的作用现如今大数据时代已经到来，网络爬虫技术成为这个时代不可或缺的一部分，企业需要数据来分析用户行为，来分析自己产品的不足之处，来分析竞争对手的信息等等，但是这些的首要条件就是数据的采集。 爬虫协议什么是爬虫协议？ 爬虫协议就是你想用爬虫爬我的网站，那么你得听我的，哪些你能爬，哪些你不能爬。 怎么查看一个网站的爬虫协议呢，就在这个网站的域名后面加上robots.txt 比如说下面有：jd、百度、淘宝的爬虫协议 jd：https://www.jd.com/robots.txt 淘宝的：https://www.taobao.com/robots.txt 百度的：https://www.baidu.com/robots.txt 如果你要爬的网站域名加上robots.txt是404，那你就可以随心所欲的爬了。 不过就算爬虫协议里面写了，你也可以不遵守，但是也得注意一下，有的公司发现的话，会起诉的。比如说前几年著名的百度控诉360爬虫违反爬虫协议，索赔1亿元。 爬虫协议里面有这么几个字段： User-agent：* 这个字段的意思是允许哪个引擎的爬虫获取数据 * 代表所有类型的爬虫都可以 Disallow:/admin/ 这个字段代表爬虫不允许爬哪个路径下面的数据，如果是/的话，就代表所有的路径下面的数据都不能爬。 Allow: /cgi-bin/ 这里定义是允许爬寻cgi-bin目录下面的目录 Sitemap: 网站地图 告诉爬虫这个页面是网站地图反爬虫防止有人不遵守爬虫协议恶意爬取网站信息，于是，很多网站开始反网络爬虫,想方设法保护自己的内容。他们根据ip访问频率，浏览网页速度，账户登录，输入验证码，flash封装，ajax混淆，js加密，图片，css混淆等五花八门的技术，来对反网络爬虫。防的一方不惜工本，迫使抓的一方在考虑成本效益后放弃抓的一方不惜工本，防的一方在考虑用户流失后放弃 #请求与响应 HTTP和HTTPS HTTP(HyperText Transfer Protocol，超文本传输协议)：是一种发布和接收HTML页面的方法 HTTPS(HyperText Transfer Protocol over Secure Socket Layer)简单讲是HTTP的安全版，在HTTP下加入SSL层。SSL(Secure Socket Layer安全套接层)主要用于web的安全传输协议，在传输层对网络连接进行加密，保障在Internet上数据传输的安全。 HTTP的端口号为80 HTTPS的端口号为443 http请求过程打开一个网站的时候，过程是这样的客户端（浏览器）发送请求到服务端（你打开的网站所在的服务器），服务端接收到请求，处理，返回数据给客户端（浏览器），然后咱们在浏览器里面看到了数据。 明白了这个过程之后呢，咱们再来说http请求里面都包含了什么东西。 请求方式主要有：GET/POST两种类型常用，另外还有HEAD/PUT/DELETE/OPTIONSGET和POST的区别就是：请求的数据GET是在url中，POST则是存放在请求体里面。 GET:一般向服务器获取数据用get请求，get请求的数据都是放在url中的，实质上和post请求没有太大的区别，当然也可以用来向服务器发送数据。 POST:一般向服务器发送数据用post请求，post请求的数据放在请求体里。 HEAD：与GET方法一样，都是向服务器发出指定资源的请求。只不过服务器将不传回资源的本文部分。它的好处在于，使用这个方法可以在不必传输全部内容的情况下，就可以获取其中“关于该资源的信息”（元信息或称元数据）。 PUT：向指定资源位置上传其最新内容。 OPTIONS：这个方法可使服务器传回该资源所支持的所有HTTP请求方法。用’*’来代替资源名称，向Web服务器发送OPTIONS请求，可以测试服务器功能是否正常运作。 DELETE：请求服务器删除Request-URI所标识的资源。 请求urlURL，即统一资源定位符，也就是我们说的网址，统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。 URL的格式由三个部分组成：第一部分是协议(或称为服务方式)。 http/https第二部分是存有该资源的主机IP地址(有时也包括端口号)。 www.nnzhp.cn/192.168.1.1:8888第三部分是主机资源的具体地址，如目录和文件名等。 /index 爬虫爬取数据时必须要有一个目标的URL才可以获取数据，因此，它是爬虫获取数据的基本依据。 请求头一个请求由两部分组成， 请求头和请求体。 包含请求时的头部信息，如User-Agent,Host,Cookies等信息，user-agent就是你请求用的是什么浏览器，host就是服务端的地址，还有很多信息，服务端是如何分辨你是用的什么浏览器，你的ip地址就是从请求头里面获取到的。下面就是在请求我博客的时候，发送的头信息。 请求体请求体就是发送数据的时候，数据放在请求体里面。get请求是没有请求体的，从上面的截图也能看到，下面是没有这个请求体的。post请求才有请求体。下面的截图可以看到登陆的这个请求是一个post请求，登陆的账号密码就是放在请求体里面的。 http响应 发送了请求，服务端要返回数据。这个就是响应，请求是你发出去的，响应是服务端返回给你的。 响应包含了2个部分，一个是响应头，一个是响应体。响应头里面包含了响应的状态码，返回数据的类型，类型的长度，服务器信息，Cookie信息等等。 响应体里面就是具体返回的数据了。 响应状态码有很多响应状态，不同的状态码代表不同的状态，常见的状态码如：200代表成功，301跳转，404找不到页面，502服务端错误 1xx消息——请求已被服务器接收，继续处理 2xx成功——请求已成功被服务器接收、理解、并接受 3xx重定向——需要后续操作才能完成这一请求 4xx请求错误——请求含有词法错误或者无法被执行 5xx服务器错误——服务器在处理某个正确请求时发生错误 常见代码： 200 OK 请求成功 400 Bad Request 客户端请求有语法错误，不能被服务器所理解 401 Unauthorized 请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden 服务器收到请求，但是拒绝提供服务 404 Not Found 请求资源不存在 503 Server Unavailable 服务器当前不能处理客户端的请求，一段时间后可能恢复正常 301 目标暂时性转移 302 目标永久性转移 响应头、响应体 看下图~ 代码示例12345678910111213import requests# 请求头header = &#123; &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36&quot;&#125;发送get请求response = requests.get(&quot;http://www.xinhuanet.com/politics/2019-06/25/c_1124669748.htm&quot;, headers=header)response.encoding = &quot;utf-8&quot;写入文件with open(&quot;b.html&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: f.write(response.text)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫从零开始(五)]]></title>
    <url>%2F2019%2F06%2F25%2FPython%E7%88%AC%E8%99%AB%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B(%E4%BA%94)%2F</url>
    <content type="text"><![CDATA[Python 基础Python 输入输出读文件读写文件是最常见的IO操作。Python内置了读写文件的函数，用法和C是兼容的。要以读文件的模式打开一个文件对象，使用Python内置的open()函数，传入文件名和标示符： 1&gt;&gt;&gt; f = open(&apos;/Users/michael/test.txt&apos;, &apos;r&apos;) 标示符’r’表示读，这样，我们就成功地打开了一个文件。 如果文件不存在，open()函数就会抛出一个IOError的错误，并且给出错误码和详细的信息告诉你文件不存在： 1234&gt;&gt;&gt; f=open(&apos;/Users/michael/notfound.txt&apos;, &apos;r&apos;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;FileNotFoundError: [Errno 2] No such file or directory: &apos;/Users/michael/notfound.txt&apos; 如果文件打开成功，接下来，调用read()方法可以一次读取文件的全部内容，Python把内容读到内存，用一个str对象表示： 12&gt;&gt;&gt; f.read()&apos;Hello, world!&apos; 最后一步是调用close()方法关闭文件。文件使用完毕后必须关闭，因为文件对象会占用操作系统的资源，并且操作系统同一时间能打开的文件数量也是有限的： 1&gt;&gt;&gt; f.close() 由于文件读写时都有可能产生IOError，一旦出错，后面的f.close()就不会调用。所以，为了保证无论是否出错都能正确地关闭文件，我们可以使用try ... finally来实现： 123456try: f = open(&apos;/path/to/file&apos;, &apos;r&apos;) print(f.read())finally: if f: f.close() 但是每次都这么写实在太繁琐，所以，Python引入了with语句来自动帮我们调用close()方法： 12with open(&apos;/path/to/file&apos;, &apos;r&apos;) as f: print(f.read()) 这和前面的try ... finally是一样的，但是代码更佳简洁，并且不必调用f.close()方法。 调用read()会一次性读取文件的全部内容，如果文件有10G，内存就爆了，所以，要保险起见，可以反复调用read(size)方法，每次最多读取size个字节的内容。另外，调用readline()可以每次读取一行内容，调用readlines()一次读取所有内容并按行返回list。因此，要根据需要决定怎么调用。 如果文件很小，read()一次性读取最方便；如果不能确定文件大小，反复调用read(size)比较保险；如果是配置文件，调用readlines()最方便： 123for line in f.readlines(): print(line.strip()) # 把末尾的&apos;\n&apos;删掉file-like Object 像open()函数返回的这种有个read()方法的对象，在Python中统称为file-like Object。除了file外，还可以是内存的字节流，网络流，自定义流等等。file-like Object不要求从特定类继承，只要写个read()方法就行。 StringIO就是在内存中创建的file-like Object，常用作临时缓冲。 二进制文件前面讲的默认都是读取文本文件，并且是UTF-8编码的文本文件。要读取二进制文件，比如图片、视频等等，用&#39;rb&#39;模式打开文件即可： 123&gt;&gt;&gt; f = open(&apos;/Users/michael/test.jpg&apos;, &apos;rb&apos;)&gt;&gt;&gt; f.read()b&apos;\xff\xd8\xff\xe1\x00\x18Exif\x00\x00...&apos; # 十六进制表示的字节 字符编码要读取非UTF-8编码的文本文件，需要给open()函数传入encoding参数，例如，读取GBK编码的文件： 123&gt;&gt;&gt; f = open(&apos;/Users/michael/gbk.txt&apos;, &apos;r&apos;, encoding=&apos;gbk&apos;)&gt;&gt;&gt; f.read()&apos;测试&apos; 遇到有些编码不规范的文件，你可能会遇到UnicodeDecodeError，因为在文本文件中可能夹杂了一些非法编码的字符。遇到这种情况，open()函数还接收一个errors参数，表示如果遇到编码错误后如何处理。最简单的方式是直接忽略： 1&gt;&gt;&gt; f = open(&apos;/Users/michael/gbk.txt&apos;, &apos;r&apos;, encoding=&apos;gbk&apos;, errors=&apos;ignore&apos;) 写文件写文件和读文件是一样的，唯一区别是调用open()函数时，传入标识符&#39;w&#39;或者&#39;wb&#39;表示写文本文件或写二进制文件： 123&gt;&gt;&gt; f = open(&apos;/Users/michael/test.txt&apos;, &apos;w&apos;)&gt;&gt;&gt; f.write(&apos;Hello, world!&apos;)&gt;&gt;&gt; f.close() 你可以反复调用write()来写入文件，但是务必要调用f.close()来关闭文件。当我们写文件时，操作系统往往不会立刻把数据写入磁盘，而是放到内存缓存起来，空闲的时候再慢慢写入。只有调用close()方法时，操作系统才保证把没有写入的数据全部写入磁盘。忘记调用close()的后果是数据可能只写了一部分到磁盘，剩下的丢失了。所以，还是用with语句来得保险： 12with open(&apos;/Users/michael/test.txt&apos;, &apos;w&apos;) as f: f.write(&apos;Hello, world!&apos;) 要写入特定编码的文本文件，请给open()函数传入encoding参数，将字符串自动转换成指定编码。 细心的童鞋会发现，以&#39;w&#39;模式写入文件时，如果文件已存在，会直接覆盖（相当于删掉后新写入一个文件）。如果我们希望追加到文件末尾怎么办？可以传入&#39;a&#39;以追加（append）模式写入。 所有模式的定义及含义可以参考Python的官方文档。 读写CSV文件csv文件的读取前期工作：在定义的py文件里边创建一个excel文件，并另存为csv文件，放入三行数据，我这里是姓名+年龄（可以自己随意写） 首先我们要在python环境里导入csv板块 12# -*- coding: utf-8 -*-import csv 然后我们定义一个csv文件的变量csv_file,然后通过open对此文件进行打开，打开模式采用‘r’（read：读模式） 12345with open(&quot;my.csv&quot;, &quot;r&quot;)as f: print(f) csv_file = csv.reader(f) for stu in csv_file: print(stu) 输出结果如下 1234&lt;_csv.reader object at 0x000001D3A1A1B730&gt;[&apos;zhangsan&apos;,&apos;32&apos;][&apos;wangwu&apos;,&apos;34&apos;][&apos;zhaoliu&apos;,&apos;35&apos;] csv文件的写入在开始前我们要定义两组数据，进行下面的写入 12stu1 = [&apos;marry&apos;,26]stu2 = [&apos;bob&apos;,23] 1.写入的第一步同样也是打开文件，因为我们是要写入，所以我们用的模式就是&#39;a&#39;模式，追加内容，至于&quot;newline=&quot;就是说因为我们的csv文件的类型，如果不加这个东西，当我们写入东西的时候，就会出现空行。 1out = open(&apos;Stu_csv.csv&apos;,&apos;a&apos;, newline=&apos;&apos;) 2.下面我们定义一个变量进行写入，将刚才的文件变量传进来，dialect就是定义一下文件的类型，我们定义为excel类型 1csv_write = csv.writer(out,dialect=&apos;excel&apos;) 3.然后进行数据的写入啦，写入的方法是writerow，通过写入模式对象，调用方法进行写入 12csv_write.writerow(stu1)csv_write.writerow(stu2) 4.最后各位可以用你们最熟悉的一句语法进行漂亮的收尾. 1print (&quot;write over&quot;) 具体的代码如下： 12345678910111213import csv#csv 写入stu1 = [&apos;marry&apos;,26]stu2 = [&apos;bob&apos;,23]#打开文件，追加aout = open(&apos;Stu_csv.csv&apos;,&apos;a&apos;, newline=&apos;&apos;)#设定写入模式csv_write = csv.writer(out,dialect=&apos;excel&apos;)#写入具体内容csv_write.writerow(stu1)csv_write.writerow(stu2)print (&quot;write over&quot;) mongoDB安装MongoDB首先到 MongoDB官网 下载安装MongoDB。并且到 MongoChef 官网下载安装可视化管理工具MongoChef studio3t 参考文件 安装PyMongo 驱动用可视化管理工具新建mydb数据库并添加student集合使用以下命令安装pymongo 驱动 1python -m pip install pymongo 也可以指定安装的版本: 1$ python -m pip install pymongo==3.5.1 更新 pymongo 命令： 1$ python -m pip install --upgrade pymongo 测试 PyMongo接下来我们可以创建一个测试文件 demo_test_mongodb.py，代码如下： demo_test_mongodb.py 文件代码： 123#!/usr/bin/python3 import pymongo 执行以上代码文件，如果没有出现错误，表示安装成功。 创建数据库创建一个数据库创建数据库需要使用 MongoClient 对象，并且指定连接的 URL 地址和要创建的数据库名。 如下实例中，我们创建的数据库 runoobdb : 实例 123456#!/usr/bin/python3 import pymongo myclient = pymongo.MongoClient(&quot;mongodb://localhost:27017/&quot;)mydb = myclient[&quot;runoobdb&quot;] 注意: 在 MongoDB 中，数据库只有在内容插入后才会创建! 就是说，数据库创建后要创建集合(数据表)并插入一个文档(记录)，数据库才会真正创建。 判断数据库是否已存在我们可以读取 MongoDB 中的所有数据库，并判断指定的数据库是否存在： 实例 12345678910#!/usr/bin/python3 import pymongo myclient = pymongo.MongoClient(&apos;mongodb://localhost:27017/&apos;) dblist = myclient.list_database_names()# dblist = myclient.database_names() if &quot;runoobdb&quot; in dblist: print(&quot;数据库已存在！&quot;) 注意：database_names 在最新版本的 Python 中已废弃，Python3.7+ 之后的版本改为了 list_database_names()。 创建集合MongoDB 中的集合类似 SQL 的表。 创建一个集合MongoDB 使用数据库对象来创建集合，实例如下： 实例 1234567#!/usr/bin/python3import pymongo myclient = pymongo.MongoClient(&quot;mongodb://localhost:27017/&quot;)mydb = myclient[&quot;runoobdb&quot;] mycol = mydb[&quot;sites&quot;] 注意: 在 MongoDB 中，集合只有在内容插入后才会创建! 就是说，创建集合(数据表)后要再插入一个文档(记录)，集合才会真正创建。 判断集合是否已存在我们可以读取 MongoDB 数据库中的所有集合，并判断指定的集合是否存在： 实例 12345678910#!/usr/bin/python3import pymongomyclient = pymongo.MongoClient(&apos;mongodb://localhost:27017/&apos;)mydb = myclient[&apos;runoobdb&apos;]collist = mydb. list_collection_names()# collist = mydb.collection_names()if &quot;sites&quot; in collist: # 判断 sites 集合是否存在 print(&quot;集合已存在！&quot;) 添加文档操作123456789101112131415161718from pymongo import MongoClient# 连接服务器conn = MongoClient(&quot;localhost&quot;, 27017)# 连接数据库db = conn.mydb# 获取集合collection = db.student# 添加文档# collection.insert(&#123;&quot;name&quot;:&quot;abc&quot;, &quot;age&quot;:19, &quot;gender&quot;:1,&quot;address&quot;:&quot;北京&quot;, &quot;isDelete&quot;:0&#125;)collection.insert_many([&#123;&quot;name&quot;: &quot;abc1&quot;, &quot;age&quot;: 19, &quot;gender&quot;: 1, &quot;address&quot;: &quot;北京&quot;, &quot;isDelete&quot;: 0&#125;, &#123;&quot;name&quot;: &quot;abc2&quot;, &quot;age&quot;: 19, &quot;gender&quot;: 1, &quot;address&quot;: &quot;北京&quot;, &quot;isDelete&quot;: 0&#125;])# 断开conn.close() 查询文档操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import pymongofrom pymongo import MongoClientfrom bson.objectid import ObjectId # 用于ID查询# 连接服务器conn = MongoClient(&quot;localhost&quot;, 27017)# 连接数据库db = conn.mydb# 获取集合collection = db.student# 查询文档# 查询部分文档&apos;&apos;&apos;res = collection.find(&#123;&quot;age&quot;:&#123;&quot;$gt&quot;:18&#125;&#125;)for row in res: print(row) print(type(row))&apos;&apos;&apos;# 查询所有文档&apos;&apos;&apos;res = collection.find()for row in res: print(row) print(type(row))&apos;&apos;&apos;# 统计查询&apos;&apos;&apos;res = collection.find(&#123;&quot;age&quot;:&#123;&quot;$gt&quot;:18&#125;&#125;).count()print(res)&apos;&apos;&apos;# 根据id查询&apos;&apos;&apos;res = collection.find(&#123;&quot;_id&quot;:ObjectId(&quot;5995084b019723fe2a0d8d14&quot;)&#125;)print(res[0])&apos;&apos;&apos;# 排序&apos;&apos;&apos;# res = collection.find().sort(&quot;age&quot;)#升序res = collection.find().sort(&quot;age&quot;, pymongo.DESCENDING)for row in res: print(row)&apos;&apos;&apos;# 分页查询res = collection.find().skip(3).limit(5)for row in res: print(row)# 断开conn.close() 更新文档操作12345678910111213from pymongo import MongoClient# 连接服务器conn = MongoClient(&quot;localhost&quot;, 27017)# 连接数据库db = conn.mydb# 获取集合collection = db.studentcollection.update(&#123;&quot;name&quot;: &quot;lilei&quot;&#125;, &#123;&quot;$set&quot;: &#123;&quot;age&quot;: 25&#125;&#125;)# 断开conn.close() 删除文档操作1234567891011121314from pymongo import MongoClient# 连接服务器conn = MongoClient(&quot;localhost&quot;, 27017)# 连接数据库db = conn.mydb# 获取集合collection = db.studentcollection.remove(&#123;&quot;name&quot;: &quot;lilei&quot;&#125;)# 全部删除collection.remove()# 断开conn.close()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 爬虫从零开始(四)]]></title>
    <url>%2F2019%2F06%2F24%2FPython%E7%88%AC%E8%99%AB%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B(%E5%9B%9B)%2F</url>
    <content type="text"><![CDATA[Python 基础面向对象面向对象（Object Oriented）的英文缩写是OO，它是一种设计思想。我们经常听说的面向对象编程（Object Oriented Programming，即OOP）就是主要针对大型软件设计而提出的，它可以使软件设计更加灵活，并且能更好地进行代码复用。 面向对象中的对象（Object），通常是指客观世界中存在的对象，这个对象具有唯一性，对象之间个不相同，各有各的特点，每一个对象都有自己的运动规律和内部状态；对象和对象之间又是可以相互联系、相互作用的。 对象：是一个抽象概念，英文称为Object，表示任意存在的事物。世间万物皆对象，现实世间中随处可以的一种事物就是对象，对象是事物存在的实体，如一个人。 通常将对象划分为两个部分：静态部分和动态部分。静态部分被称为【属性】，任何对象都具备自身属性，这些属性不仅是客观存在的，而且是不能被忽视的。如人的性别。动态部分指的是对象的行为，即对象执行的动作，如人可以行走。 类和实例面向对象最重要的概念就是类（Class）和实例（Instance），必须牢记类是抽象的模板，比如Student类，而实例是根据类创建出来的一个个具体的“对象”，每个对象都拥有相同的方法，但各自的数据可能不同。 代码示例 1234567class Student(object): def __init__(self, name, score): self.name = name self.score = score def print_score(self): print(&apos;%s: %s&apos; % (self.name, self.score)) 通过class关键字来定义类，类名通常要大写，后面的括号里面表示该类继承于哪个类，所有类最终继承于object类，这类似于Java python允许对实例变量绑定任何数据，因此同一个类的不同实例拥有的变量名称可能会不同 特殊的__init__方法可以把必须绑定给类实例的属性填写进去，它的第一个参数永远是self，表示创建的实例本身，在创建实例的时候，必须传入与init方法匹配的参数，self除外 实际上，Python中类中定义的函数的第一个参数都必须是self，调用时不用传递该参数对Student类使用实例代码： 12345678chen = Student(&apos;chen&apos;, 13)bart = Student(&apos;bart&apos;, 81)chen.print_score()bart.print_score()chen.score = 100bart.name = &apos;kobe&apos;chen.print_score()bart.print_score() 输出结果 1234chen: 13bart: 81chen: 100kobe: 81 访问限制 如果要让内部属性不被外部访问，可以把属性的名称前加上两个下划线__，在Python中，实例的变量名如果以__开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问:1234567891011121314class Student(object): def __init__(self, name, score): self.__name = name self.__score = score def print_score(self): print(&apos;%s: %s&apos; % (self.__name, self.__score))&gt;&gt;&gt; bart = Student(&apos;Bart Simpson&apos;, 59)&gt;&gt;&gt; bart.__nameTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: &apos;Student&apos; object has no attribute &apos;__name&apos; 这样就确保了外部代码不能随意修改对象内部的状态，这样通过访问限制的保护，代码更加健壮。 但是如果外部代码要获取name和score怎么办？可以给Student类增加get_name和get_score这样的方法： 12345678class Student(object): ... def get_name(self): return self.__name def get_score(self): return self.__score 如果又要允许外部代码修改score怎么办？可以再给Student类增加set_score方法： 12345class Student(object): ... def set_score(self, score): self.__score = score 你也许会问，原先那种直接通过bart.score = 99也可以修改啊，为什么要定义一个方法大费周折？因为在方法中，可以对参数做检查，避免传入无效的参数： 12345678class Student(object): ... def set_score(self, score): if 0 &lt;= score &lt;= 100: self.__score = score else: raise ValueError(&apos;bad score&apos;) 需要注意的是，在Python中，变量名类似xxx的，也就是以双下划线开头，并且以双下划线结尾的，是特殊变量，特殊变量是可以直接访问的，不是private变量，所以，不能用name、score这样的变量名。以一个下划线开头的变量_name外部可以访问，但是最好把它视为私有变量，不要随便访问python解释器对外把__name改成了_Student__name，因此可以通过_Student__name来访问私有的内部属性(强烈建议不要采用这种方式)： 12&gt;&gt;&gt; bart._Student__name&apos;Bart Simpson&apos; 我们在前面发现无法输出bart.__name，会报错Student类不含有此成员，但是bart.__name = &#39;New Name&#39;语 句是不会报错的，随后再输出bart.__name也不会再报错了： 123456&gt;&gt;&gt; bart = Student(&apos;Bart Simpson&apos;, 59)&gt;&gt;&gt; bart.get_name()&apos;Bart Simpson&apos;&gt;&gt;&gt; bart.__name = &apos;New Name&apos; # 设置__name变量！&gt;&gt;&gt; bart.__name&apos;New Name&apos; 这是一种错误的写法，这样设置的__name变量并不是类内部的__name变量，相当于给bart新增了一个变量 继承和多态在OOP程序设计中，当我们定义一个class的时候，可以从某个现有的class继承，新的class称为子类（Subclass），而被继承的class称为基类、父类或超类（Base class、Super class）。 比如，我们已经编写了一个名为Animal的class，有一个run()方法可以直接打印： 123class Animal(object): def run(self): print(&apos;Animal is running...&apos;) 当我们需要编写Dog和Cat类时，就可以直接从Animal类继承： 1234class Dog(Animal): passclass Cat(Animal): pass 对于Dog来说，Animal就是它的父类，对于Animal来说，Dog就是它的子类。Cat和Dog类似。 继承有什么好处？最大的好处是子类获得了父类的全部功能。由于Animial实现了run()方法，因此，Dog和Cat作为它的子类，什么事也没干，就自动拥有了run()方法： 12345dog = Dog()dog.run()cat = Cat()cat.run() 运行结果如下： 12Animal is running...Animal is running... 当然，也可以对子类增加一些方法，比如Dog类： 1234567class Dog(Animal): def run(self): print(&apos;Dog is running...&apos;) def eat(self): print(&apos;Eating meat...&apos;) 继承的第二个好处需要我们对代码做一点改进。你看到了，无论是Dog还是Cat，它们run()的时候，显示的都是Animal is running...，符合逻辑的做法是分别显示Dog is running...和Cat is running...，因此，对Dog和Cat类改进如下： 123456789class Dog(Animal): def run(self): print(&apos;Dog is running...&apos;)class Cat(Animal): def run(self): print(&apos;Cat is running...&apos;) 再次运行，结果如下： 12Dog is running...Cat is running... 当子类和父类都存在相同的run()方法时，我们说，子类的run()覆盖了父类的run()，在代码运行的时候，总是会调用子类的run()。这样，我们就获得了继承的另一个好处：多态。 要理解什么是多态，我们首先要对数据类型再作一点说明。当我们定义一个class的时候，我们实际上就定义了一种数据类型。我们定义的数据类型和Python自带的数据类型，比如str、list、dict没什么两样： 123a = list() # a是list类型b = Animal() # b是Animal类型c = Dog() # c是Dog类型 判断一个变量是否是某个类型可以用isinstance()判断： 123456&gt;&gt;&gt; isinstance(a, list)True&gt;&gt;&gt; isinstance(b, Animal)True&gt;&gt;&gt; isinstance(c, Dog)True 看来a、b、c确实对应着list、Animal、Dog这3种类型。 但是等等，试试： 12&gt;&gt;&gt; isinstance(c, Animal)True 看来c不仅仅是Dog，c还是Animal！ 不过仔细想想，这是有道理的，因为Dog是从Animal继承下来的，当我们创建了一个Dog的实例c时，我们认为c的数据类型是Dog没错，但c同时也是Animal也没错，Dog本来就是Animal的一种！ 所以，在继承关系中，如果一个实例的数据类型是某个子类，那它的数据类型也可以被看做是父类。但是，反过来就不行： 123&gt;&gt;&gt; b = Animal()&gt;&gt;&gt; isinstance(b, Dog)False Dog可以看成Animal，但Animal不可以看成Dog。 要理解多态的好处，我们还需要再编写一个函数，这个函数接受一个Animal类型的变量： 123def run_twice(animal): animal.run() animal.run() 当我们传入Animal的实例时，run_twice()就打印出： 123&gt;&gt;&gt; run_twice(Animal())Animal is running...Animal is running... 当我们传入Dog的实例时，run_twice()就打印出： 123&gt;&gt;&gt; run_twice(Dog())Dog is running...Dog is running... 当我们传入Cat的实例时，run_twice()就打印出： 123&gt;&gt;&gt; run_twice(Cat())Cat is running...Cat is running... 看上去没啥意思，但是仔细想想，现在，如果我们再定义一个Tortoise类型，也从Animal派生： 123class Tortoise(Animal): def run(self): print(&apos;Tortoise is running slowly...&apos;) 当我们调用run_twice()时，传入Tortoise的实例： 123&gt;&gt;&gt; run_twice(Tortoise())Tortoise is running slowly...Tortoise is running slowly... 你会发现，新增一个Animal的子类，不必对run_twice()做任何修改，实际上，任何依赖Animal作为参数的函数或者方法都可以不加修改地正常运行，原因就在于多态。 多态的好处就是，当我们需要传入Dog、Cat、Tortoise……时，我们只需要接收Animal类型就可以了，因为Dog、Cat、Tortoise……都是Animal类型，然后，按照Animal类型进行操作即可。由于Animal类型有run()方法，因此，传入的任意类型，只要是Animal类或者子类，就会自动调用实际类型的run()方法，这就是多态的意思： 对于一个变量，我们只需要知道它是Animal类型，无需确切地知道它的子类型，就可以放心地调用run()方法，而具体调用的run()方法是作用在Animal、Dog、Cat还是Tortoise对象上，由运行时该对象的确切类型决定，这就是多态真正的威力：调用方只管调用，不管细节，而当我们新增一种Animal的子类时，只要确保run()方法编写正确，不用管原来的代码是如何调用的。这就是著名的“开闭”原则： 对扩展开放：允许新增Animal子类； 对修改封闭：不需要修改依赖Animal类型的run_twice()等函数。 继承还可以一级一级地继承下来，就好比从爷爷到爸爸、再到儿子这样的关系。而任何类，最终都可以追溯到根类object，这些继承关系看上去就像一颗倒着的树。比如如下的继承树： 1234567891011121314151617 ┌───────────────┐ │ object │ └───────────────┘ │ ┌────────────┴────────────┐ │ │ ▼ ▼ ┌─────────────┐ ┌─────────────┐ │ Animal │ │ Plant │ └─────────────┘ └─────────────┘ │ │ ┌─────┴──────┐ ┌─────┴──────┐ │ │ │ │ ▼ ▼ ▼ ▼┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐│ Dog │ │ Cat │ │ Tree │ │ Flower │└─────────┘ └─────────┘ └─────────┘ └─────────┘ 获取对象信息使用type()来判断对象类型，它返回对应的Class类型。基本类型可以使用type()来判断： 12345678&gt;&gt;&gt; type(123)&lt;class &apos;int&apos;&gt;&gt;&gt;&gt; type(2.0)&lt;class &apos;float&apos;&gt;&gt;&gt;&gt; type([&quot;sd&quot;,&apos;s&apos;,12])&lt;class &apos;list&apos;&gt;&gt;&gt;&gt; type(None)&lt;class &apos;NoneType&apos;&gt; 函数和类也可以使用type()来判断： 1234567891011&gt;&gt;&gt; class Person:... pass... &gt;&gt;&gt; def test():... pass... &gt;&gt;&gt; p = Person()&gt;&gt;&gt; type(p)&lt;class &apos;__main__.Person&apos;&gt;&gt;&gt;&gt; type(test)&lt;class &apos;function&apos;&gt; 利用type来作为if的判断条件，当类型不是int,str等基本类型时，需要使用types模块中定义的常量： 123456789101112&gt;&gt;&gt; import types&gt;&gt;&gt; def fn():... pass...&gt;&gt;&gt; type(fn)==types.FunctionTypeTrue&gt;&gt;&gt; type(abs)==types.BuiltinFunctionTypeTrue&gt;&gt;&gt; type(lambda x: x)==types.LambdaTypeTrue&gt;&gt;&gt; type((x for x in range(10)))==types.GeneratorTypeTrue 使用isinstance()来判断对象是否是某种类型isinstance()判断的是一个对象是否是该类型本身，或者位于该类型的父继承链上 能用type()判断的基本类型也可以用isinstance()判断: 123456&gt;&gt;&gt; isinstance(&apos;a&apos;, str)True&gt;&gt;&gt; isinstance(123, int)True&gt;&gt;&gt; isinstance(b&apos;a&apos;, bytes)True isinstance不仅可以判断是否是某种类型，还可以判断是否是某些类型的一种： 1234&gt;&gt;&gt; isinstance([1, 2, 3], (list, tuple))True&gt;&gt;&gt; isinstance((1, 2, 3), (list, tuple))True 使用dir()来获得一个对象的所有属性和方法获取str的所有属性和方法： 12&gt;&gt;&gt; dir(&apos;ABC&apos;)[&apos;__add__&apos;, &apos;__class__&apos;,..., &apos;__subclasshook__&apos;, &apos;capitalize&apos;, &apos;casefold&apos;,..., &apos;zfill&apos;] 利用getattr()、setattr()以及hasattr()，我们可以直接操作一个对象,不过这几个函数是在不知道对象信息的时候使用的，在了解对象信息时没有必要使用: 123456789101112131415161718192021&gt;&gt;&gt; class MyObject(object):... def __init__(self):... self.x = 9... def power(self):... return self.x * self.x...&gt;&gt;&gt; obj = MyObject()&gt;&gt;&gt; hasattr(obj, &apos;x&apos;) # 有属性&apos;x&apos;吗？True&gt;&gt;&gt; obj.x9&gt;&gt;&gt; hasattr(obj, &apos;y&apos;) # 有属性&apos;y&apos;吗？False&gt;&gt;&gt; setattr(obj, &apos;y&apos;, 19) # 设置一个属性&apos;y&apos;&gt;&gt;&gt; hasattr(obj, &apos;y&apos;) # 有属性&apos;y&apos;吗？True&gt;&gt;&gt; getattr(obj, &apos;y&apos;) # 获取属性&apos;y&apos;19&gt;&gt;&gt; obj.y # 获取属性&apos;y&apos;19 对于getattr()函数，可以在传入的属性参数后面加一个默认的参数，这样在对象不含有这个属性时就会返回默认参数，而不是抛出异常了： 123# 获取属性&apos;z&apos;，如果不存在，返回默认值404&gt;&gt;&gt; getattr(obj, &apos;z&apos;, 404) 404 getattr()函数还可以用于获取方法，把获取的方法赋值给一个变量，那个变量就指向这个方法，调用那个变量就相当于调用了这个方法： 1234567&gt;&gt;&gt; getattr(obj, &apos;power&apos;) # 获取属性&apos;power&apos;&lt;bound method MyObject.power of &lt;__main__.MyObject object at 0x10077a6a0&gt;&gt;&gt;&gt;&gt; fn = getattr(obj, &apos;power&apos;) # 获取属性&apos;power&apos;并赋值到变量fn&gt;&gt;&gt; fn # fn指向obj.power&lt;bound method MyObject.power of &lt;__main__.MyObject object at 0x10077a6a0&gt;&gt;&gt;&gt;&gt; fn() # 调用fn()与调用obj.power()是一样的81 实例属性和类属性实例属性我们在之前就已经使用过了，我们也了解到在python中可以给一个类实例绑定任何属性，方法是在类方法中利用self或者直接通过实例来绑定。 要给类绑定一个属性的话，可以在类中直接定义它： 12class Student(object): name = &apos;Student&apos; 也可以把类属性成为类的静态成员变量，这个属性是归类所有的，所有实例可以共享它。需要注意的是，虽然name属性归类Student所有，但是类的所有实例都可以访问到，并且实例属性的优先级比类属性高，所以如果实例绑定了一个与类属性同名的实例属性时，优先调用的是实例属性： 12345678910111213141516&gt;&gt;&gt; class Student(object):... name = &apos;Student&apos;...&gt;&gt;&gt; s = Student() # 创建实例s&gt;&gt;&gt; print(s.name) # 打印name属性，因为实例并没有name属性，所以会继续查找class的name属性Student&gt;&gt;&gt; print(Student.name) # 打印类的name属性Student&gt;&gt;&gt; s.name = &apos;Michael&apos; # 给实例绑定name属性&gt;&gt;&gt; print(s.name) # 由于实例属性优先级比类属性高，因此，它会屏蔽掉类的name属性Michael&gt;&gt;&gt; print(Student.name) # 但是类属性并未消失，用Student.name仍然可以访问Student&gt;&gt;&gt; del s.name # 如果删除实例的name属性&gt;&gt;&gt; print(s.name) # 再次调用s.name，由于实例的name属性没有找到，类的name属性就显示出来了Student 异常处理异常处理捕捉异常可以使用try/except语句。 try/except语句用来检测try语句块中的错误，从而让except语句捕获异常信息并处理。 如果你不想在异常发生时结束你的程序，只需在try里捕获它。 语法： 以下为简单的try....except...else的语法： 12345678try:&lt;语句&gt; #运行别的代码except &lt;名字&gt;：&lt;语句&gt; #如果在try部份引发了&apos;name&apos;异常except &lt;名字&gt;，&lt;数据&gt;:&lt;语句&gt; #如果引发了&apos;name&apos;异常，获得附加的数据else:&lt;语句&gt; #如果没有异常发生 try的工作原理是，当开始一个try语句后，python就在当前程序的上下文中作标记，这样当异常出现时就可以回到这里，try子句先执行，接下来会发生什么依赖于执行时是否出现异常。 如果当try后的语句执行时发生异常，python就跳回到try并执行第一个匹配该异常的except子句，异常处理完毕，控制流就通过整个try语句（除非在处理异常时又引发新的异常）。 如果在try后的语句里发生了异常，却没有匹配的except子句，异常将被递交到上层的try，或者到程序的最上层（这样将结束程序，并打印缺省的出错信息）。 如果在try子句执行时没有发生异常，python将执行else语句后的语句（如果有else的话），然后控制流通过整个try语句。 实例 下面是简单的例子，它打开一个文件，在该文件中的内容写入内容，且并未发生异常： 1234567891011#!/usr/bin/python# -*- coding: UTF-8 -*-try: fh = open(&quot;testfile&quot;, &quot;w&quot;) fh.write(&quot;这是一个测试文件，用于测试异常!!&quot;)except IOError: print &quot;Error: 没有找到文件或读取文件失败&quot;else: print &quot;内容写入文件成功&quot; fh.close() 以上程序输出结果： 1234$ python test.py 内容写入文件成功$ cat testfile # 查看写入的内容这是一个测试文件，用于测试异常!! 实例 下面是简单的例子，它打开一个文件，在该文件中的内容写入内容，但文件没有写入权限，发生了异常： 1234567891011#!/usr/bin/python# -*- coding: UTF-8 -*-try: fh = open(&quot;testfile&quot;, &quot;w&quot;) fh.write(&quot;这是一个测试文件，用于测试异常!!&quot;)except IOError: print &quot;Error: 没有找到文件或读取文件失败&quot;else: print &quot;内容写入文件成功&quot; fh.close() 在执行代码前为了测试方便，我们可以先去掉 testfile 文件的写权限，命令如下： 1chmod -w testfile 再执行以上代码： 12$ python test.py Error: 没有找到文件或读取文件失败 使用except而不带任何异常类型你可以不带任何异常类型使用except，如下实例： 12345678try: 正常的操作 ......................except: 发生异常，执行这块代码 ......................else: 如果没有异常执行这块代码 以上方式try-except语句捕获所有发生的异常。但这不是一个很好的方式，我们不能通过该程序识别出具体的异常信息。因为它捕获所有的异常。 使用except而带多种异常类型你也可以使用相同的except语句来处理多个异常信息，如下所示： 12345678try: 正常的操作 ......................except(Exception1[, Exception2[,...ExceptionN]]]): 发生以上多个异常中的一个，执行这块代码 ......................else: 如果没有异常执行这块代码 try-finally 语句try-finally 语句无论是否发生异常都将执行最后的代码。 12345try:&lt;语句&gt;finally:&lt;语句&gt; #退出try时总会执行raise 实例 12345678#!/usr/bin/python# -*- coding: UTF-8 -*-try: fh = open(&quot;testfile&quot;, &quot;w&quot;) fh.write(&quot;这是一个测试文件，用于测试异常!!&quot;)finally: print &quot;Error: 没有找到文件或读取文件失败&quot; 如果打开的文件没有可写权限，输出如下所示： 12$ python test.py Error: 没有找到文件或读取文件失败 同样的例子也可以写成如下方式： 123456789101112#!/usr/bin/python# -*- coding: UTF-8 -*-try: fh = open(&quot;testfile&quot;, &quot;w&quot;) try: fh.write(&quot;这是一个测试文件，用于测试异常!!&quot;) finally: print &quot;关闭文件&quot; fh.close()except IOError: print &quot;Error: 没有找到文件或读取文件失败&quot; 当在try块中抛出一个异常，立即执行finally块代码。 finally块中的所有语句执行后，异常被再次触发，并执行except块代码。 参数的内容不同于异常。 异常的参数一个异常可以带上参数，可作为输出的异常信息参数。 你可以通过except语句来捕获异常的参数，如下所示： 12345try: 正常的操作 ......................except ExceptionType, Argument: 你可以在这输出 Argument 的值... 变量接收的异常值通常包含在异常的语句中。在元组的表单中变量可以接收一个或者多个值。 元组通常包含错误字符串，错误数字，错误位置。 实例以下为单个异常的实例： 123456789101112#!/usr/bin/python# -*- coding: UTF-8 -*-# 定义函数def temp_convert(var): try: return int(var) except ValueError, Argument: print &quot;参数没有包含数字\n&quot;, Argument# 调用函数temp_convert(&quot;xyz&quot;); 以上程序执行结果如下： 123$ python test.py 参数没有包含数字invalid literal for int() with base 10: &apos;xyz&apos; 触发异常我们可以使用raise语句自己触发异常 raise语法格式如下： 1raise [Exception [, args [, traceback]]] 语句中 Exception 是异常的类型（例如，NameError）参数标准异常中任一种，args 是自已提供的异常参数。 最后一个参数是可选的（在实践中很少使用），如果存在，是跟踪异常对象。 实例一个异常可以是一个字符串，类或对象。 Python的内核提供的异常，大多数都是实例化的类，这是一个类的实例的参数。 定义一个异常非常简单，如下所示： 1234def functionName( level ): if level &lt; 1: raise Exception(&quot;Invalid level!&quot;, level) # 触发异常后，后面的代码就不会再执行 注意：为了能够捕获异常，”except“语句必须有用相同的异常来抛出类对象或者字符串。 例如我们捕获以上异常，”except“语句如下所示： 123456try: 正常逻辑except Exception,err: 触发自定义异常 else: 其余代码 实例 1234567891011121314#!/usr/bin/python# -*- coding: UTF-8 -*-# 定义函数def mye( level ): if level &lt; 1: raise Exception,&quot;Invalid level!&quot; # 触发异常后，后面的代码就不会再执行try: mye(0) # 触发异常except Exception,err: print 1,errelse: print 2 执行以上代码，输出结果为： 12$ python test.py 1 Invalid level! 用户自定义异常通过创建一个新的异常类，程序可以命名它们自己的异常。异常应该是典型的继承自Exception类，通过直接或间接的方式。 以下为与RuntimeError相关的实例,实例中创建了一个类，基类为RuntimeError，用于在异常触发时输出更多的信息。 在try语句块中，用户自定义的异常后执行except块语句，变量 e 是用于创建Networkerror类的实例。 123class Networkerror(RuntimeError): def __init__(self, arg): self.args = arg 在你定义以上类后，你可以触发该异常，如下所示： 1234try: raise Networkerror(&quot;Bad hostname&quot;)except Networkerror,e: print e.args]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 爬虫从零开始(三)]]></title>
    <url>%2F2019%2F06%2F23%2FPython%E7%88%AC%E8%99%AB%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B(%E4%B8%89)%2F</url>
    <content type="text"><![CDATA[Python 基础函数定义函数在Python中定义函数需要使用def语句，接着是函数名、左括号、参数、右括号、冒号：，在缩进块中编写函数体，返回值使用return语句返回。例子：定义my_fun 1234567def my_fun(): print(&quot;Hello World!&quot;)my_fun()#结果 输出Hello World! 函数调用定义一个函数：给了函数一个名称，指定了函数里包含的参数，和代码块结构。 这个函数的基本结构完成以后，你可以通过另一个函数调用执行，也可以直接从 Python 命令提示符执行。 如下实例调用了 printme() 函数： 123456789# 定义函数def printme( str ): # 打印任何传入的字符串 print (str) return # 调用函数printme(&quot;我要调用用户自定义函数!&quot;)printme(&quot;再次调用同一函数&quot;) 以上实例输出结果： 12我要调用用户自定义函数!再次调用同一函数 参数传递在 python 中，类型属于对象，变量是没有类型的： 12a=[1,2,3]a=&quot;Runoob&quot; 以上代码中，[1,2,3] 是 List 类型，”Runoob” 是 String 类型，而变量 a 是没有类型，她仅仅是一个对象的引用（一个指针），可以是指向 List 类型对象，也可以是指向 String 类型对象。 可更改(mutable)与不可更改(immutable)对象在 python 中，strings, tuples, 和 numbers 是不可更改的对象，而 list,dict 等则是可以修改的对象。 不可变类型：变量赋值 a=5 后再赋值 a=10，这里实际是新生成一个 int 值对象 10，再让 a 指向它，而 5 被丢弃，不是改变a的值，相当于新生成了a。 可变类型：变量赋值 la=[1,2,3,4] 后再赋值 la[2]=5 则是将 list la 的第三个元素值更改，本身la没有动，只是其内部的一部分值被修改了。 python 函数的参数传递： 不可变类型：类似 c++ 的值传递，如 整数、字符串、元组。如fun（a），传递的只是a的值，没有影响a对象本身。比如在 fun（a）内部修改 a 的值，只是修改另一个复制的对象，不会影响 a 本身。 可变类型：类似 c++ 的引用传递，如 列表，字典。如 fun（la），则是将 la 真正的传过去，修改后fun外部的la也会受影响 python 中一切都是对象，严格意义我们不能说值传递还是引用传递，我们应该说传不可变对象和传可变对象。 python 传不可变对象实例123456def ChangeInt( a ): a = 10 b = 2ChangeInt(b)print( b ) # 结果是 2 实例中有 int 对象 2，指向它的变量是 b，在传递给 ChangeInt 函数时，按传值的方式复制了变量 b，a 和 b 都指向了同一个 Int 对象，在 a=10 时，则新生成一个 int 值对象 10，并让 a 指向它。 传递可变对象实例可变对象在函数里修改了参数，那么在调用这个函数的函数里，原始的参数也被改变了。例如： 1234567891011# 可写函数说明def changeme( mylist ): &quot;修改传入的列表&quot; mylist.append([1,2,3,4]) print (&quot;函数内取值: &quot;, mylist) return # 调用changeme函数mylist = [10,20,30]changeme( mylist )print (&quot;函数外取值: &quot;, mylist) 传入函数的和在末尾添加新内容的对象用的是同一个引用。故输出结果如下： 12函数内取值: [10, 20, 30, [1, 2, 3, 4]]函数外取值: [10, 20, 30, [1, 2, 3, 4]] 参数以下是调用函数时可使用的正式参数类型： 必需参数 关键字参数 默认参数 不定长参数 必需参数必需参数须以正确的顺序传入函数。调用时的数量必须和声明时的一样。 调用 printme() 函数，你必须传入一个参数，不然会出现语法错误： 实例 12345678#可写函数说明def printme( str ): &quot;打印任何传入的字符串&quot; print (str) return # 调用 printme 函数，不加参数会报错printme() 以上实例输出结果： 1234Traceback (most recent call last): File &quot;test.py&quot;, line 10, in &lt;module&gt; printme()TypeError: printme() missing 1 required positional argument: &apos;str&apos; 关键字参数关键字参数和函数调用关系紧密，函数调用使用关键字参数来确定传入的参数值。 使用关键字参数允许函数调用时参数的顺序与声明时不一致，因为 Python 解释器能够用参数名匹配参数值。 以下实例在函数 printme() 调用时使用参数名： 实例 12345678#可写函数说明def printme( str ): &quot;打印任何传入的字符串&quot; print (str) return #调用printme函数printme( str = &quot;elichen&quot;) 以上实例输出结果： 1elichen 以下实例中演示了函数参数的使用不需要使用指定顺序： 123456789#可写函数说明def printinfo( name, age ): &quot;打印任何传入的字符串&quot; print (&quot;名字: &quot;, name) print (&quot;年龄: &quot;, age) return #调用printinfo函数printinfo( age=50, name=&quot;runoob&quot; ) 以上实例输出结果： 12名字: runoob年龄: 50 默认参数调用函数时，如果没有传递参数，则会使用默认参数。以下实例中如果没有传入 age 参数，则使用默认值： 实例 1234567891011#可写函数说明def printinfo( name, age = 35 ): &quot;打印任何传入的字符串&quot; print (&quot;名字: &quot;, name) print (&quot;年龄: &quot;, age) return #调用printinfo函数printinfo( age=50, name=&quot;runoob&quot; )print (&quot;------------------------&quot;)printinfo( name=&quot;runoob&quot; ) 以上实例输出结果： 12345名字: runoob年龄: 50------------------------名字: runoob年龄: 35 不定长参数你可能需要一个函数能处理比当初声明时更多的参数。这些参数叫做不定长参数，和上述 2 种参数不同，声明时不会命名。基本语法如下： 1234def functionname([formal_args,] *var_args_tuple ): &quot;函数_文档字符串&quot; function_suite return [expression] 加了星号 * 的参数会以元组(tuple)的形式导入，存放所有未命名的变量参数。 实例 123456789# 可写函数说明def printinfo( arg1, *vartuple ): &quot;打印任何传入的参数&quot; print (&quot;输出: &quot;) print (arg1) print (vartuple) # 调用printinfo 函数printinfo( 70, 60, 50 ) 以上实例输出结果： 123输出: 70(60, 50) 如果在函数调用时没有指定参数，它就是一个空元组。我们也可以不向函数传递未命名的变量。如下实例： 123456789101112# 可写函数说明def printinfo( arg1, *vartuple ): &quot;打印任何传入的参数&quot; print (&quot;输出: &quot;) print (arg1) for var in vartuple: print (var) return # 调用printinfo 函数printinfo( 10 )printinfo( 70, 60, 50 ) 以上实例输出结果： 123456输出:10输出:706050 还有一种就是参数带两个星号 **基本语法如下： 1234def functionname([formal_args,] **var_args_dict ): &quot;函数_文档字符串&quot; function_suite return [expression] 加了两个星号 ** 的参数会以字典的形式导入。 123456789# 可写函数说明def printinfo( arg1, **vardict ): &quot;打印任何传入的参数&quot; print (&quot;输出: &quot;) print (arg1) print (vardict) # 调用printinfo 函数printinfo(1, a=2,b=3) 以上实例输出结果： 123输出: 1&#123;&apos;a&apos;: 2, &apos;b&apos;: 3&#125; 声明函数时，参数中星号 * 可以单独出现，例如: 12def f(a,b,*,c): return a+b+c 如果单独出现星号 * 后的参数必须用关键字传入。 12345678910&gt;&gt;&gt; def f(a,b,*,c):... return a+b+c... &gt;&gt;&gt; f(1,2,3) # 报错Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: f() takes 2 positional arguments but 3 were given&gt;&gt;&gt; f(1,2,c=3) # 正常6&gt;&gt;&gt; 模块Python3模块在前面的几个章节中我们脚本上是用 python 解释器来编程，如果你从 Python 解释器退出再进入，那么你定义的所有的方法和变量就都消失了。 为此 Python 提供了一个办法，把这些定义存放在文件中，为一些脚本或者交互式的解释器实例使用，这个文件被称为模块。 模块是一个包含所有你定义的函数和变量的文件，其后缀名是 .py。模块可以被别的程序引入，以使用该模块中的函数等功能。这也是使用 python 标准库的方法。 下面是一个使用 python 标准库中模块的例子。 1234567import sys print(&apos;命令行参数如下:&apos;)for i in sys.argv: print(i) print(&apos;\n\nPython 路径为：&apos;, sys.path, &apos;\n&apos;) 执行结果如下所示： 12345678$ python using_sys.py 参数1 参数2命令行参数如下:using_sys.py参数1参数2Python 路径为： [&apos;/root&apos;, &apos;/usr/lib/python3.4&apos;, &apos;/usr/lib/python3.4/plat-x86_64-linux-gnu&apos;, &apos;/usr/lib/python3.4/lib-dynload&apos;, &apos;/usr/local/lib/python3.4/dist-packages&apos;, &apos;/usr/lib/python3/dist-packages&apos;] 1、import sys 引入 python 标准库中的 sys.py 模块；这是引入某一模块的方法。 2、sys.argv 是一个包含命令行参数的列表。 3、sys.path 包含了一个 Python 解释器自动查找所需模块的路径的列表。导入模块语句想使用 Python 源文件，只需在另一个源文件里执行 import 语句，语法如下：1import module1[, module2[,... moduleN] Python 的 from 语句让你从模块中导入一个指定的部分到当前命名空间中，语法如下： 1from modname import name1[, name2[, ... nameN]] 把一个模块的所有内容全都导入到当前的命名空间也是可行的，只需使用如下声明： 1from modname import *]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 爬虫从零开始(二)]]></title>
    <url>%2F2019%2F06%2F22%2FPython%E7%88%AC%E8%99%AB%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[Python 基础基本数据类型Python 中的变量不需要声明。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。在 Python 中，变量就是变量，它没有类型，我们所说的”类型”是变量所指的内存中对象的类型。 123a = 100 #整型变量b = 100.1 #浮点型变量c = &quot;elichen&quot; #字符型 Python允许多个变量同时赋值例如 1a = b = c = 1 你也可以为多个对象指定多个变量 1a, b, c = 1, 2, &quot;elichen&quot; 标准数据类型Python3 中有六个标准数据类型： Number （数字） String （字符串） List （列表） Tuple （元组） Set （集合） Dictionary（字典） Python3 的六个标准数据类型中： 不可变数据（3 个）：Number（数字）、String（字符串）、Tuple（元组）； 可变数据（3 个）：List（列表）、Dictionary（字典）、Set（集合）。 1、Number（数字）Python3 支持 int、float、bool、complex（复数） 关于布尔类型： Python3中，把True和False定义成关键字，他们的值是1和0，它们可以和数字相加。 在Python 3里，只有一种整数类型 int，表示为长整型，没有 python2 中的 Long。 像大多数语言一样，数值类型的赋值和计算都是很直观的。 内置的 type() 函数可以用来查询变量所指的对象类型。 此外还可以用 isinstance 来判断： 123a = 1isinstance(a, int)# 结果为True isinstance 和 type 的区别在于： type()不会认为子类是一种父类类型。 isinstance()会认为子类是一种父类类型。 当你指定一个值时，Number 对象就会被创建： 12var1 = 1var2 = 10 你也可以使用del语句删除一些对象引用。 del语句的语法是： 1del var1[,var2[,var3[....,varN]]] 您可以通过使用del语句删除单个或多个对象。例如: 12del vardel var_a, var_b 数值运算 1234567891011121314&gt;&gt;&gt;5 + 4 # 加法9&gt;&gt;&gt; 4.3 - 2 # 减法2.3&gt;&gt;&gt; 3 * 7 # 乘法21&gt;&gt;&gt; 2 / 4 # 除法，得到一个浮点数0.5&gt;&gt;&gt; 2 // 4 # 除法，得到一个整数0&gt;&gt;&gt; 17 % 3 # 取余 2&gt;&gt;&gt; 2 ** 5 # 乘方32 2、String（字符串）Python中的字符串用单引号(‘)或双引号(“)括起来，同时使用反斜杠()转义特殊字符。 字符串的截取的语法格式如下： 变量[头下标:尾下标] 加号 + 是字符串的连接符， 星号 * 表示复制当前字符串，紧跟的数字为复制的次数。实例如下： 123456789str = &apos;Elichen&apos; print (str) # 输出字符串print (str[0:-1]) # 输出第一个到倒数第二个的所有字符print (str[0]) # 输出字符串第一个字符print (str[2:5]) # 输出从第三个开始到第五个的字符print (str[2:]) # 输出从第三个开始的后的所有字符print (str * 2) # 输出字符串两次print (str + &quot;TEST&quot;) # 连接字符串 执行以上程序会输出如下结果： 1234567ElichenElicheEichichenElichenElichenElichenTEST Python 使用反斜杠()转义特殊字符，如果你不想让反斜杠发生转义，可以在字符串前面添加一个 r，表示原始字符串： 123456&gt;&gt;&gt; print(&apos;Ru\noob&apos;)Ruoob&gt;&gt;&gt; print(r&apos;Ru\noob&apos;)Ru\noob&gt;&gt;&gt; 3、List（列表）List（列表） 是 Python 中使用最频繁的数据类型。 列表可以完成大多数集合类的数据结构实现。列表中元素的类型可以不相同，它支持数字，字符串甚至可以包含列表（所谓嵌套）。 列表是写在方括号 [] 之间、用逗号分隔开的元素列表。 和字符串一样，列表同样可以被索引和截取，列表被截取后返回一个包含所需元素的新列表。 列表截取的语法格式如下： 变量[头下标:尾下标] 加号 + 是列表连接运算符，星号 * 是重复操作。如下实例： 123456789list = [&apos;abcd&apos;, 786, 2.23, &apos;elichen&apos;, 70.2]tinylist = [123, &apos;elichen&apos;]print(list) # 输出完整列表print(list[0]) # 输出列表第一个元素print(list[1:3]) # 从第二个开始输出到第三个元素print(list[2:]) # 输出从第三个元素开始的所有元素print(tinylist * 2) # 输出两次列表print(list + tinylist) # 连接列表 以上实例输出结果： 123456[&apos;abcd&apos;, 786, 2.23, &apos;elichen&apos;, 70.2]abcd[786, 2.23][2.23, &apos;elichen&apos;, 70.2][123, &apos;elichen&apos;, 123, &apos;elichen&apos;][&apos;abcd&apos;, 786, 2.23, &apos;elichen&apos;, 70.2, 123, &apos;elichen&apos;] 与Python字符串不一样的是，列表中的元素是可以改变的： 12345678&gt;&gt;&gt;a = [1, 2, 3, 4, 5, 6]&gt;&gt;&gt; a[0] = 9&gt;&gt;&gt; a[2:5] = [13, 14, 15]&gt;&gt;&gt; a[9, 2, 13, 14, 15, 6]&gt;&gt;&gt; a[2:5] = [] # 将对应的元素值设置为 [] &gt;&gt;&gt; a[9, 2, 6] List写在方括号之间，元素用逗号隔开。 和字符串一样，list可以被索引和切片。 List可以使用+操作符进行拼接。 List中的元素是可以改变的。 Python 列表截取可以接收第三个参数，参数作用是截取的步长，以下实例在索引 1 到索引 4 的位置并设置为步长为 2（间隔一个位置）来截取字符串： 123list1 = [&apos;e&apos;, &apos;l&apos;, &apos;i&apos;, &apos;c&apos;, &apos;h&apos;, &apos;e&apos;, &apos;n&apos;]print(list1[1:4:2])# 结果为 [&apos;l&apos;, &apos;c&apos;] 4、Tuple（元组）语法：(,,,)。使用()标识，元素之间用逗号分隔。 元组（tuple）与列表类似，不同之处在于元组的元素不能修改。 元组中的元素类型也可以不相同； 123456789tuple = (&apos;abcd&apos;, 786, 2.23, &apos;elichen&apos;, 70.2)tinytuple = (123, &apos;elichen&apos;)print(tuple) # 输出完整元组print(tuple[0]) # 输出元组的第一个元素print(tuple[1:3]) # 输出从第二个元素开始到第三个元素print(tuple[2:]) # 输出从第三个元素开始的所有元素print(tinytuple * 2) # 输出两次元组print(tuple + tinytuple) # 连接元组 以上实例输出结果： 123456(&apos;abcd&apos;, 786, 2.23, &apos;elichen&apos;, 70.2)abcd(786, 2.23)(2.23, &apos;elichen&apos;, 70.2)(123, &apos;elichen&apos;, 123, &apos;elichen&apos;)(&apos;abcd&apos;, 786, 2.23, &apos;elichen&apos;, 70.2, 123, &apos;elichen&apos;) 虽然tuple的元素不可改变，但它可以包含可变的对象，比如list列表。 构造包含 0 个或 1 个元素的元组比较特殊，所以有一些额外的语法规则： 12tup1 = () # 空元组tup2 = (20,) # 一个元素，需要在元素后添加逗号 string、list 和 tuple 都属于 sequence（序列）。 注意： 1、与字符串一样，元组的元素不能修改。 2、元组也可以被索引和切片，方法一样。 3、注意构造包含 0 或 1 个元素的元组的特殊语法规则。 4、元组也可以使用+操作符进行拼接。 5、Set（集合）集合（set）是一个无序不重复元素的序列。创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典。 创建格式： 123parame = &#123;value01,value02,...&#125;或者set(value) 实例 12345678910111213141516171819student = &#123;&apos;Tom&apos;, &apos;Jim&apos;, &apos;Mary&apos;, &apos;Tom&apos;, &apos;Jack&apos;, &apos;Rose&apos;&#125; print(student) # 输出集合，重复的元素被自动去掉 # 成员测试if &apos;Rose&apos; in student : print(&apos;Rose 在集合中&apos;)else : print(&apos;Rose 不在集合中&apos;)# set可以进行集合运算a = set(&apos;abracadabra&apos;)b = set(&apos;alacazam&apos;) print(a)print(a - b) # a 和 b 的差集print(a | b) # a 和 b 的并集print(a &amp; b) # a 和 b 的交集print(a ^ b) # a 和 b 中不同时存在的元素 以上实例输出结果： 1234567&#123;&apos;Mary&apos;, &apos;Jim&apos;, &apos;Rose&apos;, &apos;Jack&apos;, &apos;Tom&apos;&#125;Rose 在集合中&#123;&apos;b&apos;, &apos;a&apos;, &apos;c&apos;, &apos;r&apos;, &apos;d&apos;&#125;&#123;&apos;b&apos;, &apos;d&apos;, &apos;r&apos;&#125;&#123;&apos;l&apos;, &apos;r&apos;, &apos;a&apos;, &apos;c&apos;, &apos;z&apos;, &apos;m&apos;, &apos;b&apos;, &apos;d&apos;&#125;&#123;&apos;a&apos;, &apos;c&apos;&#125;&#123;&apos;l&apos;, &apos;r&apos;, &apos;z&apos;, &apos;m&apos;, &apos;b&apos;, &apos;d&apos;&#125; 6、Dictionary（字典）字典是一种映射类型，字典用{ }标识，它是一个无序的键(key) : 值(value)对集合。 字典（dictionary）是Python中另一个非常有用的内置数据类型，键(key)必须使用不可变类型，键(key)必须是唯一的。 123456789101112dict = &#123;&#125;dict[&apos;one&apos;] = &quot;1&quot;dict[2] = &quot;2&quot; tinydict = &#123;&apos;name&apos;: &apos;elichen&apos;,&apos;code&apos;:1, &apos;site&apos;: &apos;www.elichen.club&apos;&#125; print (dict[&apos;one&apos;]) # 输出键为 &apos;one&apos; 的值print (dict[2]) # 输出键为 2 的值print (tinydict) # 输出完整的字典print (tinydict.keys()) # 输出所有键print (tinydict.values()) # 输出所有值 以上实例输出结果： 1234512&#123;&apos;name&apos;: &apos;elichen&apos;, &apos;code&apos;: 1, &apos;site&apos;: &apos;www.elichen.club&apos;&#125;dict_keys([&apos;name&apos;, &apos;code&apos;, &apos;site&apos;])dict_values([&apos;elichen&apos;, 1, &apos;www.elichen.club&apos;]) 注意： 1、字典是一种映射类型，它的元素是键值对。 2、字典的关键字必须为不可变类型，且不能重复。 3、创建空字典使用 { }。 Python数据类型转换有时候，我们需要对数据内置的类型进行转换，数据类型的转换，你只需要将数据类型作为函数名即可。 以下几个内置的函数可以执行数据类型之间的转换。这些函数返回一个新的对象，表示转换的值。 函数 描述 int(x [,base]) 将x转换为一个整数 float(x) 将x转换到一个浮点数 complex(real [,imag]) 创建一个复数 str(x) 将对象 x 转换为字符串 repr(x) 将对象 x 转换为表达式字符串 eval(str) 用来计算在字符串中的有效Python表达式,并返回一个对象 tuple(s) 将序列 s 转换为一个元组 list(s) 将序列 s 转换为一个列表 set(s) 转换为可变集合 dict(d) 创建一个字典。d 必须是一个序列 (key,value)元组。 frozenset(s) 转换为不可变集合 chr(x) 将一个整数转换为一个字符 ord(x) 将一个字符转换为它的整数值 hex(x) 将一个整数转换为一个十六进制字符串 oct(x) 将一个整数转换为一个八进制字符串]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 爬虫从零开始(一)]]></title>
    <url>%2F2019%2F06%2F21%2FPython%E7%88%AC%E8%99%AB%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[（一） Python环境搭建第一步:Python安装包下载首先从 Python官网 下载Python安装程序，注意32位和64位的区别，如果你的电脑是32位的请选择32位的安装包，如果你的电脑是64位的请选择64位的安装包。 第二步:安装双击下载好的Python安装包。 勾选添加Python到环境变量，选择默认安装或者自定义安装，如果忘记打勾，则需要手工加到环境变量中；在这里我选择的是自定义安装，点击“自定义安装”进行下一步操作； 进入到下一步后，选择需要安装的组件，然后点击下一步: 在这里可以自定义路径选择安装： 点击下一步后，就开始真正安装了，安装完成后点击关闭即可。 第三步: 测试python安装好之后，我们要检测一下是否安装成功，用系统管理员打开命令行工具cmd，输入“python”,然后敲回车，如果出现如下界面，则表示我们安装成功了； 如果不显示版本号，提示‘python’ 不是内部或外部命令，也不是可运行的程序或批处理文件，则需要手动将python.exe所在的目录添加到环境变量PATH中。 （二）pycharm的安装与配置第一步:下载pycharm点击 pycharm官网 进入pycharm 官网下载免费的Community版本 第二步:安装pycharm双击下载的安装包,出现如下界面点击next 可以选择安装位置，继续点击next 勾选以下选项，点击next 点击install 第三步:pycharm的入门基础配置安装完成打开pycharm显示如下，因为之前安装过所以会让我当如配置文件，没安装过的选择不导入设置，点击OK。 同意协议点击继续 选择喜欢的主题 选择你要安装的插件继续 打开设置 展开editor 双击Font 设置你喜欢的字体样式和大小 第四步:pycharm的Python解释器的搭建 完成如下操作点击OK。 点击OK完成设置 更新Python解释器 完成后就可以新建工程愉快的写代码了。 第五步:pycharm的建立一个新项目点击create New Project 选择项目路径 选择项目解释器，选择刚刚设置的Python解释器 点击create 项目目录右键点击选择new 选择 Python File 给文件起个名字点击OK 写入代码右键单击编辑区选择Run 输出结果 大功告成! 至此，pycharm已经全部安装完成。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>pycharm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper常用Shell命令]]></title>
    <url>%2F2019%2F06%2F10%2FZookeeper%E5%B8%B8%E7%94%A8Shell%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Zookeeper常用Shell命令一、节点增删改查1.1 启动服务和连接服务12345# 启动服务bin/zkServer.sh start#连接服务 不指定服务地址则默认连接到localhost:2181zkCli.sh -server hadoop001:2181 1.2 help命令使用 help 可以查看所有命令及格式。 1.3 查看节点列表查看节点列表有 ls path 和 ls2 path 两个命令，后者是前者的增强，不仅可以查看指定路径下的所有节点，还可以查看当前节点的信息。 123456789101112131415[zk: localhost:2181(CONNECTED) 0] ls /[cluster, controller_epoch, brokers, storm, zookeeper, admin, ...][zk: localhost:2181(CONNECTED) 1] ls2 /[cluster, controller_epoch, brokers, storm, zookeeper, admin, ....]cZxid = 0x0ctime = Thu Jan 01 08:00:00 CST 1970mZxid = 0x0mtime = Thu Jan 01 08:00:00 CST 1970pZxid = 0x130cversion = 19dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 0numChildren = 11 1.4 新增节点1create [-s] [-e] path data acl #其中-s 为有序节点，-e 临时节点 创建节点并写入数据： 1create /hadoop 123456 创建有序节点，此时创建的节点名为指定节点名 + 自增序号： 123456[zk: localhost:2181(CONNECTED) 23] create -s /a "aaa"Created /a0000000022[zk: localhost:2181(CONNECTED) 24] create -s /b "bbb"Created /b0000000023[zk: localhost:2181(CONNECTED) 25] create -s /c "ccc"Created /c0000000024 创建临时节点，临时节点会在会话过期后被删除： 12[zk: localhost:2181(CONNECTED) 26] create -e /tmp "tmp"Created /tmp 1.5 查看节点1. 获取节点数据12# 格式get path [watch] 12345678910111213[zk: localhost:2181(CONNECTED) 31] get /hadoop123456 #节点数据cZxid = 0x14bctime = Fri May 24 17:03:06 CST 2019mZxid = 0x14bmtime = Fri May 24 17:03:06 CST 2019pZxid = 0x14bcversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 6numChildren = 0 节点各个属性如下表。其中一个重要的概念是 Zxid(ZooKeeper Transaction Id)，ZooKeeper 节点的每一次更改都具有唯一的 Zxid，如果 Zxid1 小于 Zxid2，则 Zxid1 的更改发生在 Zxid2 更改之前。 状态属性 说明 cZxid 数据节点创建时的事务 ID ctime 数据节点创建时的时间 mZxid 数据节点最后一次更新时的事务 ID mtime 数据节点最后一次更新时的时间 pZxid 数据节点的子节点最后一次被修改时的事务 ID cversion 子节点的更改次数 dataVersion 节点数据的更改次数 aclVersion 节点的 ACL 的更改次数 ephemeralOwner 如果节点是临时节点，则表示创建该节点的会话的 SessionID；如果节点是持久节点，则该属性值为 0 dataLength 数据内容的长度 numChildren 数据节点当前的子节点个数 2. 查看节点状态可以使用 stat 命令查看节点状态，它的返回值和 get 命令类似，但不会返回节点数据。 123456789101112[zk: localhost:2181(CONNECTED) 32] stat /hadoopcZxid = 0x14bctime = Fri May 24 17:03:06 CST 2019mZxid = 0x14bmtime = Fri May 24 17:03:06 CST 2019pZxid = 0x14bcversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 6numChildren = 0 1.6 更新节点更新节点的命令是 set，可以直接进行修改，如下： 123456789101112[zk: localhost:2181(CONNECTED) 33] set /hadoop 345cZxid = 0x14bctime = Fri May 24 17:03:06 CST 2019mZxid = 0x14cmtime = Fri May 24 17:13:05 CST 2019pZxid = 0x14bcversion = 0dataVersion = 1 # 注意更改后此时版本号为 1，默认创建时为 0aclVersion = 0ephemeralOwner = 0x0dataLength = 3numChildren = 0 也可以基于版本号进行更改，此时类似于乐观锁机制，当你传入的数据版本号 (dataVersion) 和当前节点的数据版本号不符合时，zookeeper 会拒绝本次修改： 12[zk: localhost:2181(CONNECTED) 34] set /hadoop 678 0version No is not valid : /hadoop #无效的版本号 1.7 删除节点删除节点的语法如下： 1delete path [version] 和更新节点数据一样，也可以传入版本号，当你传入的数据版本号 (dataVersion) 和当前节点的数据版本号不符合时，zookeeper 不会执行删除操作。 1234[zk: localhost:2181(CONNECTED) 36] delete /hadoop 0version No is not valid : /hadoop #无效的版本号[zk: localhost:2181(CONNECTED) 37] delete /hadoop 1[zk: localhost:2181(CONNECTED) 38] 要想删除某个节点及其所有后代节点，可以使用递归删除，命令为 rmr path。 二、监听器2.1 get path [watch]使用 get path [watch] 注册的监听器能够在节点内容发生改变的时候，向客户端发出通知。需要注意的是 zookeeper 的触发器是一次性的 (One-time trigger)，即触发一次后就会立即失效。 1234[zk: localhost:2181(CONNECTED) 4] get /hadoop watch[zk: localhost:2181(CONNECTED) 5] set /hadoop 45678WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/hadoop #节点值改变 2.2 stat path [watch]使用 stat path [watch] 注册的监听器能够在节点状态发生改变的时候，向客户端发出通知。 1234[zk: localhost:2181(CONNECTED) 7] stat /hadoop watch[zk: localhost:2181(CONNECTED) 8] set /hadoop 112233WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/hadoop #节点值改变 2.3 ls\ls2 path [watch]使用 ls path [watch] 或 ls2 path [watch] 注册的监听器能够监听该节点下所有子节点的增加和删除操作。 12345[zk: localhost:2181(CONNECTED) 9] ls /hadoop watch[][zk: localhost:2181(CONNECTED) 10] create /hadoop/yarn "aaa"WATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/hadoop 三、 zookeeper 四字命令 命令 功能描述 conf 打印服务配置的详细信息。 cons 列出连接到此服务器的所有客户端的完整连接/会话详细信息。包括接收/发送的数据包数量，会话 ID，操作延迟，上次执行的操作等信息。 dump 列出未完成的会话和临时节点。这只适用于 Leader 节点。 envi 打印服务环境的详细信息。 ruok 测试服务是否处于正确状态。如果正确则返回“imok”，否则不做任何相应。 stat 列出服务器和连接客户端的简要详细信息。 wchs 列出所有 watch 的简单信息。 wchc 按会话列出服务器 watch 的详细信息。 wchp 按路径列出服务器 watch 的详细信息。 更多四字命令可以参阅官方文档：https://zookeeper.apache.org/doc/current/zookeeperAdmin.html 使用前需要使用 yum install nc 安装 nc 命令，使用示例如下： 1234567891011121314[root@hadoop001 bin]# echo stat | nc localhost 2181Zookeeper version: 3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 04:05 GMTClients: /0:0:0:0:0:0:0:1:50584[1](queued=0,recved=371,sent=371) /0:0:0:0:0:0:0:1:50656[0](queued=0,recved=1,sent=0)Latency min/avg/max: 0/0/19Received: 372Sent: 371Connections: 2Outstanding: 0Zxid: 0x150Mode: standaloneNode count: 167]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper简介及核心概念]]></title>
    <url>%2F2019%2F06%2F05%2FZookeeper%E7%AE%80%E4%BB%8B%E5%8F%8A%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[Zookeeper简介及核心概念一、Zookeeper简介Zookeeper 是一个开源的分布式协调服务，目前由 Apache 进行维护。Zookeeper 可以用于实现分布式系统中常见的发布/订阅、负载均衡、命令服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。它具有以下特性： 顺序一致性：从一个客户端发起的事务请求，最终都会严格按照其发起顺序被应用到 Zookeeper 中； 原子性：所有事务请求的处理结果在整个集群中所有机器上都是一致的；不存在部分机器应用了该事务，而另一部分没有应用的情况； 单一视图：所有客户端看到的服务端数据模型都是一致的； 可靠性：一旦服务端成功应用了一个事务，则其引起的改变会一直保留，直到被另外一个事务所更改； 实时性：一旦一个事务被成功应用后，Zookeeper 可以保证客户端立即可以读取到这个事务变更后的最新状态的数据。 二、Zookeeper设计目标Zookeeper 致力于为那些高吞吐的大型分布式系统提供一个高性能、高可用、且具有严格顺序访问控制能力的分布式协调服务。它具有以下四个目标： 2.1 目标一：简单的数据模型Zookeeper 通过树形结构来存储数据，它由一系列被称为 ZNode 的数据节点组成，类似于常见的文件系统。不过和常见的文件系统不同，Zookeeper 将数据全量存储在内存中，以此来实现高吞吐，减少访问延迟。 2.2 目标二：构建集群可以由一组 Zookeeper 服务构成 Zookeeper 集群，集群中每台机器都会单独在内存中维护自身的状态，并且每台机器之间都保持着通讯，只要集群中有半数机器能够正常工作，那么整个集群就可以正常提供服务。 2.3 目标三：顺序访问对于来自客户端的每个更新请求，Zookeeper 都会分配一个全局唯一的递增 ID，这个 ID 反映了所有事务请求的先后顺序。 2.4 目标四：高性能高可用ZooKeeper 将数据存全量储在内存中以保持高性能，并通过服务集群来实现高可用，由于 Zookeeper 的所有更新和删除都是基于事务的，所以其在读多写少的应用场景中有着很高的性能表现。 三、核心概念3.1 集群角色Zookeeper 集群中的机器分为以下三种角色： Leader ：为客户端提供读写服务，并维护集群状态，它是由集群选举所产生的； Follower ：为客户端提供读写服务，并定期向 Leader 汇报自己的节点状态。同时也参与写操作“过半写成功”的策略和 Leader 的选举； Observer ：为客户端提供读写服务，并定期向 Leader 汇报自己的节点状态，但不参与写操作“过半写成功”的策略和 Leader 的选举，因此 Observer 可以在不影响写性能的情况下提升集群的读性能。 3.2 会话Zookeeper 客户端通过 TCP 长连接连接到服务集群，会话 (Session) 从第一次连接开始就已经建立，之后通过心跳检测机制来保持有效的会话状态。通过这个连接，客户端可以发送请求并接收响应，同时也可以接收到 Watch 事件的通知。 关于会话中另外一个核心的概念是 sessionTimeOut(会话超时时间)，当由于网络故障或者客户端主动断开等原因，导致连接断开，此时只要在会话超时时间之内重新建立连接，则之前创建的会话依然有效。 3.3 数据节点Zookeeper 数据模型是由一系列基本数据单元 Znode(数据节点) 组成的节点树，其中根节点为 /。每个节点上都会保存自己的数据和节点信息。Zookeeper 中节点可以分为两大类： 持久节点 ：节点一旦创建，除非被主动删除，否则一直存在； 临时节点 ：一旦创建该节点的客户端会话失效，则所有该客户端创建的临时节点都会被删除。 临时节点和持久节点都可以添加一个特殊的属性：SEQUENTIAL，代表该节点是否具有递增属性。如果指定该属性，那么在这个节点创建时，Zookeeper 会自动在其节点名称后面追加一个由父节点维护的递增数字。 3.4 节点信息每个 ZNode 节点在存储数据的同时，都会维护一个叫做 Stat 的数据结构，里面存储了关于该节点的全部状态信息。如下： 状态属性 说明 czxid 数据节点创建时的事务 ID ctime 数据节点创建时的时间 mzxid 数据节点最后一次更新时的事务 ID mtime 数据节点最后一次更新时的时间 pzxid 数据节点的子节点最后一次被修改时的事务 ID cversion 子节点的更改次数 version 节点数据的更改次数 aversion 节点的 ACL 的更改次数 ephemeralOwner 如果节点是临时节点，则表示创建该节点的会话的 SessionID；如果节点是持久节点，则该属性值为 0 dataLength 数据内容的长度 numChildren 数据节点当前的子节点个数 3.5 WatcherZookeeper 中一个常用的功能是 Watcher(事件监听器)，它允许用户在指定节点上针对感兴趣的事件注册监听，当事件发生时，监听器会被触发，并将事件信息推送到客户端。该机制是 Zookeeper 实现分布式协调服务的重要特性。 3.6 ACLZookeeper 采用 ACL(Access Control Lists) 策略来进行权限控制，类似于 UNIX 文件系统的权限控制。它定义了如下五种权限： CREATE：允许创建子节点； READ：允许从节点获取数据并列出其子节点； WRITE：允许为节点设置数据； DELETE：允许删除子节点； ADMIN：允许为节点设置权限。 四、ZAB协议4.1 ZAB协议与数据一致性ZAB 协议是 Zookeeper 专门设计的一种支持崩溃恢复的原子广播协议。通过该协议，Zookeepe 基于主从模式的系统架构来保持集群中各个副本之间数据的一致性。具体如下： Zookeeper 使用一个单一的主进程来接收并处理客户端的所有事务请求，并采用原子广播协议将数据状态的变更以事务 Proposal 的形式广播到所有的副本进程上去。如下图： 具体流程如下： 所有的事务请求必须由唯一的 Leader 服务来处理，Leader 服务将事务请求转换为事务 Proposal，并将该 Proposal 分发给集群中所有的 Follower 服务。如果有半数的 Follower 服务进行了正确的反馈，那么 Leader 就会再次向所有的 Follower 发出 Commit 消息，要求将前一个 Proposal 进行提交。 4.2 ZAB协议的内容ZAB 协议包括两种基本的模式，分别是崩溃恢复和消息广播： 1. 崩溃恢复当整个服务框架在启动过程中，或者当 Leader 服务器出现异常时，ZAB 协议就会进入恢复模式，通过过半选举机制产生新的 Leader，之后其他机器将从新的 Leader 上同步状态，当有过半机器完成状态同步后，就退出恢复模式，进入消息广播模式。 2. 消息广播ZAB 协议的消息广播过程使用的是原子广播协议。在整个消息的广播过程中，Leader 服务器会每个事物请求生成对应的 Proposal，并为其分配一个全局唯一的递增的事务 ID(ZXID)，之后再对其进行广播。具体过程如下： Leader 服务会为每一个 Follower 服务器分配一个单独的队列，然后将事务 Proposal 依次放入队列中，并根据 FIFO(先进先出) 的策略进行消息发送。Follower 服务在接收到 Proposal 后，会将其以事务日志的形式写入本地磁盘中，并在写入成功后反馈给 Leader 一个 Ack 响应。当 Leader 接收到超过半数 Follower 的 Ack 响应后，就会广播一个 Commit 消息给所有的 Follower 以通知其进行事务提交，之后 Leader 自身也会完成对事务的提交。而每一个 Follower 则在接收到 Commit 消息后，完成事务的提交。 五、Zookeeper的典型应用场景5.1数据的发布/订阅数据的发布/订阅系统，通常也用作配置中心。在分布式系统中，你可能有成千上万个服务节点，如果想要对所有服务的某项配置进行更改，由于数据节点过多，你不可逐台进行修改，而应该在设计时采用统一的配置中心。之后发布者只需要将新的配置发送到配置中心，所有服务节点即可自动下载并进行更新，从而实现配置的集中管理和动态更新。 Zookeeper 通过 Watcher 机制可以实现数据的发布和订阅。分布式系统的所有的服务节点可以对某个 ZNode 注册监听，之后只需要将新的配置写入该 ZNode，所有服务节点都会收到该事件。 5.2 命名服务在分布式系统中，通常需要一个全局唯一的名字，如生成全局唯一的订单号等，Zookeeper 可以通过顺序节点的特性来生成全局唯一 ID，从而可以对分布式系统提供命名服务。 5.3 Master选举分布式系统一个重要的模式就是主从模式 (Master/Salves)，Zookeeper 可以用于该模式下的 Matser 选举。可以让所有服务节点去竞争性地创建同一个 ZNode，由于 Zookeeper 不能有路径相同的 ZNode，必然只有一个服务节点能够创建成功，这样该服务节点就可以成为 Master 节点。 5.4 分布式锁可以通过 Zookeeper 的临时节点和 Watcher 机制来实现分布式锁，这里以排它锁为例进行说明： 分布式系统的所有服务节点可以竞争性地去创建同一个临时 ZNode，由于 Zookeeper 不能有路径相同的 ZNode，必然只有一个服务节点能够创建成功，此时可以认为该节点获得了锁。其他没有获得锁的服务节点通过在该 ZNode 上注册监听，从而当锁释放时再去竞争获得锁。锁的释放情况有以下两种： 当正常执行完业务逻辑后，客户端主动将临时 ZNode 删除，此时锁被释放； 当获得锁的客户端发生宕机时，临时 ZNode 会被自动删除，此时认为锁已经释放。 当锁被释放后，其他服务节点则再次去竞争性地进行创建，但每次都只有一个服务节点能够获取到锁，这就是排他锁。 5.5 集群管理Zookeeper 还能解决大多数分布式系统中的问题： 如可以通过创建临时节点来建立心跳检测机制。如果分布式系统的某个服务节点宕机了，则其持有的会话会超时，此时该临时节点会被删除，相应的监听事件就会被触发。 分布式系统的每个服务节点还可以将自己的节点状态写入临时节点，从而完成状态报告或节点工作进度汇报。 通过数据的订阅和发布功能，Zookeeper 还能对分布式系统进行模块的解耦和任务的调度。 通过监听机制，还能对分布式系统的服务节点进行动态上下线，从而实现服务的动态扩容。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群资源管理器——YARN]]></title>
    <url>%2F2019%2F05%2F22%2FHadoop-YARN%2F</url>
    <content type="text"><![CDATA[集群资源管理器——YARN一、hadoop yarn 简介Apache YARN (Yet Another Resource Negotiator) 是 hadoop 2.0 引入的集群资源管理系统。用户可以将各种服务框架部署在 YARN 上，由 YARN 进行统一地管理和资源分配。 二、YARN架构 1. ResourceManagerResourceManager 通常在独立的机器上以后台进程的形式运行，它是整个集群资源的主要协调者和管理者。ResourceManager 负责给用户提交的所有应用程序分配资源，它根据应用程序优先级、队列容量、ACLs、数据位置等信息，做出决策，然后以共享的、安全的、多租户的方式制定分配策略，调度集群资源。 2. NodeManagerNodeManager 是 YARN 集群中的每个具体节点的管理者。主要负责该节点内所有容器的生命周期的管理，监视资源和跟踪节点健康。具体如下： 启动时向 ResourceManager 注册并定时发送心跳消息，等待 ResourceManager 的指令； 维护 Container 的生命周期，监控 Container 的资源使用情况； 管理任务运行时的相关依赖，根据 ApplicationMaster 的需要，在启动 Container 之前将需要的程序及其依赖拷贝到本地。 3. ApplicationMaster在用户提交一个应用程序时，YARN 会启动一个轻量级的进程 ApplicationMaster。ApplicationMaster 负责协调来自 ResourceManager 的资源，并通过 NodeManager 监视容器内资源的使用情况，同时还负责任务的监控与容错。具体如下： 根据应用的运行状态来决定动态计算资源需求； 向 ResourceManager 申请资源，监控申请的资源的使用情况； 跟踪任务状态和进度，报告资源的使用情况和应用的进度信息； 负责任务的容错。 4. ContainContainer 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。当 AM 向 RM 申请资源时，RM 为 AM 返回的资源是用 Container 表示的。YARN 会为每个任务分配一个 Container，该任务只能使用该 Container 中描述的资源。ApplicationMaster 可在 Container 内运行任何类型的任务。例如，MapReduce ApplicationMaster 请求一个容器来启动 map 或 reduce 任务，而 Giraph ApplicationMaster 请求一个容器来运行 Giraph 任务。 三、YARN工作原理简述 Client 提交作业到 YARN 上； Resource Manager 选择一个 Node Manager，启动一个 Container 并运行 Application Master 实例； Application Master 根据实际需要向 Resource Manager 请求更多的 Container 资源（如果作业很小, 应用管理器会选择在其自己的 JVM 中运行任务）； Application Master 通过获取到的 Container 资源执行分布式计算。 四、YARN工作原理详述 1. 作业提交client 调用 job.waitForCompletion 方法，向整个集群提交 MapReduce 作业 (第 1 步) 。新的作业 ID(应用 ID) 由资源管理器分配 (第 2 步)。作业的 client 核实作业的输出, 计算输入的 split, 将作业的资源 (包括 Jar 包，配置文件, split 信息) 拷贝给 HDFS(第 3 步)。 最后, 通过调用资源管理器的 submitApplication() 来提交作业 (第 4 步)。 2. 作业初始化当资源管理器收到 submitApplciation() 的请求时, 就将该请求发给调度器 (scheduler), 调度器分配 container, 然后资源管理器在该 container 内启动应用管理器进程, 由节点管理器监控 (第 5 步)。 MapReduce 作业的应用管理器是一个主类为 MRAppMaster 的 Java 应用，其通过创造一些 bookkeeping 对象来监控作业的进度, 得到任务的进度和完成报告 (第 6 步)。然后其通过分布式文件系统得到由客户端计算好的输入 split(第 7 步)，然后为每个输入 split 创建一个 map 任务, 根据 mapreduce.job.reduces 创建 reduce 任务对象。 3. 任务分配如果作业很小, 应用管理器会选择在其自己的 JVM 中运行任务。 如果不是小作业, 那么应用管理器向资源管理器请求 container 来运行所有的 map 和 reduce 任务 (第 8 步)。这些请求是通过心跳来传输的, 包括每个 map 任务的数据位置，比如存放输入 split 的主机名和机架 (rack)，调度器利用这些信息来调度任务，尽量将任务分配给存储数据的节点, 或者分配给和存放输入 split 的节点相同机架的节点。 4. 任务运行当一个任务由资源管理器的调度器分配给一个 container 后，应用管理器通过联系节点管理器来启动 container(第 9 步)。任务由一个主类为 YarnChild 的 Java 应用执行， 在运行任务之前首先本地化任务需要的资源，比如作业配置，JAR 文件, 以及分布式缓存的所有文件 (第 10 步。 最后, 运行 map 或 reduce 任务 (第 11 步)。 YarnChild 运行在一个专用的 JVM 中, 但是 YARN 不支持 JVM 重用。 5. 进度和状态更新YARN 中的任务将其进度和状态 (包括 counter) 返回给应用管理器, 客户端每秒 (通 mapreduce.client.progressmonitor.pollinterval 设置) 向应用管理器请求进度更新, 展示给用户。 6. 作业完成除了向应用管理器请求作业进度外, 客户端每 5 分钟都会通过调用 waitForCompletion() 来检查作业是否完成，时间间隔可以通过 mapreduce.client.completion.pollinterval 来设置。作业完成之后, 应用管理器和 container 会清理工作状态， OutputCommiter 的作业清理方法也会被调用。作业的信息会被作业历史服务器存储以备之后用户核查。 五、提交作业到YARN上运行这里以提交 Hadoop Examples 中计算 Pi 的 MApReduce 程序为例，相关 Jar 包在 Hadoop 安装目录的 share/hadoop/mapreduce 目录下： 12# 提交格式: hadoop jar jar包路径 主类名称 主类参数# hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.15.2.jar pi 3 3 参考资料 初步掌握 Yarn 的架构及原理 Apache Hadoop 2.9.2 &gt; Apache Hadoop YARN]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce]]></title>
    <url>%2F2019%2F05%2F18%2FHadoop-MapReduce%2F</url>
    <content type="text"><![CDATA[分布式计算框架——MapReduce一、MapReduce概述Hadoop MapReduce 是一个分布式计算框架，用于编写批处理应用程序。编写好的程序可以提交到 Hadoop 集群上用于并行处理大规模的数据集。 MapReduce 作业通过将输入的数据集拆分为独立的块，这些块由 map 以并行的方式处理，框架对 map 的输出进行排序，然后输入到 reduce 中。MapReduce 框架专门用于 &lt;key，value&gt; 键值对处理，它将作业的输入视为一组 &lt;key，value&gt; 对，并生成一组 &lt;key，value&gt; 对作为输出。输出和输出的 key 和 value 都必须实现Writable 接口。 1(input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt; -&gt; combine -&gt; &lt;k2, v2&gt; -&gt; reduce -&gt; &lt;k3, v3&gt; (output) 二、MapReduce编程模型简述这里以词频统计为例进行说明，MapReduce 处理的流程如下： input : 读取文本文件； splitting : 将文件按照行进行拆分，此时得到的 K1 行数，V1 表示对应行的文本内容； mapping : 并行将每一行按照空格进行拆分，拆分得到的 List(K2,V2)，其中 K2 代表每一个单词，由于是做词频统计，所以 V2 的值为 1，代表出现 1 次； shuffling：由于 Mapping 操作可能是在不同的机器上并行处理的，所以需要通过 shuffling 将相同 key 值的数据分发到同一个节点上去合并，这样才能统计出最终的结果，此时得到 K2 为每一个单词，List(V2) 为可迭代集合，V2 就是 Mapping 中的 V2； Reducing : 这里的案例是统计单词出现的总次数，所以 Reducing 对 List(V2) 进行归约求和操作，最终输出。 MapReduce 编程模型中 splitting 和 shuffing 操作都是由框架实现的，需要我们自己编程实现的只有 mapping 和 reducing，这也就是 MapReduce 这个称呼的来源。 三、combiner &amp; partitioner 3.1 InputFormat &amp; RecordReadersInputFormat 将输出文件拆分为多个 InputSplit，并由 RecordReaders 将 InputSplit 转换为标准的&lt;key，value&gt;键值对，作为 map 的输出。这一步的意义在于只有先进行逻辑拆分并转为标准的键值对格式后，才能为多个 map 提供输入，以便进行并行处理。 3.2 Combinercombiner 是 map 运算后的可选操作，它实际上是一个本地化的 reduce 操作，它主要是在 map 计算出中间文件后做一个简单的合并重复 key 值的操作。这里以词频统计为例： map 在遇到一个 hadoop 的单词时就会记录为 1，但是这篇文章里 hadoop 可能会出现 n 多次，那么 map 输出文件冗余就会很多，因此在 reduce 计算前对相同的 key 做一个合并操作，那么需要传输的数据量就会减少，传输效率就可以得到提升。 但并非所有场景都适合使用 combiner，使用它的原则是 combiner 的输出不会影响到 reduce 计算的最终输入，例如：求总数，最大值，最小值时都可以使用 combiner，但是做平均值计算则不能使用 combiner。 不使用 combiner 的情况： 使用 combiner 的情况： 可以看到使用 combiner 的时候，需要传输到 reducer 中的数据由 12keys，降低到 10keys。降低的幅度取决于你 keys 的重复率，下文词频统计案例会演示用 combiner 降低数百倍的传输量。 3.3 Partitionerpartitioner 可以理解成分类器，将 map 的输出按照 key 值的不同分别分给对应的 reducer，支持自定义实现，下文案例会给出演示。 四、MapReduce词频统计案例4.1 项目简介这里给出一个经典的词频统计的案例：统计如下样本数据中每个单词出现的次数。 12345678910111213Spark HBaseHive Flink Storm Hadoop HBase SparkFlinkHBase StormHBase Hadoop Hive FlinkHBase Flink Hive StormHive Flink HadoopHBase HiveHadoop Spark HBase StormHBase Hadoop Hive FlinkHBase Flink Hive StormHive Flink HadoopHBase Hive 为方便大家开发，我在项目源码中放置了一个工具类 WordCountDataUtils，用于模拟产生词频统计的样本，生成的文件支持输出到本地或者直接写到 HDFS 上。 项目完整源码下载地址：hadoop-word-count 4.2 项目依赖想要进行 MapReduce 编程，需要导入 hadoop-client 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;&lt;/dependency&gt; 4.3 WordCountMapper将每行数据按照指定分隔符进行拆分。这里需要注意在 MapReduce 中必须使用 Hadoop 定义的类型，因为 Hadoop 预定义的类型都是可序列化，可比较的，所有类型均实现了 WritableComparable 接口。 123456789101112public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] words = value.toString().split("\t"); for (String word : words) &#123; context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125; WordCountMapper 对应下图的 Mapping 操作： WordCountMapper 继承自 Mappe 类，这是一个泛型类，定义如下： 12345WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;public class Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; &#123; ......&#125; KEYIN : mapping 输入 key 的类型，即每行的偏移量 (每行第一个字符在整个文本中的位置)，Long 类型，对应 Hadoop 中的 LongWritable 类型； VALUEIN : mapping 输入 value 的类型，即每行数据；String 类型，对应 Hadoop 中 Text 类型； KEYOUT ：mapping 输出的 key 的类型，即每个单词；String 类型，对应 Hadoop 中 Text 类型； VALUEOUT：mapping 输出 value 的类型，即每个单词出现的次数；这里用 int 类型，对应 IntWritable 类型。 4.4 WordCountReducer在 Reduce 中进行单词出现次数的统计： 123456789101112public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; for (IntWritable value : values) &#123; count += value.get(); &#125; context.write(key, new IntWritable(count)); &#125;&#125; 如下图，shuffling 的输出是 reduce 的输入。这里的 key 是每个单词，values 是一个可迭代的数据类型，类似 (1,1,1,...)。 4.4 WordCountApp组装 MapReduce 作业，并提交到服务器运行，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * 组装作业 并提交到集群运行 */public class WordCountApp &#123; // 这里为了直观显示参数 使用了硬编码，实际开发中可以通过外部传参 private static final String HDFS_URL = "hdfs://192.168.0.107:8020"; private static final String HADOOP_USER_NAME = "root"; public static void main(String[] args) throws Exception &#123; // 文件输入路径和输出路径由外部传参指定 if (args.length &lt; 2) &#123; System.out.println("Input and output paths are necessary!"); return; &#125; // 需要指明 hadoop 用户名，否则在 HDFS 上创建目录时可能会抛出权限不足的异常 System.setProperty("HADOOP_USER_NAME", HADOOP_USER_NAME); Configuration configuration = new Configuration(); // 指明 HDFS 的地址 configuration.set("fs.defaultFS", HDFS_URL); // 创建一个 Job Job job = Job.getInstance(configuration); // 设置运行的主类 job.setJarByClass(WordCountApp.class); // 设置 Mapper 和 Reducer job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); // 设置 Mapper 输出 key 和 value 的类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 设置 Reducer 输出 key 和 value 的类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 如果输出目录已经存在，则必须先删除，否则重复运行程序时会抛出异常 FileSystem fileSystem = FileSystem.get(new URI(HDFS_URL), configuration, HADOOP_USER_NAME); Path outputPath = new Path(args[1]); if (fileSystem.exists(outputPath)) &#123; fileSystem.delete(outputPath, true); &#125; // 设置作业输入文件和输出文件的路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, outputPath); // 将作业提交到群集并等待它完成，参数设置为 true 代表打印显示对应的进度 boolean result = job.waitForCompletion(true); // 关闭之前创建的 fileSystem fileSystem.close(); // 根据作业结果,终止当前运行的 Java 虚拟机,退出程序 System.exit(result ? 0 : -1); &#125;&#125; 需要注意的是：如果不设置 Mapper 操作的输出类型，则程序默认它和 Reducer 操作输出的类型相同。 4.5 提交到服务器运行在实际开发中，可以在本机配置 hadoop 开发环境，直接在 IDE 中启动进行测试。这里主要介绍一下打包提交到服务器运行。由于本项目没有使用除 Hadoop 外的第三方依赖，直接打包即可： 1# mvn clean package 使用以下命令提交作业： 123hadoop jar /usr/appjar/hadoop-word-count-1.0.jar \com.heibaiying.WordCountApp \/wordcount/input.txt /wordcount/output/WordCountApp 作业完成后查看 HDFS 上生成目录： 12345# 查看目录hadoop fs -ls /wordcount/output/WordCountApp# 查看统计结果hadoop fs -cat /wordcount/output/WordCountApp/part-r-00000 五、词频统计案例进阶之Combiner5.1 代码实现想要使用 combiner 功能只要在组装作业时，添加下面一行代码即可： 12// 设置 Combinerjob.setCombinerClass(WordCountReducer.class); 5.2 执行结果加入 combiner 后统计结果是不会有变化的，但是可以从打印的日志看出 combiner 的效果： 没有加入 combiner 的打印日志： 加入 combiner 后的打印日志如下： 这里我们只有一个输入文件并且小于 128M，所以只有一个 Map 进行处理。可以看到经过 combiner 后，records 由 3519 降低为 6(样本中单词种类就只有 6 种)，在这个用例中 combiner 就能极大地降低需要传输的数据量。 六、词频统计案例进阶之Partitioner6.1 默认的Partitioner这里假设有个需求：将不同单词的统计结果输出到不同文件。这种需求实际上比较常见，比如统计产品的销量时，需要将结果按照产品种类进行拆分。要实现这个功能，就需要用到自定义 Partitioner。 这里先介绍下 MapReduce 默认的分类规则：在构建 job 时候，如果不指定，默认的使用的是 HashPartitioner：对 key 值进行哈希散列并对 numReduceTasks 取余。其实现如下： 12345678public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt; &#123; public int getPartition(K key, V value, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; 6.2 自定义Partitioner这里我们继承 Partitioner 自定义分类规则，这里按照单词进行分类： 123456public class CustomPartitioner extends Partitioner&lt;Text, IntWritable&gt; &#123; public int getPartition(Text text, IntWritable intWritable, int numPartitions) &#123; return WordCountDataUtils.WORD_LIST.indexOf(text.toString()); &#125;&#125; 在构建 job 时候指定使用我们自己的分类规则，并设置 reduce 的个数： 1234// 设置自定义分区规则job.setPartitionerClass(CustomPartitioner.class);// 设置 reduce 个数job.setNumReduceTasks(WordCountDataUtils.WORD_LIST.size()); 6.3 执行结果执行结果如下，分别生成 6 个文件，每个文件中为对应单词的统计结果： 参考资料 分布式计算框架 MapReduce Apache Hadoop 2.9.2 &gt; MapReduce Tutorial MapReduce - Combiners]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS原理]]></title>
    <url>%2F2019%2F05%2F10%2FHadoop-HDFS%2F</url>
    <content type="text"><![CDATA[Hadoop分布式文件系统——HDFS一、介绍HDFS （Hadoop Distributed File System）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。 二、HDFS 设计原理 2.1 HDFS 架构HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成： NameNode : 负责执行有关 文件系统命名空间 的操作，例如打开，关闭、重命名文件和目录等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。 DataNode：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。 2.2 文件系统命名空间HDFS 的 文件系统命名空间 的层次结构与大多数文件系统类似 (如 Linux)， 支持目录和文件的创建、移动、删除和重命名等操作，支持配置用户和访问权限，但不支持硬链接和软连接。NameNode 负责维护文件系统名称空间，记录对名称空间或其属性的任何更改。 2.3 数据复制由于 Hadoop 被设计运行在廉价的机器上，这意味着硬件是不可靠的，为了保证容错性，HDFS 提供了数据复制机制。HDFS 将每一个文件存储为一系列块，每个块由多个副本来保证容错，块的大小和复制因子可以自行配置（默认情况下，块大小是 128M，默认复制因子是 3）。 2.4 数据复制的实现原理大型的 HDFS 实例在通常分布在多个机架的多台服务器上，不同机架上的两台服务器之间通过交换机进行通讯。在大多数情况下，同一机架中的服务器间的网络带宽大于不同机架中的服务器之间的带宽。因此 HDFS 采用机架感知副本放置策略，对于常见情况，当复制因子为 3 时，HDFS 的放置策略是： 在写入程序位于 datanode 上时，就优先将写入文件的一个副本放置在该 datanode 上，否则放在随机 datanode 上。之后在另一个远程机架上的任意一个节点上放置另一个副本，并在该机架上的另一个节点上放置最后一个副本。此策略可以减少机架间的写入流量，从而提高写入性能。 如果复制因子大于 3，则随机确定第 4 个和之后副本的放置位置，同时保持每个机架的副本数量低于上限，上限值通常为 （复制系数 - 1）/机架数量 + 2，需要注意的是不允许同一个 dataNode 上具有同一个块的多个副本。 2.5 副本的选择为了最大限度地减少带宽消耗和读取延迟，HDFS 在执行读取请求时，优先读取距离读取器最近的副本。如果在与读取器节点相同的机架上存在副本，则优先选择该副本。如果 HDFS 群集跨越多个数据中心，则优先选择本地数据中心上的副本。 2.6 架构的稳定性1. 心跳机制和重新复制每个 DataNode 定期向 NameNode 发送心跳消息，如果超过指定时间没有收到心跳消息，则将 DataNode 标记为死亡。NameNode 不会将任何新的 IO 请求转发给标记为死亡的 DataNode，也不会再使用这些 DataNode 上的数据。 由于数据不再可用，可能会导致某些块的复制因子小于其指定值，NameNode 会跟踪这些块，并在必要的时候进行重新复制。 2. 数据的完整性由于存储设备故障等原因，存储在 DataNode 上的数据块也会发生损坏。为了避免读取到已经损坏的数据而导致错误，HDFS 提供了数据完整性校验机制来保证数据的完整性，具体操作如下： 当客户端创建 HDFS 文件时，它会计算文件的每个块的 校验和，并将 校验和 存储在同一 HDFS 命名空间下的单独的隐藏文件中。当客户端检索文件内容时，它会验证从每个 DataNode 接收的数据是否与存储在关联校验和文件中的 校验和 匹配。如果匹配失败，则证明数据已经损坏，此时客户端会选择从其他 DataNode 获取该块的其他可用副本。 3.元数据的磁盘故障FsImage 和 EditLog 是 HDFS 的核心数据，这些数据的意外丢失可能会导致整个 HDFS 服务不可用。为了避免这个问题，可以配置 NameNode 使其支持 FsImage 和 EditLog 多副本同步，这样 FsImage 或 EditLog 的任何改变都会引起每个副本 FsImage 和 EditLog 的同步更新。 4.支持快照快照支持在特定时刻存储数据副本，在数据意外损坏时，可以通过回滚操作恢复到健康的数据状态。 三、HDFS 的特点3.1 高容错由于 HDFS 采用数据的多副本方案，所以部分硬件的损坏不会导致全部数据的丢失。 3.2 高吞吐量HDFS 设计的重点是支持高吞吐量的数据访问，而不是低延迟的数据访问。 3.3 大文件支持HDFS 适合于大文件的存储，文档的大小应该是是 GB 到 TB 级别的。 3.3 简单一致性模型HDFS 更适合于一次写入多次读取 (write-once-read-many) 的访问模型。支持将内容追加到文件末尾，但不支持数据的随机访问，不能从文件任意位置新增数据。 3.4 跨平台移植性HDFS 具有良好的跨平台移植性，这使得其他大数据计算框架都将其作为数据持久化存储的首选方案。 附：图解HDFS存储原理 说明：以下图片引用自博客：翻译经典 HDFS 原理讲解漫画 1. HDFS写数据原理 2. HDFS读数据原理 3. HDFS故障类型和其检测方法 第二部分：读写故障的处理 第三部分：DataNode 故障处理 副本布局策略： 参考资料 Apache Hadoop 2.9.2 &gt; HDFS Architecture Tom White . hadoop 权威指南 [M] . 清华大学出版社 . 2017. 翻译经典 HDFS 原理讲解漫画]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS架构和shell操作]]></title>
    <url>%2F2019%2F05%2F02%2FHDFS%E6%9E%B6%E6%9E%84%E5%92%8Cshell%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[HDFS概述HDFS的产生背景随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种。 HDFS概念HDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。集群不一定是分布式的，但是分布式一定是集群。HDFS的设计适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。 HDFS优缺点优点 1）高容错性（1）数据自动保存多个副本。它通过增加副本的形式，提高容错性；（2）某一个副本丢失以后，它可以自动恢复。 2）适合大数据处理（1）数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据；（2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。 3）可构建在廉价机器上，通过多副本机制，提高可靠性。缺点 1）不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。 2）无法高效的对大量小文件进行存储。（1）存储大量小文件的话，它会占用NameNode大量的内存来存储文件、目录和块信息。这样是不可取的，因为NameNode的内存总是有限的；（2）小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。注：寻址时间，目前技术水平在10ms左右 2、传输寻址时间/传输时间=1%，传输时间1000ms=1s，磁盘传输速度100M/S,计算机是2的n次方，所以hadoop2.x默认块的大小为128m 3）不支持并发写入、文件随机修改。（1）一个文件只能有一个写，不允许多个线程同时写；（2）仅支持数据append（追加），不支持文件的随机修改。 HDFS组成架构HDFS组成架构如下图所示 架构主要由四个部分组成，分别为HDFS Client、NameNode、DataNode和Secondary NameNode。下面我们分别介绍这四个组成部分。 1）Client：就是客户端。 ​ （1）文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行存储； ​ （2）与NameNode交互，获取文件的位置信息； ​ （3）与DataNode交互，读取或者写入数据； ​ （4）Client提供一些命令来管理HDFS，比如启动或者关闭HDFS； ​ （5）Client可以通过一些命令来访问HDFS； 2）NameNode：就是Master，它是一个主管、管理者。 ​ （1）管理HDFS的名称空间；namespace ​ （2）管理数据块（Block）映射信息； ​ （3）配置副本策略（默认）；3 ​ （4）处理客户端读写请求。 3） DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。 ​ （1）存储实际的数据块； ​ （2）执行数据块的读/写操作。 4） SecondaryNameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。 ​ （1）辅助NameNode，分担其工作量； ​ （2）定期合并Fsimage和Edits，并推送给NameNode； ​ （3）在紧急情况下，可辅助恢复NameNode。 HDFS文件块大小HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M。 HDFS的Shell客户端操作cat使用方法：hadoop fs -cat URI [URI …] 将路径指定文件的内容输出到stdout。 示例： hadoop fs -cat hdfs://host1:port1/file1 hdfs://host2:port2/file2 hadoop fs -cat file:///file3 /user/hadoop/file4 返回值：成功返回0，失败返回-1。 chgrp使用方法：hadoop fs -chgrp [-R] GROUP URI [URI …] Change group association of files. With -R, make the change recursively through the directory structure. The user must be the owner of files, or else a super-user. Additional information is in the Permissions User Guide. –&gt; 改变文件所属的组。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。 chmod使用方法：hadoop fs -chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; URI [URI …] 改变文件的权限。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。 chown使用方法：hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ] 改变文件的拥有者。使用-R将使改变在目录结构下递归进行。命令的使用者必须是超级用户。更多的信息请参见HDFS权限用户指南。 copyFromLocal使用方法：hadoop fs -copyFromLocal URI 除了限定源路径是一个本地文件外，和put命令相似。 copyToLocal使用方法：hadoop fs -copyToLocal [-ignorecrc] [-crc] URI 除了限定目标路径是一个本地文件外，和get命令类似。 cp使用方法：hadoop fs -cp URI [URI …] 将文件从源路径复制到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。示例： hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir 返回值： 成功返回0，失败返回-1。 du使用方法：hadoop fs -du URI [URI …] 显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。示例：hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://host:port/user/hadoop/dir1返回值：成功返回0，失败返回-1。 dus使用方法：hadoop fs -dus 显示文件的大小。 expunge使用方法：hadoop fs -expunge 清空回收站。请参考HDFS设计文档以获取更多关于回收站特性的信息。 get使用方法：hadoop fs -get [-ignorecrc] [-crc] 复制文件到本地文件系统。可用-ignorecrc选项复制CRC校验失败的文件。使用-crc选项复制文件以及CRC信息。 示例： hadoop fs -get /user/hadoop/file localfile hadoop fs -get hdfs://host:port/user/hadoop/file localfile 返回值： 成功返回0，失败返回-1。 getmerge使用方法：hadoop fs -getmerge [addnl] 接受一个源目录和一个目标文件作为输入，并且将源目录中所有的文件连接成本地目标文件。addnl是可选的，用于指定在每个文件结尾添加一个换行符。 ls使用方法：hadoop fs -ls 如果是文件，则按照如下格式返回文件信息：文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：目录名 修改日期 修改时间 权限 用户ID 组ID示例：hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile返回值：成功返回0，失败返回-1。 lsr使用方法：hadoop fs -lsr ls命令的递归版本。类似于Unix中的ls -R。 mkdir使用方法：hadoop fs -mkdir 接受路径指定的uri作为参数，创建这些目录。其行为类似于Unix的mkdir -p，它会创建路径中的各级父目录。 示例： hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2 hadoop fs -mkdir hdfs://host1:port1/user/hadoop/dir hdfs://host2:port2/user/hadoop/dir 返回值： 成功返回0，失败返回-1。 movefromLocal使用方法：dfs -moveFromLocal 输出一个”not implemented“信息。 mv使用方法：hadoop fs -mv URI [URI …] 将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。示例： hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2 hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1 返回值： 成功返回0，失败返回-1。 put使用方法：hadoop fs -put … 从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。 hadoop fs -put localfile /user/hadoop/hadoopfile hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir hadoop fs -put localfile hdfs://host:port/hadoop/hadoopfile hadoop fs -put - hdfs://host:port/hadoop/hadoopfile从标准输入中读取输入。 返回值： 成功返回0，失败返回-1。 rm使用方法：hadoop fs -rm URI [URI …] 删除指定的文件。只删除非空目录和文件。请参考rmr命令了解递归删除。示例： hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir 返回值： 成功返回0，失败返回-1。 rmr使用方法：hadoop fs -rmr URI [URI …] delete的递归版本。示例： hadoop fs -rmr /user/hadoop/dir hadoop fs -rmr hdfs://host:port/user/hadoop/dir 返回值： 成功返回0，失败返回-1。 setrep使用方法：hadoop fs -setrep [-R] 改变一个文件的副本系数。-R选项用于递归改变目录下所有文件的副本系数。 示例： hadoop fs -setrep -w 3 -R /user/hadoop/dir1 返回值： 成功返回0，失败返回-1。 stat使用方法：hadoop fs -stat URI [URI …] 返回指定路径的统计信息。 示例： hadoop fs -stat path 返回值：成功返回0，失败返回-1。 tail使用方法：hadoop fs -tail [-f] URI 将文件尾部1K字节的内容输出到stdout。支持-f选项，行为和Unix中一致。 示例： hadoop fs -tail pathname 返回值：成功返回0，失败返回-1。 test使用方法：hadoop fs -test -[ezd] URI 选项：-e 检查文件是否存在。如果存在则返回0。-z 检查文件是否是0字节。如果是则返回0。-d 如果路径是个目录，则返回1，否则返回0。 示例： hadoop fs -test -e filename text使用方法：hadoop fs -text 将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。 touchz使用方法：hadoop fs -touchz URI [URI …] 创建一个0字节的空文件。 示例： hadoop -touchz pathname 返回值：成功返回0，失败返回-1。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[happybirthday]]></title>
    <url>%2F2019%2F04%2F18%2F%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90%2F</url>
    <content type="text"><![CDATA[function doDecrypt (pwd, onError) { console.log('in doDecrypt'); const txt = document.getElementById('enc_content').innerHTML; let plantext; try { const bytes = CryptoJS.AES.decrypt(txt, pwd); var plaintext = bytes.toString(CryptoJS.enc.Utf8); } catch(err) { if(onError) { onError(err); } return; } document.getElementById('enc_content').innerHTML = plaintext; document.getElementById('enc_content').style.display = 'block'; document.getElementById('enc_passwd').style.display = 'none'; if(typeof MathJax !== 'undefined') { MathJax.Hub.Queue( ['resetEquationNumbers', MathJax.InputJax.TeX], ['PreProcess', MathJax.Hub], ['Reprocess', MathJax.Hub] ); } } U2FsdGVkX1/JrkvgVFMLWNVh9q/h1QB5FQ9VbcdgwSrv1AO+TYR6UN2tAdrqmabEpACRRkP7L4mauChxI6hjRRwMkuwQcrJB7GuOz3UYmIQsOEMoETEFPcS2jM+vzQYD5d+zdbQRmCp6+hDJhXkb055Bf8UWPEHE/L9YzJHZXCl0S50M3jhkmx2DOuYBcr2N var onError = function(error) { document.getElementById("enc_error").innerHTML = "password error!" }; function decrypt() { var passwd = document.getElementById("enc_pwd_input").value; console.log(passwd); doDecrypt(passwd, onError); }]]></content>
  </entry>
  <entry>
    <title><![CDATA[给🐖的信]]></title>
    <url>%2F2019%2F03%2F30%2F%E4%B8%80%E5%91%A8%E5%B9%B4%E5%BF%AB%E4%B9%90%2F</url>
    <content type="text"><![CDATA[function doDecrypt (pwd, onError) { console.log('in doDecrypt'); const txt = document.getElementById('enc_content').innerHTML; let plantext; try { const bytes = CryptoJS.AES.decrypt(txt, pwd); var plaintext = bytes.toString(CryptoJS.enc.Utf8); } catch(err) { if(onError) { onError(err); } return; } document.getElementById('enc_content').innerHTML = plaintext; document.getElementById('enc_content').style.display = 'block'; document.getElementById('enc_passwd').style.display = 'none'; if(typeof MathJax !== 'undefined') { MathJax.Hub.Queue( ['resetEquationNumbers', MathJax.InputJax.TeX], ['PreProcess', MathJax.Hub], ['Reprocess', MathJax.Hub] ); } } U2FsdGVkX19Wshe+s9B0c3R6R10YiFwq+mqmZTX+LUguIhvF4LaFW1ii2RDthBPEXxTgXCdLKf+eOqts1iFUSXhczkI4jHCGH0FRSgWc4P3QcctCIpC+ua+a+7xynrlQUTiWmDH4Mh7LgZimCAiGwcXuNcCTLeXTjlsJRjia6WAyVNe7p7RTvpWy4xOFxN2S var onError = function(error) { document.getElementById("enc_error").innerHTML = "password error!" }; function decrypt() { var passwd = document.getElementById("enc_pwd_input").value; console.log(passwd); doDecrypt(passwd, onError); }]]></content>
  </entry>
  <entry>
    <title><![CDATA[==和equals()方法]]></title>
    <url>%2F2018%2F12%2F17%2F%3D%3D%E5%92%8Cequals%2F</url>
    <content type="text"><![CDATA[“==”java中==为关系运算符,表示左边的数据是否和右边的数据相等，相等返回true否则返回false。基本数据类型之间的比较直接使用==，java是面向对象的语言，除了基本数据类型外的全部都为引用数据类型，也就是对象类型。引用数据类型在内存中的存储分为两部分，栈空间存储的为堆空间内存地址，堆空间存储实际的数据。直接使用==比较的是内存地址，但所得到的结果可能并不是我们想要的。比如String的比较 12345678910代码段1：String str1 = &quot;aa&quot;;String str2 = &quot;aa&quot;;System.out.println(str1 == str2); //trueSystem.out.println(str1.equals(str2)); //true代码段2：String str3 = new String(&quot;cc&quot;);String str4 = new String(&quot;cc&quot;);System.out.println(str3 == str4); //falseSystem.out.println(str3.equals(str4)); //true 看以上代码发现str1==str2为true而str3==str4为false，原因是因为java在编译期会将相同的字符串当做一个对象放入常量池，所以str1和str2为同一个对象，==也为true。使用new String()来创建对象会强制创建两个不同的对象，所以str3==str4为false。所以引用数据类型之间的比较一定不能使用==，而应使用equals()方法。 equals()方法equals()方法为Object类的方法，源码如下； Object类中的equals()方法和==比较是相同的，返回的是==比较的结果。为了能使equals()方法比较对象的内容我们必须重写equals()方法。 String类中重写了equals()方法。源码如下： String类的equals()方法首先比较的是不是同一个对象，是直接返回true，不是则判断是否是String类的对象，不是返回false，是接着比较字符串中的每一个char是否都相同，相同则返回true，否则返回false。 所以String使用equals()方法比较的是字符串的内容。 我们自己写的类中可以使用IDE工具直接生成equals()方法，使其比较对象中的内容。]]></content>
      <categories>
        <category>java</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础]]></title>
    <url>%2F2018%2F12%2F15%2FLinux%E5%9F%BA%E7%A1%80%E5%92%8C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux教程Linux教程参考： 1http://www.runoob.com/linux/Linux-intro.html 必备命令1.man和page12345678910111213141516171819201.内部命令：echo查看内部命令帮助：help echo 或者 man echo2.外部命令：ls查看外部命令帮助：ls --help 或者 man ls 或者 info ls3.man文档的类型(1~9)man 7 manman 5 passwd4.快捷键：ctrl + c：停止进程ctrl + l：清屏ctrl + r：搜索历史命令ctrl + q：退出5.善于用tab键 2.常用1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283说明：安装linux时，创建一个hadoop用户，然后使用root用户登陆系统1.进入到用户根目录cd ~ 或 cd2.查看当前所在目录pwd3.进入到hadoop用户根目录cd ~hadoop4.返回到原来目录cd -5.返回到上一级目录cd ..6.查看hadoop用户根目录下的所有文件ls -la7.在根目录下创建一个hadoop的文件夹mkdir /hadoop8.在/hadoop目录下创建src和WebRoot两个文件夹分别创建：mkdir /hadoop/src mkdir /hadoop/WebRoot同时创建：mkdir /hadoop/&#123;src,WebRoot&#125;进入到/hadoop目录，在该目录下创建.classpath和README文件分别创建：touch .classpath touch README同时创建：touch &#123;.classpath,README&#125;查看/hadoop目录下面的所有文件ls -la在/hadoop目录下面创建一个test.txt文件,同时写入内容&quot;this is test&quot;echo &quot;this is test&quot; &gt; test.txt查看一下test.txt的内容cat test.txtmore test.txtless test.txt向README文件追加写入&quot;please read me first&quot;echo &quot;please read me first&quot; &gt;&gt; README将test.txt的内容追加到README文件中cat test.txt &gt;&gt; README拷贝/hadoop目录下的所有文件到/hadoop-bakcp -r /hadoop /hadoop-bak进入到/hadoop-bak目录，将test.txt移动到src目录下，并修改文件名为Student.javamv test.txt src/Student.java在src目录下创建一个struts.xml&gt; struts.xml删除所有的xml类型的文件rm -rf *.xml删除/hadoop-bak目录和下面的所有文件rm -rf /hadoop-bak返回到/hadoop目录，查看一下README文件有多单词，多少个少行wc -w READMEwc -l README返回到根目录，将/hadoop目录先打包，再用gzip压缩分步完成：tar -cvf hadoop.tar hadoop gzip hadoop.tar一步完成：tar -zcvf hadoop.tar.gz hadoop 将其解压缩，再取消打包分步完成：gzip -d hadoop.tar.gz 或 gunzip hadoop.tar.gz一步完成：tar -zxvf hadoop.tar.gz将/hadoop目录先打包，同时用bzip2压缩，并保存到/tmp目录下tar -jcvf /tmp/hadoop.tar.bz2 hadoop将/tmp/hadoop.tar.bz2解压到/usr目录下面tar -jxvf hadoop.tar.bz2 -C /usr/ 3. 文件命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869701.进入到用户根目录cd ~ 或者 cdcd ~hadoop回到原来路径cd -2.查看文件详情stat a.txt3.移动mv a.txt /ect/改名mv b.txt a.txt移动并改名mv a.txt ../b.txt4拷贝并改名cp a.txt /etc/b.txt5.vi撤销修改ctrl + u (undo)恢复ctrl + r (redo)6.名令设置别名(重启后无效)alias ll=&quot;ls -l&quot;取消unalias ll7.如果想让别名重启后仍然有效需要修改vi ~/.bashrc8.添加用户useradd hadooppasswd hadoop9创建多个文件touch a.txt b.txttouch /home/&#123;a.txt,b.txt&#125;10.将一个文件的内容复制到里另一个文件中cat a.txt &gt; b.txt追加内容cat a.txt &gt;&gt; b.txt 11.将a.txt 与b.txt设为其拥有者和其所属同一个组者可写入，但其他以外的人则不可写入:chmod ug+w,o-w a.txt b.txtchmod a=wx c.txt12.将当前目录下的所有文件与子目录皆设为任何人可读取:chmod -R a+r *13.将a.txt的用户拥有者设为users,组的拥有者设为jessie:chown users:jessie a.txt14.将当前目录下的所有文件与子目录的用户的使用者为lamport,组拥有者皆设为users，chown -R lamport:users *15.将所有的java语言程式拷贝至finished子目录中:cp *.java finished16.将目前目录及其子目录下所有扩展名是java的文件列出来。find -name &quot;*.java&quot;查找当前目录下扩展名是java 的文件find -name *.java17.删除当前目录下扩展名是java的文件rm -f *.java 4.系统命令1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253541.查看主机名hostname2.修改主机名(重启后无效)hostname hadoop3.修改主机名(重启后永久生效)vi /ect/sysconfig/network4.修改IP(重启后无效)ifconfig eth0 192.168.12.225.修改IP(重启后永久生效)vi /etc/sysconfig/network-scripts/ifcfg-eth06.查看系统信息uname -auname -r7.查看ID命令id -uid -g8.日期datedate +%Y-%m-%ddate +%Tdate +%Y-%m-%d&quot; &quot;%T9.日历cal 201210.查看文件信息file filename11.挂载硬盘mountumount加载windows共享mount -t cifs //192.168.1.100/tools /mnt12.查看文件大小du -hdu -ah13.查看分区df -h14.sshssh hadoop@192.168.1.115.关机shutdown -h now /init 0shutdown -r now /reboot 5.用户和组12345678910111213141516171819202122232425262728293031323334添加一个tom用户，设置它属于users组，并添加注释信息分步完成：useradd tom usermod -g users tom usermod -c &quot;hr tom&quot; tom一步完成：useradd -g users -c &quot;hr tom&quot; tom设置tom用户的密码passwd tom修改tom用户的登陆名为tomcatusermod -l tomcat tom将tomcat添加到sys和root组中usermod -G sys,root tomcat查看tomcat的组信息groups tomcat添加一个jerry用户并设置密码useradd jerrypasswd jerry添加一个交america的组groupadd america将jerry添加到america组中usermod -g america jerry将tomcat用户从root组和sys组删除gpasswd -d tomcat rootgpasswd -d tomcat sys将america组名修改为amgroupmod -n am america 6. 权限1234567891011121314创建a.txt和b.txt文件，将他们设为其拥有者和所在组可写入，但其他以外的人则不可写入:chmod ug+w,o-w a.txt b.txt创建c.txt文件所有人都可以写和执行chmod a=wx c.txt 或chmod 666 c.txt将/hadoop目录下的所有文件与子目录皆设为任何人可读取chmod -R a+r /hadoop将/hadoop目录下的所有文件与子目录的拥有者设为root，用户拥有组为userschown -R root:users /hadoop将当前目录下的所有文件与子目录的用户皆设为hadoop，组设为userschown -R hadoop:users * 7.目录属性12345678910111.查看文件夹属性ls -ld test2.文件夹的rwx--x:可以cd进去r-x:可以cd进去并ls-wx:可以cd进去并touch，rm自己的文件，并且可以vi其他用户的文件-wt:可以cd进去并touch，rm自己的文件ls -ld /tmpdrwxrwxrwt的权限值是1777(sticky) 8.软件安装1234567891011121314151617181920212223241.安装JDK *添加执行权限 chmod u+x jdk-6u45-linux-i586.bin *解压 ./jdk-6u45-linux-i586.bin *在/usr目录下创建java目录 mkdir /usr/java *将/soft目录下的解压的jdk1.6.0_45剪切到/usr/java目录下 mv jdk1.6.0_45/ /usr/java/ *添加环境变量 vim /etc/profile *在/etc/profile文件最后添加 export JAVA_HOME=/usr/java/jdk1.6.0_45 export CLASSPATH=$JAVA_HOME/lib export PATH=$PATH:$JAVA_HOME/bin *更新配置 source /etc/profile 2.安装tomcat tar -zxvf /soft/apache-tomcat-7.0.47.tar.gz -C /programs/ cd /programs/apache-tomcat-7.0.47/bin/ ./startup.sh 3.安装eclipse 9.vim1234567891011121314151617181920212223242526272829303132333435ia/Ao/Or + ?替换0:文件当前行的开头$:文件当前行的末尾G:文件的最后一行开头1 + G到第一行 9 + G到第九行 = :9dd:删除一行3dd：删除3行yy:复制一行3yy:复制3行p:粘贴u:undoctrl + r:redo&quot;a剪切板a&quot;b剪切板b&quot;ap粘贴剪切板a的内容每次进入vi就有行号vi ~/.vimrcset nu:w a.txt另存为:w &gt;&gt; a.txt内容追加到a.txt:e!恢复到最初状态:1,$s/hadoop/root/g 将第一行到追后一行的hadoop替换为root:1,$s/hadoop/root/c 将第一行到追后一行的hadoop替换为root(有提示) 10.查找123456789101112131415161718192021222324252627282930313233341.查找可执行的命令：which ls2.查找可执行的命令和帮助的位置：whereis ls3.查找文件(需要更新库:updatedb)locate hadoop.txt4.从某个文件夹开始查找find / -name &quot;hadooop*&quot;find / -name &quot;hadooop*&quot; -ls5.查找并删除find / -name &quot;hadooop*&quot; -ok rm &#123;&#125; \;find / -name &quot;hadooop*&quot; -exec rm &#123;&#125; \;6.查找用户为hadoop的文件find /usr -user hadoop -ls7.查找用户为hadoop并且(-a)拥有组为root的文件find /usr -user hadoop -a -group root -ls8.查找用户为hadoop或者(-o)拥有组为root并且是文件夹类型的文件find /usr -user hadoop -o -group root -a -type d9.查找权限为777的文件find / -perm -777 -type d -ls10.显示命令历史history11.grepgrep hadoop /etc/password 11.打包与压缩123456789101112131415161718192021222324252627282930313233343536373839401.gzip压缩gzip a.txt2.解压gunzip a.txt.gzgzip -d a.txt.gz3.bzip2压缩bzip2 a4.解压bunzip2 a.bz2bzip2 -d a.bz25.将当前目录的文件打包tar -cvf bak.tar .将/etc/password追加文件到bak.tar中(r)tar -rvf bak.tar /etc/password6.解压tar -xvf bak.tar7.打包并压缩gziptar -zcvf a.tar.gz8.解压缩tar -zxvf a.tar.gz解压到/usr/下tar -zxvf a.tar.gz -C /usr9.查看压缩包内容tar -ztvf a.tar.gzzip/unzip10.打包并压缩成bz2tar -jcvf a.tar.bz211.解压bz2tar -jxvf a.tar.bz2 12.正则12345678910111213141516171819202122232425262728293031323334353637383940414243441.cut截取以:分割保留第七段grep hadoop /etc/passwd | cut -d: -f72.排序du | sort -n 3.查询不包含hadoop的grep -v hadoop /etc/passwd4.正则表达包含hadoopgrep &apos;hadoop&apos; /etc/passwd5.正则表达(点代表任意一个字符)grep &apos;h.*p&apos; /etc/passwd6.正则表达以hadoop开头grep &apos;^hadoop&apos; /etc/passwd7.正则表达以hadoop结尾grep &apos;hadoop$&apos; /etc/passwd规则：. : 任意一个字符a* : 任意多个a(零个或多个a)a? : 零个或一个aa+ : 一个或多个a.* : 任意多个任意字符\. : 转义.\&lt;h.*p\&gt; ：以h开头，p结尾的一个单词o\&#123;2\&#125; : o重复两次grep &apos;^i.\&#123;18\&#125;n$&apos; /usr/share/dict/words查找不是以#开头的行grep -v &apos;^#&apos; a.txt | grep -v &apos;^$&apos; 以h或r开头的grep &apos;^[hr]&apos; /etc/passwd不是以h和r开头的grep &apos;^[^hr]&apos; /etc/passwd不是以h到r开头的grep &apos;^[^h-r]&apos; /etc/passwd 13.输入输出重定向及管道1234567891011121314151617181920212223242526271.新建一个文件touch a.txt&gt; b.txt2.错误重定向:2&gt;find /etc -name zhaoxing.txt 2&gt; error.txt3.将正确或错误的信息都输入到log.txt中find /etc -name passwd &gt; /tmp/log.txt 2&gt;&amp;1 find /etc -name passwd &amp;&gt; /tmp/log.txt4.追加&gt;&gt;5.将小写转为大写（输入重定向）tr &quot;a-z&quot; &quot;A-Z&quot; &lt; /etc/passwd6.自动创建文件cat &gt; log.txt &lt;&lt; EXIT&gt; ccc&gt; ddd&gt; EXI7.查看/etc下的文件有多少个？ls -l /etc/ | grep &apos;^d&apos; | wc -l8.查看/etc下的文件有多少个，并将文件详情输入到result.txt中ls -l /etc/ | grep &apos;^d&apos; | tee result.txt | wc -l 14.进程控制123456789101112131415161718192021222324252627281.查看用户最近登录情况lastlastlog2.查看硬盘使用情况df3.查看文件大小du4.查看内存使用情况free5.查看文件系统/proc6.查看日志ls /var/log/7.查看系统报错日志tail /var/log/messages8.查看进程top9.结束进程kill 1234kill -9 4333]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String、StringBuffer与StringBuilder]]></title>
    <url>%2F2018%2F11%2F21%2FString%E3%80%81StringBuilder%E4%B8%8EStringBuffer%2F</url>
    <content type="text"><![CDATA[StringString简介java中String为引用数据类型，其本身也为一个class文件，实现原理为char类型数组。（看下图）由于String类为final修饰的并且数组char value[]也为final修饰，所以String为不可变类型,赋值后不能改变，为字符串常量。 举个栗子🌰 12String a = &quot;eli&quot;;String a = &quot;chen&quot;; 以上语句看似字符串a发生了改变，从eli变为chen,但是String不可变所说的为字符串本身不可变，不是其引用所指向的对象地址。这里创建了一个新的String对象并使用a引用指向其地址，原来的eli并没有发生改变而是失去了地址引用,变成了内存中的垃圾。 String之间的比较我们在比较字符串时我们想比较的是字符串的内容是否相等，因为String为引用数据类型，String对象存储的只是指向该字符串堆空间的一个内存地址，使用==比较并不能得到正确结果，这与我们的初衷违背，因为String重写了Object类的equals()方法，我们在进行字符串比较式必须使用equals()方法，不能使用==。 代码演示 123456789101112代码段1：String str1 = &quot;aa&quot;;String str2 = &quot;aa&quot;;System.out.println(str1 == str2); //trueSystem.out.println(str1.equals(str2)); //true代码段2：String str3 = new String(&quot;cc&quot;);String str4 = new String(&quot;cc&quot;);System.out.println(str3 == str4); //falseSystem.out.println(str3.equals(str4)); //true代码段3：String e = &quot;eli&quot; + &quot;chen&quot;; 看以上代码发现str1==str2为true而str3==str4为false，原因是因为java在编译期会将相同的字符串当做一个对象放入常量池，所以str1和str2为同一个对象，==也为true。使用new String()来创建对象会强制创建两个不同的对象，所以str3==str4为false。 以上代码共创建几个对象？ 代码段1：String str1 = “aa”;在字符串常量池创建一个对象”aa”，后面str2时直接引用该对象，不用再创建，共创建一个对象 代码段2：先在字符串常量池创建一个对象”cc”,然后在堆空间创建两个对象，使str3和str4分别指向它们，共创建3个对象 代码段3:在字符串常量区创建3个对象，分别是”eli” ，“chen”，“elichen” 方法简介 判断功能 12345678910111213141516171819202122232425262728293031323334//1 equals判断字符串内容是否相等 引用数据类型 ==比较的是内存地址（是不是同一个对象）String s1 = &quot;aaa&quot;;String s2 = new String(&quot;aaa&quot;);System.out.println(s1.equals(s2)); //常量和变量使用equals比较内容时，尽量把常量写在前面//equalsIgnoreCase 忽略大小写的比较String str1 = &quot;aaa&quot;;String str2 = &quot;AAA&quot;;System.out.println(str1.equals(str2));System.out.println(str1.equalsIgnoreCase(str2));//contains 判断字符串是否包含某内容(子串)String str = &quot;abcde&quot;;System.out.println(str.contains(&quot;ab&quot;));System.out.println(str.contains(&quot;cde&quot;));System.out.println(str.contains(&quot;ac&quot;)); // startsWith endsWith 判断字符串是否以某个值开头 是否以某个值结尾String str=&quot;http://www.sina.com.cn&quot;;System.out.println(str.startsWith(&quot;h&quot;));System.out.println(str.startsWith(&quot;http&quot;));System.out.println(str.endsWith(&quot;cn&quot;));System.out.println(str.endsWith(&quot;.cn&quot;));// isEmpty 判断字符串是否 为空串String str1 = &quot;&quot;;String str2 = new String();String str3 = new String(&quot;&quot;);String str4 = &quot; &quot;;System.out.println(str1.isEmpty());System.out.println(str2.isEmpty());System.out.println(str3.isEmpty());System.out.println(str4.isEmpty()); 获取功能 1234567891011121314151617181920212223242526272829303132333435// 1 length() 获取字符串长度（由多少个字符组成）String str = &quot;abc&quot;;int x = str.length();System.out.println(x); //2 charAt 获取某个位置上的字符String str = &quot;abcde&quot;;char c = str.charAt(1);System.out.println(c); //3 substring 截取子串String str = &quot;abcdefg&quot;;String result = str.substring(2); //取到最后System.out.println(result);String result2 = str.substring(2,4); // 左闭右开System.out.println(result2); //4 indexOf 查找位置String str = &quot;abcdeabcd&quot;;int idx = str.indexOf(&quot;cd&quot;);System.out.println(idx); int idx2 = str.indexOf(&quot;ab&quot;,3);//int idx2 = str.indexOf(&quot;ab&quot;,-3);// 第2个参数为负数，相当于0System.out.println(idx2);int idx3 = str.indexOf(&quot;xy&quot;);System.out.println(idx3); // 找不到时，返回-1//5 lastIndexOf 查找位置 从后向前找String str = &quot;abcdeabcd&quot;;int idx = str.lastIndexOf(&quot;cd&quot;);System.out.println(idx); int idx2 = str.lastIndexOf(&quot;ab&quot;,3);System.out.println(idx2); int idx3 = str.lastIndexOf(&quot;xy&quot;);System.out.println(idx3); // 找不到时，返回-1 转换功能 12345678910111213141516171819202122232425262728293031323334353637// 1 getBytes() 将字符串转换成字节数组String str = &quot;abcd&quot;;byte[] b = str.getBytes();for(byte bb:b)&#123; System.out.println(bb);&#125;// 2 toCharArray() 将字符串转成字符数组String str = &quot;abcd&quot;;char[] cc = str.toCharArray();for(char c:cc)&#123; System.out.println(c);&#125;// 3 toUpperCase 小写转大写String str = &quot;abcd&quot;;String str2 = str.toUpperCase();System.out.println(str2); //4 toLowerCase 大写转小写String str = &quot;AbCd&quot;;String str2 = str.toLowerCase();System.out.println(str2); //5 concat 用来拼接字符串 和 +的作用相同 . 不同点： + 可以加任何类型 而concat只能拼接字符串String s1 = &quot;aaa&quot;;String s2 = &quot;bbb&quot;;String s3= s1.concat(s2);System.out.println(s3); //6 valueOf 将其他类型转换成字符串 静态方法 int x = 100;String str = String.valueOf(x);System.out.println(str+1); // &quot;1001&quot;int y = 200;String str2 = y+&quot;&quot;; //任何类型和字符串相加结果都是字符串System.out.println(str2+1); //&quot;2001&quot; StringBuffer(JDK1.0)StringBuffer简介StringBuffer为可变长字符串，原理与字符串相同，数组为可变长数组，默认容量为16，创建更长的字符串可以自动扩容。StringBuffer对象可以被多次修改并不会产生新的对象,进行字符串处理效率高。StringBuffer的方法都是使用synchronized修饰的方法，所以StringBuffer为线程安全的。 StringBuffer方法简介12345678910111213141516171819202122232425262728293031323334353637//StringBuffer构造方法1StringBuffer sb1=new StringBuffer(&quot;Hello&quot;);System.out.println(sb1); String s1=&quot;World&quot;;//StringBuffer构造方法2StringBuffer sb2=new StringBuffer(s1);System.out.println(sb2); //length()返回字符串的长度System.out.println(sb2.length());//toString()这个方法重写了Object中的toString()方法，返回String类型的字符串//输出StringBuffer对象时候，会默认调用此方法System.out.println(sb2); //append(String s)方法在原有的字符串后面添加字符串,返回的是添加后的StringBuffer对象sb1.append(&quot; World&quot;);System.out.println(sb1); //public StringBuffer deleteCharAt(int index)//该方法的作用是删除指定位置的字符，然后将剩余的内容形成新的字符串sb1.deleteCharAt(0);System.out.println(sb1);//ello World //public StringBuffer delete(int start,int end)//从字符缓冲区中从start索引删除到end索引所对应的字符，其中包括start索引不包括end索引对应的值sb1.delete(1, 3);System.out.println(sb1); //public StringBuffer insert(int offset,String str)//在字符串缓冲区的第offset个字符后面插入指定字符串sb1.insert(1, &quot;ME&quot;);System.out.println(sb1); //public StringBuffer reverse(),将字符串反转sb1.reverse();System.out.println(sb1); StringBuilder(JDK1.5)StringBuilder简介StringBuilder在JDK5.0新增的，StringBuilder与StringBuffer基本相同只不过不是线程安全的，方法也大致相同。StringBuilder效率更高。 总结执行速度 三者在执行速度方面的比较：StringBuilder &gt; StringBuffer &gt; String线程安全 StringBuffer：线程安全的，StringBuilder：非线程安全的 使用策略 1.如果要操作少量的数据用String 2.单线程操作字符串缓冲区 下操作大量数据用StringBuilder 3.多线程操作字符串缓冲区 下操作大量数据用StringBuffer 4.不要使用String类的”+”来进行频繁的拼接，因为那样的性能极差的，应该使用StringBuffer或StringBuilder类]]></content>
      <categories>
        <category>java</category>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一千行MySQL学习笔记]]></title>
    <url>%2F2018%2F11%2F12%2F%E4%B8%80%E5%8D%83%E8%A1%8CMySQL%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[原文地址：https://shockerli.net/post/1000-line-mysql-note/ 作者：格物 基本操作123456789/* Windows服务 */-- 启动MySQL net start mysql-- 创建Windows服务 sc create mysql binPath= mysqld_bin_path(注意：等号与值之间有空格)/* 连接与断开服务器 */mysql -h 地址 -P 端口 -u 用户名 -p 密码SHOW PROCESSLIST -- 显示哪些线程正在运行SHOW VARIABLES -- 显示系统变量信息 数据库操作12345678910111213141516171819/* 数据库操作 */ -------------------- 查看当前数据库 SELECT DATABASE();-- 显示当前时间、用户名、数据库版本 SELECT now(), user(), version();-- 创建库 CREATE DATABASE[ IF NOT EXISTS] 数据库名 数据库选项 数据库选项： CHARACTER SET charset_name COLLATE collation_name-- 查看已有库 SHOW DATABASES[ LIKE &apos;PATTERN&apos;]-- 查看当前库信息 SHOW CREATE DATABASE 数据库名-- 修改库的选项信息 ALTER DATABASE 库名 选项信息-- 删除库 DROP DATABASE[ IF EXISTS] 数据库名 同时删除该数据库相关的目录及其目录内容 表的操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576-- 创建表 CREATE [TEMPORARY] TABLE[ IF NOT EXISTS] [库名.]表名 ( 表的结构定义 )[ 表选项] 每个字段必须有数据类型 最后一个字段后不能有逗号 TEMPORARY 临时表，会话结束时表自动消失 对于字段的定义： 字段名 数据类型 [NOT NULL | NULL] [DEFAULT default_value] [AUTO_INCREMENT] [UNIQUE [KEY] | [PRIMARY] KEY] [COMMENT &apos;string&apos;]-- 表选项 -- 字符集 CHARSET = charset_name 如果表没有设定，则使用数据库字符集 -- 存储引擎 ENGINE = engine_name 表在管理数据时采用的不同的数据结构，结构不同会导致处理方式、提供的特性操作等不同 常见的引擎：InnoDB MyISAM Memory/Heap BDB Merge Example CSV MaxDB Archive 不同的引擎在保存表的结构和数据时采用不同的方式 MyISAM表文件含义：.frm表定义，.MYD表数据，.MYI表索引 InnoDB表文件含义：.frm表定义，表空间数据和日志文件 SHOW ENGINES -- 显示存储引擎的状态信息 SHOW ENGINE 引擎名 &#123;LOGS|STATUS&#125; -- 显示存储引擎的日志或状态信息 -- 自增起始数 AUTO_INCREMENT = 行数 -- 数据文件目录 DATA DIRECTORY = &apos;目录&apos; -- 索引文件目录 INDEX DIRECTORY = &apos;目录&apos; -- 表注释 COMMENT = &apos;string&apos; -- 分区选项 PARTITION BY ... (详细见手册)-- 查看所有表 SHOW TABLES[ LIKE &apos;pattern&apos;] SHOW TABLES FROM 库名-- 查看表机构 SHOW CREATE TABLE 表名 （信息更详细） DESC 表名 / DESCRIBE 表名 / EXPLAIN 表名 / SHOW COLUMNS FROM 表名 [LIKE &apos;PATTERN&apos;] SHOW TABLE STATUS [FROM db_name] [LIKE &apos;pattern&apos;]-- 修改表 -- 修改表本身的选项 ALTER TABLE 表名 表的选项 eg: ALTER TABLE 表名 ENGINE=MYISAM; -- 对表进行重命名 RENAME TABLE 原表名 TO 新表名 RENAME TABLE 原表名 TO 库名.表名 （可将表移动到另一个数据库） -- RENAME可以交换两个表名 -- 修改表的字段机构（13.1.2. ALTER TABLE语法） ALTER TABLE 表名 操作名 -- 操作名 ADD[ COLUMN] 字段定义 -- 增加字段 AFTER 字段名 -- 表示增加在该字段名后面 FIRST -- 表示增加在第一个 ADD PRIMARY KEY(字段名) -- 创建主键 ADD UNIQUE [索引名] (字段名)-- 创建唯一索引 ADD INDEX [索引名] (字段名) -- 创建普通索引 DROP[ COLUMN] 字段名 -- 删除字段 MODIFY[ COLUMN] 字段名 字段属性 -- 支持对字段属性进行修改，不能修改字段名(所有原有属性也需写上) CHANGE[ COLUMN] 原字段名 新字段名 字段属性 -- 支持对字段名修改 DROP PRIMARY KEY -- 删除主键(删除主键前需删除其AUTO_INCREMENT属性) DROP INDEX 索引名 -- 删除索引 DROP FOREIGN KEY 外键 -- 删除外键-- 删除表 DROP TABLE[ IF EXISTS] 表名 ...-- 清空表数据 TRUNCATE [TABLE] 表名-- 复制表结构 CREATE TABLE 表名 LIKE 要复制的表名-- 复制表结构和数据 CREATE TABLE 表名 [AS] SELECT * FROM 要复制的表名-- 检查表是否有错误 CHECK TABLE tbl_name [, tbl_name] ... [option] ...-- 优化表 OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ...-- 修复表 REPAIR [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... [QUICK] [EXTENDED] [USE_FRM]-- 分析表 ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 数据操作1234567891011121314151617/* 数据操作 */ -------------------- 增 INSERT [INTO] 表名 [(字段列表)] VALUES (值列表)[, (值列表), ...] -- 如果要插入的值列表包含所有字段并且顺序一致，则可以省略字段列表。 -- 可同时插入多条数据记录！ REPLACE 与 INSERT 完全一样，可互换。 INSERT [INTO] 表名 SET 字段名=值[, 字段名=值, ...]-- 查 SELECT 字段列表 FROM 表名[ 其他子句] -- 可来自多个表的多个字段 -- 其他子句可以不使用 -- 字段列表可以用*代替，表示所有字段-- 删 DELETE FROM 表名[ 删除条件子句] 没有条件子句，则会删除全部-- 改 UPDATE 表名 SET 字段名=新值[, 字段名=新值] [更新条件] 字符集编码123456789101112131415161718/* 字符集编码 */ -------------------- MySQL、数据库、表、字段均可设置编码-- 数据编码与客户端编码不需一致SHOW VARIABLES LIKE &apos;character_set_%&apos; -- 查看所有字符集编码项 character_set_client 客户端向服务器发送数据时使用的编码 character_set_results 服务器端将结果返回给客户端所使用的编码 character_set_connection 连接层编码SET 变量名 = 变量值 SET character_set_client = gbk; SET character_set_results = gbk; SET character_set_connection = gbk;SET NAMES GBK; -- 相当于完成以上三个设置-- 校对集 校对集用以排序 SHOW CHARACTER SET [LIKE &apos;pattern&apos;]/SHOW CHARSET [LIKE &apos;pattern&apos;] 查看所有字符集 SHOW COLLATION [LIKE &apos;pattern&apos;] 查看所有校对集 CHARSET 字符集编码 设置字符集编码 COLLATE 校对集编码 设置校对集编码 数据类型(列类型)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697/* 数据类型（列类型） */ ------------------1. 数值类型-- a. 整型 ---------- 类型 字节 范围（有符号位） tinyint 1字节 -128 ~ 127 无符号位：0 ~ 255 smallint 2字节 -32768 ~ 32767 mediumint 3字节 -8388608 ~ 8388607 int 4字节 bigint 8字节 int(M) M表示总位数 - 默认存在符号位，unsigned 属性修改 - 显示宽度，如果某个数不够定义字段时设置的位数，则前面以0补填，zerofill 属性修改 例：int(5) 插入一个数&apos;123&apos;，补填后为&apos;00123&apos; - 在满足要求的情况下，越小越好。 - 1表示bool值真，0表示bool值假。MySQL没有布尔类型，通过整型0和1表示。常用tinyint(1)表示布尔型。-- b. 浮点型 ---------- 类型 字节 范围 float(单精度) 4字节 double(双精度) 8字节 浮点型既支持符号位 unsigned 属性，也支持显示宽度 zerofill 属性。 不同于整型，前后均会补填0. 定义浮点型时，需指定总位数和小数位数。 float(M, D) double(M, D) M表示总位数，D表示小数位数。 M和D的大小会决定浮点数的范围。不同于整型的固定范围。 M既表示总位数（不包括小数点和正负号），也表示显示宽度（所有显示符号均包括）。 支持科学计数法表示。 浮点数表示近似值。-- c. 定点数 ---------- decimal -- 可变长度 decimal(M, D) M也表示总位数，D表示小数位数。 保存一个精确的数值，不会发生数据的改变，不同于浮点数的四舍五入。 将浮点数转换为字符串来保存，每9位数字保存为4个字节。2. 字符串类型-- a. char, varchar ---------- char 定长字符串，速度快，但浪费空间 varchar 变长字符串，速度慢，但节省空间 M表示能存储的最大长度，此长度是字符数，非字节数。 不同的编码，所占用的空间不同。 char,最多255个字符，与编码无关。 varchar,最多65535字符，与编码有关。 一条有效记录最大不能超过65535个字节。 utf8 最大为21844个字符，gbk 最大为32766个字符，latin1 最大为65532个字符 varchar 是变长的，需要利用存储空间保存 varchar 的长度，如果数据小于255个字节，则采用一个字节来保存长度，反之需要两个字节来保存。 varchar 的最大有效长度由最大行大小和使用的字符集确定。 最大有效长度是65532字节，因为在varchar存字符串时，第一个字节是空的，不存在任何数据，然后还需两个字节来存放字符串的长度，所以有效长度是64432-1-2=65532字节。 例：若一个表定义为 CREATE TABLE tb(c1 int, c2 char(30), c3 varchar(N)) charset=utf8; 问N的最大值是多少？ 答：(65535-1-2-4-30*3)/3-- b. blob, text ---------- blob 二进制字符串（字节字符串） tinyblob, blob, mediumblob, longblob text 非二进制字符串（字符字符串） tinytext, text, mediumtext, longtext text 在定义时，不需要定义长度，也不会计算总长度。 text 类型在定义时，不可给default值-- c. binary, varbinary ---------- 类似于char和varchar，用于保存二进制字符串，也就是保存字节字符串而非字符字符串。 char, varchar, text 对应 binary, varbinary, blob.3. 日期时间类型 一般用整型保存时间戳，因为PHP可以很方便的将时间戳进行格式化。 datetime 8字节 日期及时间 1000-01-01 00:00:00 到 9999-12-31 23:59:59 date 3字节 日期 1000-01-01 到 9999-12-31 timestamp 4字节 时间戳 19700101000000 到 2038-01-19 03:14:07 time 3字节 时间 -838:59:59 到 838:59:59 year 1字节 年份 1901 - 2155datetime YYYY-MM-DD hh:mm:sstimestamp YY-MM-DD hh:mm:ss YYYYMMDDhhmmss YYMMDDhhmmss YYYYMMDDhhmmss YYMMDDhhmmssdate YYYY-MM-DD YY-MM-DD YYYYMMDD YYMMDD YYYYMMDD YYMMDDtime hh:mm:ss hhmmss hhmmssyear YYYY YY YYYY YY4. 枚举和集合-- 枚举(enum) ----------enum(val1, val2, val3...) 在已知的值中进行单选。最大数量为65535. 枚举值在保存时，以2个字节的整型(smallint)保存。每个枚举值，按保存的位置顺序，从1开始逐一递增。 表现为字符串类型，存储却是整型。 NULL值的索引是NULL。 空字符串错误值的索引值是0。-- 集合（set） ----------set(val1, val2, val3...) create table tab ( gender set(&apos;男&apos;, &apos;女&apos;, &apos;无&apos;) ); insert into tab values (&apos;男, 女&apos;); 最多可以有64个不同的成员。以bigint存储，共8个字节。采取位运算的形式。 当创建表时，SET成员值的尾部空格将自动被删除。 列属性(列约束)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/* 列属性（列约束） */ ------------------1. PRIMARY 主键 - 能唯一标识记录的字段，可以作为主键。 - 一个表只能有一个主键。 - 主键具有唯一性。 - 声明字段时，用 primary key 标识。 也可以在字段列表之后声明 例：create table tab ( id int, stu varchar(10), primary key (id)); - 主键字段的值不能为null。 - 主键可以由多个字段共同组成。此时需要在字段列表后声明的方法。 例：create table tab ( id int, stu varchar(10), age int, primary key (stu, age));2. UNIQUE 唯一索引（唯一约束） 使得某字段的值也不能重复。3. NULL 约束 null不是数据类型，是列的一个属性。 表示当前列是否可以为null，表示什么都没有。 null, 允许为空。默认。 not null, 不允许为空。 insert into tab values (null, &apos;val&apos;); -- 此时表示将第一个字段的值设为null, 取决于该字段是否允许为null4. DEFAULT 默认值属性 当前字段的默认值。 insert into tab values (default, &apos;val&apos;); -- 此时表示强制使用默认值。 create table tab ( add_time timestamp default current_timestamp ); -- 表示将当前时间的时间戳设为默认值。 current_date, current_time5. AUTO_INCREMENT 自动增长约束 自动增长必须为索引（主键或unique） 只能存在一个字段为自动增长。 默认为1开始自动增长。可以通过表属性 auto_increment = x进行设置，或 alter table tbl auto_increment = x;6. COMMENT 注释 例：create table tab ( id int ) comment &apos;注释内容&apos;;7. FOREIGN KEY 外键约束 用于限制主表与从表数据完整性。 alter table t1 add constraint `t1_t2_fk` foreign key (t1_id) references t2(id); -- 将表t1的t1_id外键关联到表t2的id字段。 -- 每个外键都有一个名字，可以通过 constraint 指定 存在外键的表，称之为从表（子表），外键指向的表，称之为主表（父表）。 作用：保持数据一致性，完整性，主要目的是控制存储在外键表（从表）中的数据。 MySQL中，可以对InnoDB引擎使用外键约束： 语法： foreign key (外键字段） references 主表名 (关联字段) [主表记录删除时的动作] [主表记录更新时的动作] 此时需要检测一个从表的外键需要约束为主表的已存在的值。外键在没有关联的情况下，可以设置为null.前提是该外键列，没有not null。 可以不指定主表记录更改或更新时的动作，那么此时主表的操作被拒绝。 如果指定了 on update 或 on delete：在删除或更新时，有如下几个操作可以选择： 1. cascade，级联操作。主表数据被更新（主键值更新），从表也被更新（外键值更新）。主表记录被删除，从表相关记录也被删除。 2. set null，设置为null。主表数据被更新（主键值更新），从表的外键被设置为null。主表记录被删除，从表相关记录外键被设置成null。但注意，要求该外键列，没有not null属性约束。 3. restrict，拒绝父表删除和更新。 注意，外键只被InnoDB存储引擎所支持。其他引擎是不支持的。 建表规范1234567891011121314/* 建表规范 */ ------------------ -- Normal Format, NF - 每个表保存一个实体信息 - 每个具有一个ID字段作为主键 - ID主键 + 原子表 -- 1NF, 第一范式 字段不能再分，就满足第一范式。 -- 2NF, 第二范式 满足第一范式的前提下，不能出现部分依赖。 消除符合主键就可以避免部分依赖。增加单列关键字。 -- 3NF, 第三范式 满足第二范式的前提下，不能出现传递依赖。 某个字段依赖于主键，而有其他字段依赖于该字段。这就是传递依赖。 将一个实体信息的数据放在一个表内实现。 SELECT123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/* SELECT */ ------------------SELECT [ALL|DISTINCT] select_expr FROM -&gt; WHERE -&gt; GROUP BY [合计函数] -&gt; HAVING -&gt; ORDER BY -&gt; LIMITa. select_expr -- 可以用 * 表示所有字段。 select * from tb; -- 可以使用表达式（计算公式、函数调用、字段也是个表达式） select stu, 29+25, now() from tb; -- 可以为每个列使用别名。适用于简化列标识，避免多个列标识符重复。 - 使用 as 关键字，也可省略 as. select stu+10 as add10 from tb;b. FROM 子句 用于标识查询来源。 -- 可以为表起别名。使用as关键字。 SELECT * FROM tb1 AS tt, tb2 AS bb; -- from子句后，可以同时出现多个表。 -- 多个表会横向叠加到一起，而数据会形成一个笛卡尔积。 SELECT * FROM tb1, tb2; -- 向优化符提示如何选择索引 USE INDEX、IGNORE INDEX、FORCE INDEX SELECT * FROM table1 USE INDEX (key1,key2) WHERE key1=1 AND key2=2 AND key3=3; SELECT * FROM table1 IGNORE INDEX (key3) WHERE key1=1 AND key2=2 AND key3=3;c. WHERE 子句 -- 从from获得的数据源中进行筛选。 -- 整型1表示真，0表示假。 -- 表达式由运算符和运算数组成。 -- 运算数：变量（字段）、值、函数返回值 -- 运算符： =, &lt;=&gt;, &lt;&gt;, !=, &lt;=, &lt;, &gt;=, &gt;, !, &amp;&amp;, ||, in (not) null, (not) like, (not) in, (not) between and, is (not), and, or, not, xor is/is not 加上ture/false/unknown，检验某个值的真假 &lt;=&gt;与&lt;&gt;功能相同，&lt;=&gt;可用于null比较d. GROUP BY 子句, 分组子句 GROUP BY 字段/别名 [排序方式] 分组后会进行排序。升序：ASC，降序：DESC 以下[合计函数]需配合 GROUP BY 使用： count 返回不同的非NULL值数目 count(*)、count(字段) sum 求和 max 求最大值 min 求最小值 avg 求平均值 group_concat 返回带有来自一个组的连接的非NULL值的字符串结果。组内字符串连接。e. HAVING 子句，条件子句 与 where 功能、用法相同，执行时机不同。 where 在开始时执行检测数据，对原数据进行过滤。 having 对筛选出的结果再次进行过滤。 having 字段必须是查询出来的，where 字段必须是数据表存在的。 where 不可以使用字段的别名，having 可以。因为执行WHERE代码时，可能尚未确定列值。 where 不可以使用合计函数。一般需用合计函数才会用 having SQL标准要求HAVING必须引用GROUP BY子句中的列或用于合计函数中的列。f. ORDER BY 子句，排序子句 order by 排序字段/别名 排序方式 [,排序字段/别名 排序方式]... 升序：ASC，降序：DESC 支持多个字段的排序。g. LIMIT 子句，限制结果数量子句 仅对处理好的结果进行数量限制。将处理好的结果的看作是一个集合，按照记录出现的顺序，索引从0开始。 limit 起始位置, 获取条数 省略第一个参数，表示从索引0开始。limit 获取条数h. DISTINCT, ALL 选项 distinct 去除重复记录 默认为 all, 全部记录 UNION12345678/* UNION */ ------------------ 将多个select查询的结果组合成一个结果集合。 SELECT ... UNION [ALL|DISTINCT] SELECT ... 默认 DISTINCT 方式，即所有返回的行都是唯一的 建议，对每个SELECT查询加上小括号包裹。 ORDER BY 排序时，需加上 LIMIT 进行结合。 需要各select查询的字段数量一样。 每个select查询的字段列表(数量、类型)应一致，因为结果中的字段名以第一条select语句为准。 子查询1234567891011121314151617181920212223242526272829/* 子查询 */ ------------------ - 子查询需用括号包裹。-- from型 from后要求是一个表，必须给子查询结果取个别名。 - 简化每个查询内的条件。 - from型需将结果生成一个临时表格，可用以原表的锁定的释放。 - 子查询返回一个表，表型子查询。 select * from (select * from tb where id&gt;0) as subfrom where id&gt;1;-- where型 - 子查询返回一个值，标量子查询。 - 不需要给子查询取别名。 - where子查询内的表，不能直接用以更新。 select * from tb where money = (select max(money) from tb); -- 列子查询 如果子查询结果返回的是一列。 使用 in 或 not in 完成查询 exists 和 not exists 条件 如果子查询返回数据，则返回1或0。常用于判断条件。 select column1 from t1 where exists (select * from t2); -- 行子查询 查询条件是一个行。 select * from t1 where (id, gender) in (select id, gender from t2); 行构造符：(col1, col2, ...) 或 ROW(col1, col2, ...) 行构造符通常用于与对能返回两个或两个以上列的子查询进行比较。 -- 特殊运算符 != all() 相当于 not in = some() 相当于 in。any 是 some 的别名 != some() 不等同于 not in，不等于其中某一个。 all, some 可以配合其他运算符一起使用。 连接查询(join)123456789101112131415161718192021222324/* 连接查询(join) */ ------------------ 将多个表的字段进行连接，可以指定连接条件。-- 内连接(inner join) - 默认就是内连接，可省略inner。 - 只有数据存在时才能发送连接。即连接结果不能出现空行。 on 表示连接条件。其条件表达式与where类似。也可以省略条件（表示条件永远为真） 也可用where表示连接条件。 还有 using, 但需字段名相同。 using(字段名) -- 交叉连接 cross join 即，没有条件的内连接。 select * from tb1 cross join tb2;-- 外连接(outer join) - 如果数据不存在，也会出现在连接结果中。 -- 左外连接 left join 如果数据不存在，左表记录会出现，而右表为null填充 -- 右外连接 right join 如果数据不存在，右表记录会出现，而左表为null填充-- 自然连接(natural join) 自动判断连接条件完成连接。 相当于省略了using，会自动查找相同字段名。 natural join natural left join natural right joinselect info.id, info.name, info.stu_num, extra_info.hobby, extra_info.sex from info, extra_info where info.stu_num = extra_info.stu_id; TRUNCATE123456789/* TRUNCATE */ ------------------TRUNCATE [TABLE] tbl_name清空数据删除重建表区别：1，truncate 是删除表再创建，delete 是逐条删除2，truncate 重置auto_increment的值。而delete不会3，truncate 不知道删除了几条，而delete知道。4，当被用于带分区的表时，truncate 会保留分区 备份与还原123456789101112131415161718192021/* 备份与还原 */ ------------------备份，将数据的结构与表内数据保存起来。利用 mysqldump 指令完成。-- 导出mysqldump [options] db_name [tables]mysqldump [options] ---database DB1 [DB2 DB3...]mysqldump [options] --all--database1. 导出一张表 mysqldump -u用户名 -p密码 库名 表名 &gt; 文件名(D:/a.sql)2. 导出多张表 mysqldump -u用户名 -p密码 库名 表1 表2 表3 &gt; 文件名(D:/a.sql)3. 导出所有表 mysqldump -u用户名 -p密码 库名 &gt; 文件名(D:/a.sql)4. 导出一个库 mysqldump -u用户名 -p密码 --lock-all-tables --database 库名 &gt; 文件名(D:/a.sql)可以-w携带WHERE条件-- 导入1. 在登录mysql的情况下： source 备份文件2. 在不登录的情况下 mysql -u用户名 -p密码 库名 &lt; 备份文件 视图1234567891011121314151617181920212223242526272829什么是视图： 视图是一个虚拟表，其内容由查询定义。同真实的表一样，视图包含一系列带有名称的列和行数据。但是，视图并不在数据库中以存储的数据值集形式存在。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。 视图具有表结构文件，但不存在数据文件。 对其中所引用的基础表来说，视图的作用类似于筛选。定义视图的筛选可以来自当前或其它数据库的一个或多个表，或者其它视图。通过视图进行查询没有任何限制，通过它们进行数据修改时的限制也很少。 视图是存储在数据库中的查询的sql语句，它主要出于两种原因：安全原因，视图可以隐藏一些数据，如：社会保险基金表，可以用视图只显示姓名，地址，而不显示社会保险号和工资数等，另一原因是可使复杂的查询易于理解和使用。-- 创建视图CREATE [OR REPLACE] [ALGORITHM = &#123;UNDEFINED | MERGE | TEMPTABLE&#125;] VIEW view_name [(column_list)] AS select_statement - 视图名必须唯一，同时不能与表重名。 - 视图可以使用select语句查询到的列名，也可以自己指定相应的列名。 - 可以指定视图执行的算法，通过ALGORITHM指定。 - column_list如果存在，则数目必须等于SELECT语句检索的列数-- 查看结构 SHOW CREATE VIEW view_name-- 删除视图 - 删除视图后，数据依然存在。 - 可同时删除多个视图。 DROP VIEW [IF EXISTS] view_name ...-- 修改视图结构 - 一般不修改视图，因为不是所有的更新视图都会映射到表上。 ALTER VIEW view_name [(column_list)] AS select_statement-- 视图作用 1. 简化业务逻辑 2. 对客户端隐藏真实的表结构-- 视图算法(ALGORITHM) MERGE 合并 将视图的查询语句，与外部查询需要先合并再执行！ TEMPTABLE 临时表 将视图执行完毕后，形成临时表，再做外层查询！ UNDEFINED 未定义(默认)，指的是MySQL自主去选择相应的算法。 事务(transaction)123456789101112131415161718192021222324252627282930313233343536373839404142434445事务是指逻辑上的一组操作，组成这组操作的各个单元，要不全成功要不全失败。 - 支持连续SQL的集体成功或集体撤销。 - 事务是数据库在数据晚自习方面的一个功能。 - 需要利用 InnoDB 或 BDB 存储引擎，对自动提交的特性支持完成。 - InnoDB被称为事务安全型引擎。-- 事务开启 START TRANSACTION; 或者 BEGIN; 开启事务后，所有被执行的SQL语句均被认作当前事务内的SQL语句。-- 事务提交 COMMIT;-- 事务回滚 ROLLBACK; 如果部分操作发生问题，映射到事务开启前。-- 事务的特性 1. 原子性（Atomicity） 事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 2. 一致性（Consistency） 事务前后数据的完整性必须保持一致。 - 事务开始和结束时，外部数据一致 - 在整个事务过程中，操作是连续的 3. 隔离性（Isolation） 多个用户并发访问数据库时，一个用户的事务不能被其它用户的事物所干扰，多个并发事务之间的数据要相互隔离。 4. 持久性（Durability） 一个事务一旦被提交，它对数据库中的数据改变就是永久性的。-- 事务的实现 1. 要求是事务支持的表类型 2. 执行一组相关的操作前开启事务 3. 整组操作完成后，都成功，则提交；如果存在失败，选择回滚，则会回到事务开始的备份点。-- 事务的原理 利用InnoDB的自动提交(autocommit)特性完成。 普通的MySQL执行语句后，当前的数据提交操作均可被其他客户端可见。 而事务是暂时关闭“自动提交”机制，需要commit提交持久化数据操作。-- 注意 1. 数据定义语言（DDL）语句不能被回滚，比如创建或取消数据库的语句，和创建、取消或更改表或存储的子程序的语句。 2. 事务不能被嵌套-- 保存点 SAVEPOINT 保存点名称 -- 设置一个事务保存点 ROLLBACK TO SAVEPOINT 保存点名称 -- 回滚到保存点 RELEASE SAVEPOINT 保存点名称 -- 删除保存点-- InnoDB自动提交特性设置 SET autocommit = 0|1; 0表示关闭自动提交，1表示开启自动提交。 - 如果关闭了，那普通操作的结果对其他客户端也不可见，需要commit提交后才能持久化数据操作。 - 也可以关闭自动提交来开启事务。但与START TRANSACTION不同的是， SET autocommit是永久改变服务器的设置，直到下次再次修改该设置。(针对当前连接) 而START TRANSACTION记录开启前的状态，而一旦事务提交或回滚后就需要再次开启事务。(针对当前事务) 锁表1234567/* 锁表 */表锁定只用于防止其它客户端进行不正当地读取和写入MyISAM 支持表锁，InnoDB 支持行锁-- 锁定 LOCK TABLES tbl_name [AS alias]-- 解锁 UNLOCK TABLES 触发器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* 触发器 */ ------------------ 触发程序是与表有关的命名数据库对象，当该表出现特定事件时，将激活该对象 监听：记录的增加、修改、删除。-- 创建触发器CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt 参数： trigger_time是触发程序的动作时间。它可以是 before 或 after，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event指明了激活触发程序的语句的类型 INSERT：将新行插入表时激活触发程序 UPDATE：更改某一行时激活触发程序 DELETE：从表中删除某一行时激活触发程序 tbl_name：监听的表，必须是永久性的表，不能将触发程序与TEMPORARY表或视图关联起来。 trigger_stmt：当触发程序激活时执行的语句。执行多个语句，可使用BEGIN...END复合语句结构-- 删除DROP TRIGGER [schema_name.]trigger_name可以使用old和new代替旧的和新的数据 更新操作，更新前是old，更新后是new. 删除操作，只有old. 增加操作，只有new.-- 注意 1. 对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。-- 字符连接函数concat(str1,str2,...])concat_ws(separator,str1,str2,...)-- 分支语句if 条件 then 执行语句elseif 条件 then 执行语句else 执行语句end if;-- 修改最外层语句结束符delimiter 自定义结束符号 SQL语句自定义结束符号delimiter ; -- 修改回原来的分号-- 语句块包裹begin 语句块end-- 特殊的执行1. 只要添加记录，就会触发程序。2. Insert into on duplicate key update 语法会触发： 如果没有重复记录，会触发 before insert, after insert; 如果有重复记录并更新，会触发 before insert, before update, after update; 如果有重复记录但是没有发生更新，则触发 before insert, before update3. Replace 语法 如果有记录，则执行 before insert, before delete, after delete, after insert SQL编程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130/* SQL编程 */ --------------------// 局部变量 ------------ 变量声明 declare var_name[,...] type [default value] 这个语句被用来声明局部变量。要给变量提供一个默认值，请包含一个default子句。值可以被指定为一个表达式，不需要为一个常数。如果没有default子句，初始值为null。-- 赋值 使用 set 和 select into 语句为变量赋值。 - 注意：在函数内是可以使用全局变量（用户自定义的变量）--// 全局变量 ------------ 定义、赋值set 语句可以定义并为变量赋值。set @var = value;也可以使用select into语句为变量初始化并赋值。这样要求select语句只能返回一行，但是可以是多个字段，就意味着同时为多个变量进行赋值，变量的数量需要与查询的列数一致。还可以把赋值语句看作一个表达式，通过select执行完成。此时为了避免=被当作关系运算符看待，使用:=代替。（set语句可以使用= 和 :=）。select @var:=20;select @v1:=id, @v2=name from t1 limit 1;select * from tbl_name where @var:=30;select into 可以将表中查询获得的数据赋给变量。 -| select max(height) into @max_height from tb;-- 自定义变量名为了避免select语句中，用户自定义的变量与系统标识符（通常是字段名）冲突，用户自定义变量在变量名前使用@作为开始符号。@var=10; - 变量被定义后，在整个会话周期都有效（登录到退出）--// 控制结构 ------------ if语句if search_condition then statement_list [elseif search_condition then statement_list]...[else statement_list]end if;-- case语句CASE value WHEN [compare-value] THEN result[WHEN [compare-value] THEN result ...][ELSE result]END-- while循环[begin_label:] while search_condition do statement_listend while [end_label];- 如果需要在循环内提前终止 while循环，则需要使用标签；标签需要成对出现。 -- 退出循环 退出整个循环 leave 退出当前循环 iterate 通过退出的标签决定退出哪个循环--// 内置函数 ------------ 数值函数abs(x) -- 绝对值 abs(-10.9) = 10format(x, d) -- 格式化千分位数值 format(1234567.456, 2) = 1,234,567.46ceil(x) -- 向上取整 ceil(10.1) = 11floor(x) -- 向下取整 floor (10.1) = 10round(x) -- 四舍五入去整mod(m, n) -- m%n m mod n 求余 10%3=1pi() -- 获得圆周率pow(m, n) -- m^nsqrt(x) -- 算术平方根rand() -- 随机数truncate(x, d) -- 截取d位小数-- 时间日期函数now(), current_timestamp(); -- 当前日期时间current_date(); -- 当前日期current_time(); -- 当前时间date(&apos;yyyy-mm-dd hh:ii:ss&apos;); -- 获取日期部分time(&apos;yyyy-mm-dd hh:ii:ss&apos;); -- 获取时间部分date_format(&apos;yyyy-mm-dd hh:ii:ss&apos;, &apos;%d %y %a %d %m %b %j&apos;); -- 格式化时间unix_timestamp(); -- 获得unix时间戳from_unixtime(); -- 从时间戳获得时间-- 字符串函数length(string) -- string长度，字节char_length(string) -- string的字符个数substring(str, position [,length]) -- 从str的position开始,取length个字符replace(str ,search_str ,replace_str) -- 在str中用replace_str替换search_strinstr(string ,substring) -- 返回substring首次在string中出现的位置concat(string [,...]) -- 连接字串charset(str) -- 返回字串字符集lcase(string) -- 转换成小写left(string, length) -- 从string2中的左边起取length个字符load_file(file_name) -- 从文件读取内容locate(substring, string [,start_position]) -- 同instr,但可指定开始位置lpad(string, length, pad) -- 重复用pad加在string开头,直到字串长度为lengthltrim(string) -- 去除前端空格repeat(string, count) -- 重复count次rpad(string, length, pad) --在str后用pad补充,直到长度为lengthrtrim(string) -- 去除后端空格strcmp(string1 ,string2) -- 逐字符比较两字串大小-- 流程函数case when [condition] then result [when [condition] then result ...] [else result] end 多分支if(expr1,expr2,expr3) 双分支。-- 聚合函数count()sum();max();min();avg();group_concat()-- 其他常用函数md5();default();--// 存储函数，自定义函数 ------------ 新建 CREATE FUNCTION function_name (参数列表) RETURNS 返回值类型 函数体 - 函数名，应该合法的标识符，并且不应该与已有的关键字冲突。 - 一个函数应该属于某个数据库，可以使用db_name.funciton_name的形式执行当前函数所属数据库，否则为当前数据库。 - 参数部分，由&quot;参数名&quot;和&quot;参数类型&quot;组成。多个参数用逗号隔开。 - 函数体由多条可用的mysql语句，流程控制，变量声明等语句构成。 - 多条语句应该使用 begin...end 语句块包含。 - 一定要有 return 返回值语句。-- 删除 DROP FUNCTION [IF EXISTS] function_name;-- 查看 SHOW FUNCTION STATUS LIKE &apos;partten&apos; SHOW CREATE FUNCTION function_name;-- 修改 ALTER FUNCTION function_name 函数选项--// 存储过程，自定义功能 ------------ 定义存储存储过程 是一段代码（过程），存储在数据库中的sql组成。一个存储过程通常用于完成一段业务逻辑，例如报名，交班费，订单入库等。而一个函数通常专注与某个功能，视为其他程序服务的，需要在其他语句中调用函数才可以，而存储过程不能被其他调用，是自己执行 通过call执行。-- 创建CREATE PROCEDURE sp_name (参数列表) 过程体参数列表：不同于函数的参数列表，需要指明参数类型IN，表示输入型OUT，表示输出型INOUT，表示混合型注意，没有返回值。 存储过程12345678910111213141516/* 存储过程 */ ------------------存储过程是一段可执行性代码的集合。相比函数，更偏向于业务逻辑。调用：CALL 过程名-- 注意- 没有返回值。- 只能单独调用，不可夹杂在其他语句中-- 参数IN|OUT|INOUT 参数名 数据类型IN 输入：在调用过程中，将数据输入到过程体内部的参数OUT 输出：在调用过程中，将过程体处理完的结果返回到客户端INOUT 输入输出：既可输入，也可输出-- 语法CREATE PROCEDURE 过程名 (参数列表)BEGIN 过程体END 用户和权限管理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/* 用户和权限管理 */ -------------------- root密码重置1. 停止MySQL服务2. [Linux] /usr/local/mysql/bin/safe_mysqld --skip-grant-tables &amp; [Windows] mysqld --skip-grant-tables3. use mysql;4. UPDATE `user` SET PASSWORD=PASSWORD(&quot;密码&quot;) WHERE `user` = &quot;root&quot;;5. FLUSH PRIVILEGES;用户信息表：mysql.user-- 刷新权限FLUSH PRIVILEGES;-- 增加用户CREATE USER 用户名 IDENTIFIED BY [PASSWORD] 密码(字符串) - 必须拥有mysql数据库的全局CREATE USER权限，或拥有INSERT权限。 - 只能创建用户，不能赋予权限。 - 用户名，注意引号：如 &apos;user_name&apos;@&apos;192.168.1.1&apos; - 密码也需引号，纯数字密码也要加引号 - 要在纯文本中指定密码，需忽略PASSWORD关键词。要把密码指定为由PASSWORD()函数返回的混编值，需包含关键字PASSWORD-- 重命名用户RENAME USER old_user TO new_user-- 设置密码SET PASSWORD = PASSWORD(&apos;密码&apos;) -- 为当前用户设置密码SET PASSWORD FOR 用户名 = PASSWORD(&apos;密码&apos;) -- 为指定用户设置密码-- 删除用户DROP USER 用户名-- 分配权限/添加用户GRANT 权限列表 ON 表名 TO 用户名 [IDENTIFIED BY [PASSWORD] &apos;password&apos;] - all privileges 表示所有权限 - *.* 表示所有库的所有表 - 库名.表名 表示某库下面的某表 GRANT ALL PRIVILEGES ON `pms`.* TO &apos;pms&apos;@&apos;%&apos; IDENTIFIED BY &apos;pms0817&apos;;-- 查看权限SHOW GRANTS FOR 用户名 -- 查看当前用户权限 SHOW GRANTS; 或 SHOW GRANTS FOR CURRENT_USER; 或 SHOW GRANTS FOR CURRENT_USER();-- 撤消权限REVOKE 权限列表 ON 表名 FROM 用户名REVOKE ALL PRIVILEGES, GRANT OPTION FROM 用户名 -- 撤销所有权限-- 权限层级-- 要使用GRANT或REVOKE，您必须拥有GRANT OPTION权限，并且您必须用于您正在授予或撤销的权限。全局层级：全局权限适用于一个给定服务器中的所有数据库，mysql.user GRANT ALL ON *.*和 REVOKE ALL ON *.*只授予和撤销全局权限。数据库层级：数据库权限适用于一个给定数据库中的所有目标，mysql.db, mysql.host GRANT ALL ON db_name.*和REVOKE ALL ON db_name.*只授予和撤销数据库权限。表层级：表权限适用于一个给定表中的所有列，mysql.talbes_priv GRANT ALL ON db_name.tbl_name和REVOKE ALL ON db_name.tbl_name只授予和撤销表权限。列层级：列权限适用于一个给定表中的单一列，mysql.columns_priv 当使用REVOKE时，您必须指定与被授权列相同的列。-- 权限列表ALL [PRIVILEGES] -- 设置除GRANT OPTION之外的所有简单权限ALTER -- 允许使用ALTER TABLEALTER ROUTINE -- 更改或取消已存储的子程序CREATE -- 允许使用CREATE TABLECREATE ROUTINE -- 创建已存储的子程序CREATE TEMPORARY TABLES -- 允许使用CREATE TEMPORARY TABLECREATE USER -- 允许使用CREATE USER, DROP USER, RENAME USER和REVOKE ALL PRIVILEGES。CREATE VIEW -- 允许使用CREATE VIEWDELETE -- 允许使用DELETEDROP -- 允许使用DROP TABLEEXECUTE -- 允许用户运行已存储的子程序FILE -- 允许使用SELECT...INTO OUTFILE和LOAD DATA INFILEINDEX -- 允许使用CREATE INDEX和DROP INDEXINSERT -- 允许使用INSERTLOCK TABLES -- 允许对您拥有SELECT权限的表使用LOCK TABLESPROCESS -- 允许使用SHOW FULL PROCESSLISTREFERENCES -- 未被实施RELOAD -- 允许使用FLUSHREPLICATION CLIENT -- 允许用户询问从属服务器或主服务器的地址REPLICATION SLAVE -- 用于复制型从属服务器（从主服务器中读取二进制日志事件）SELECT -- 允许使用SELECTSHOW DATABASES -- 显示所有数据库SHOW VIEW -- 允许使用SHOW CREATE VIEWSHUTDOWN -- 允许使用mysqladmin shutdownSUPER -- 允许使用CHANGE MASTER, KILL, PURGE MASTER LOGS和SET GLOBAL语句，mysqladmin debug命令；允许您连接（一次），即使已达到max_connections。UPDATE -- 允许使用UPDATEUSAGE -- “无权限”的同义词GRANT OPTION -- 允许授予权限 表维护12345678/* 表维护 */-- 分析和存储表的关键字分布ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE 表名 ...-- 检查一个或多个表是否有错误CHECK TABLE tbl_name [, tbl_name] ... [option] ...option = &#123;QUICK | FAST | MEDIUM | EXTENDED | CHANGED&#125;-- 整理数据文件的碎片OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 杂项1234567891011121314/* 杂项 */ ------------------1. 可用反引号（`）为标识符（库名、表名、字段名、索引、别名）包裹，以避免与关键字重名！中文也可以作为标识符！2. 每个库目录存在一个保存当前数据库的选项文件db.opt。3. 注释： 单行注释 # 注释内容 多行注释 /* 注释内容 */ 单行注释 -- 注释内容 (标准SQL注释风格，要求双破折号后加一空格符（空格、TAB、换行等）)4. 模式通配符： _ 任意单个字符 % 任意多个字符，甚至包括零字符 单引号需要进行转义 \&apos;5. CMD命令行内的语句结束符可以为 &quot;;&quot;, &quot;\G&quot;, &quot;\g&quot;，仅影响显示结果。其他地方还是用分号结束。delimiter 可修改当前对话的语句结束符。6. SQL对大小写不敏感7. 清除已有语句：\c]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
</search>
